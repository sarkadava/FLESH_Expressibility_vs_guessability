---
title: 'FLESH: Analysis of Expressibility Ratings (Dutch)'
author: "Aleksandra Ćwiek, Susanne Fuchs, Šárka Kadavá, Wim Pouw (alph.)"
date: "2025-04-23"
bibliography: references.bib
link-citations: true
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float: yes
    df_print: paged
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '3'
  html_notebook:
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Source setup

```{r source setup, echo=TRUE, message=FALSE, warning=FALSE}
# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())

#################### packages ####################

required_packages <- c("tidyverse", # data wrangling
                       "readxl", 
                       "furrr",
                       "stringr", 
                       "broom", # for tidy model outputs
                       "brms",
                       "cmdstanr", 
                       "HDInterval", # package for credible interval computation
                       "tidybayes", # plotting
                       "ggpubr",
                       "loo", # model comparison
                       "BayesFactor", # model comparison
                       "lubridate") # for converting seconds to minutes

# if error with cmdstanr, use this
#install.packages("cmdstanr", repos = c('https://stan-dev.r-universe.dev', getOption("repos")))

# Loop over the packages and check if they are installed
for (package in required_packages) {
  if (!require(package, character.only = TRUE)) {
    # If the package is not installed, install it
    install.packages(package)
    # Load the package
    library(package, character.only = TRUE)
  }
}

# option for Bayesian regression models:
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())

colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                       "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

#list the folders
datasets  <- paste0(parentfolder, '/data/')
models   <- paste0(parentfolder, '/models/')
figures <- paste0(parentfolder, '/plots/')
scripts <- paste0(parentfolder, '/scripts/')

data <- read.csv(paste0(datasets, 'df_expressibility_dutch_final.csv'))

#suppressWarnings(source(paste0(scripts, "exp1a-preparation.R")))
```

------------------------------------------------------------------------

# The basic idea

# Methods

## Word list construction

## Instructions

Participants were introduced to a general situation in which they have to communicate certain meaning without using a language, by using only sounds, only gestures, or both. Afterwards, three separate blocks were presented to them, with explanation of the respective condition, followed by 20 words for which they have to evaluate the expressibility in this condition. Each block was accompanied by an icon representing the current condition (i.e., mouth, hand, both).

In gesture condition, participants were only allowed to use gestures of their body. Sounds were explicitly forbidden. Furthermore, they were instructed not to use pointing to objects in their environment.

In vocal condition, participants were instructed to use only sounds of their mouth and/or voice. Gestures were explicitly forbidden. @winter2023 @birchenough2017 In the multimodal condition, participants were told that they MUST use both sounds and gestures.

All instructions, after introduced textually, were summarized to graphics using binary logic 'you must/you cannot' just before start of the presentation of the words.

In each block, 20 words one at the time were presented and participants were asked 'How well would you communicate this word/concept without language?'.

In the beginning, there was a practice round consisting of 2 words for each condition, with an additional advice how to use the rating scale.

## Rating scale

We used a continuous scale anchored at 'very bad' and 'very good' with no middle point. However, we used a blended scale in order to....FINISH

## Procedure

Figure X shows a screenshot of the trial. The 'continue' option appeard only after a rating has been given by a participant. Participants were not given the option to skip a word as in other ratings (e.g., iconicity)

The rating task was implemented using PsychoPy builder (CIT) and deployed using Pavlovia. The data were collected between DATE and DATE, mostly via Clickworker. The experiment was set to present each word 23 times, as we wanted to have minimum 20 ratings for each word.

# Variables

While *expressibility* is the dependent variable (i.e., the measured outcome), let's see what other information we have that might be relevant.

```{r all variables, echo=TRUE, message=FALSE, warning=FALSE}
summary(data)
```

*Expressibility* is certainly dependent on:

-   modality
-   SemanticCat (maybe needs to be cleaned?)
-   PoS

We will also use random intercepts for:

-   word

# Clean and prepare data

Since there are some outliers in the response times, we will remove them.

## Exclusion 1: response time

```{r filter rt, echo=TRUE, message=FALSE, warning=FALSE}
library(dplyr)

# length before
before <- nrow(data)

# extract mean and sd
mean_rt <- mean(data$response_time, na.rm = TRUE)
sd_rt <- sd(data$response_time, na.rm = TRUE)

# remove the ones that are 3 sd or more
data <- data %>%
  filter(response_time < mean_rt + 3 * sd_rt)

# Remove data points less than 0.5 from response_time column
data <- subset(data, response_time >= 0.5)

# this is how many percent of raw data are left
100*nrow(data)/before

# this is how many percent was removed
100-(100*nrow(data)/before)

# we removed 1 % of data
```

## Exclusion 2: consecutive similarity in response

```{r straightlining, echo=TRUE, message=FALSE, warning=FALSE}
# response similarity 
data <- data %>% 
  group_by(exp_ID) %>% 
  mutate(difference = round(abs(c(0,diff(expressibility))), 4))

# value 0 for each first line of each participant --> should be 215 values of 0 
participants_diff_0 <- unique(data$exp_ID[data$difference == 0]) # and indeed there is 215 zeros
# filter them out
difference <- subset(data, difference != 0)

# create a row number column
difference <- difference %>%
  mutate(row_number = row_number())

# summary statistics
hist(difference$difference, breaks = "FD", xlab = "Absolute Difference", main = "Distribution of Response Similarities")  # Plot histogram

mean_diff <- mean(data$difference)  # Calculate mean difference - 0.66
sd_diff <- sd(data$difference)  # Calculate standard deviation of difference - 0.541

# let's set a treshold 
threshold <- mean_diff - 1*sd_diff
straightlining_data <- difference[difference$difference < threshold, ]  # Filter responses below the threshold

# Find rows where the values in column 2 are increasing by 1 within gesture modality
increasing_rows_g <- straightlining_data[c(NA, diff(straightlining_data$row_number)) == 1 & straightlining_data$modality == "gesture", ]
# within vocal modality
increasing_rows_v <- straightlining_data[c(NA, diff(straightlining_data$row_number)) == 1 & straightlining_data$modality == "vocal", ]
# within multimodal modality
increasing_rows_m <- straightlining_data[c(NA, diff(straightlining_data$row_number)) == 1 & straightlining_data$modality == "multimodal", ]

# It seems like everyone did their best
```

## Exclusion 3: correlation with the item average

Measure taken from @winter2023

```{r correlation with item mean, echo=TRUE, message=FALSE, warning=FALSE}
# Set the number of parallel workers
library(future)
plan(multicore)

# mean
data_mean <- data %>%
  group_by(word, modality) %>% 
  summarize(mean = mean(expressibility, na.rm = TRUE))

# append the mean to the main df
data <- left_join(data, data_mean)


# df with each unique participant
ppt_corrs <- tibble(exp_ID = unique(data$exp_ID))
# column to be filled with correlation
ppt_corrs$r = numeric(nrow(ppt_corrs))

# compute correlations

for (i in 1:nrow(ppt_corrs)) {
  # Extract subject into subset:
  
  id <- ppt_corrs[i, ]$exp_ID
  this_df <- filter(data, exp_ID == id)
  
  # Compute and store correlation:
  
  ppt_corrs[i, ]$r <- with(this_df,
                           cor(expressibility, mean,
                               use = 'complete.obs'))
  
}

# the distribution
library(ggplot2)

ppt_corrs %>%
  ggplot(aes(x = r)) +
  geom_density(fill = 'steelblue', alpha = 0.5) +
  geom_vline(aes(xintercept = 0), linetype = 2) +
  geom_vline(aes(xintercept = 0.1), col = 'darkgrey') + # threshold value r = 0.1
  scale_y_continuous(expand = c(0, 0),
                     limits = c(0, 4)) +
  theme_classic()

# the average of correlation
ppt_corrs %>%
  summarize(r_M = mean(r),
            r_SD = sd(r))

# order
ppt_corrs <- arrange(ppt_corrs, r)
ppt_corrs

# bad participants under threshold

bad_subs <- filter(ppt_corrs, r < 0.1) %>%
  pull(exp_ID)

length(bad_subs) # 2 bads subjects

# how much %

length(bad_subs) / length(unique(data$exp_ID)) # 0.009 %
```

We decide to not exclude those 2 participants. But it is good to see that people are very consistent in their intuition and that there is some 'common sense' about expressibility. Moreover, the instructions seem to be clear enough so people understand the task in a similar manner.

# Add information on words

We will add some statistics on the concepts. We have to bear in mind that sometimes the concepts have two translations (one word in English = two words in German or the other way around). Therefore, in order to omit data doubling, we will look for the most frequent English equivalent that will serve as a translation. 


```{r add info, echo=TRUE, message=FALSE, warning=FALSE}

library(readxl)

# load concept excel with info about PoS and SemCat
concepts <- read_excel(paste0(datasets, "concept_list_info.xlsx"))
concepts <- select(concepts, -c(PoS, SemanticCat, Info))


# create df that matches info from df data and concepts
data <- merge(data, concepts, by.x = "word", by.y = "Dutch")

# Because we have multiple translations, we have to choose one.
# As long as we do not have frequency data (for English), we need a work-around.
data$English <- as.factor(data$English)
data %>%
  distinct(word, English) %>%
  group_by(word) %>%
  filter(n() > 1)
# From among those words, we need to choose the ones that fit better (for now, by hand).
# meat, shadow, hit, cry should stay (since I checked by hand that they are more frequent)
data <- data %>%
  filter(!(English %in% c("flesh", "shade", "to beat", "to weep", "loud")))

data <- data %>% select(word, English, modality, expressibility, SemanticCat, SemanticSubcat, PoS, response_time, trial_start_time, end_time, duration, exp_ID, age, gender, natlang1, natlang2, natlang3, dialect, lang1, proficiency1, lang2, proficiency2, lang3, proficiency3, lang4, proficiency4, lang5, proficiency5, country, education, dyslexia, mobility, currentloc, sensorypref)

# Remove duplicate rows
data <- unique(data)

# Make them factors.
data$PoS <- as.factor(data$PoS)
data$SemanticCat <- as.factor(data$SemanticCat)
data$SemanticSubcat <- as.factor(data$SemanticSubcat)
```

# Descriptive statistics

## Participants

How many participants?

```{r n participants, echo=TRUE, message=FALSE, warning=FALSE}
length(unique(data$exp_ID))

# 215
```

Let's also learn something about the participants.

```{r meta participants, echo=TRUE, message=FALSE, warning=FALSE}
# Gender distribution
data %>% 
  distinct(exp_ID, gender) %>% # show a list of gender of each participant
  count(gender) %>% # make a cumulative count
  arrange(desc(n)) # arrange in descending order

#"1" = "female", "2" = "male", "3" = "diverse", "4" = NA_character

# Age distribution
data %>% summarize(
  min_age = min(age),
  max_age = max(age),
  mean_age = mean(age),
  median_age = median(age)
)

# Education distribution
data %>% 
  distinct(exp_ID, education) %>%
  count(education) %>%
  arrange(desc(n))

# Current location 
data %>% 
  distinct(exp_ID, currentloc) %>%
  count(currentloc) %>%
  arrange(desc(n))

# Country
data %>% 
  distinct(exp_ID, country) %>%
  count(country) %>%
  arrange(desc(n))

# Dyslexia 
data %>% 
  distinct(exp_ID, dyslexia) %>%
  count(dyslexia) %>%
  arrange(desc(n))

# Mobility 
data %>% 
  distinct(exp_ID, mobility) %>%
  count(mobility) %>%
  arrange(desc(n))

# Second L1 
data %>% 
  distinct(exp_ID, natlang2) %>%
  count(natlang2) %>%
  arrange(desc(n))

# Third L1
data %>% 
  distinct(exp_ID, natlang3) %>%
  count(natlang3) %>%
  arrange(desc(n))

# L2
data %>% 
  distinct(exp_ID, lang1) %>%
  count(lang1) %>%
  arrange(desc(n))

# L3
data %>% 
  distinct(exp_ID, lang2) %>%
  count(lang2) %>%
  arrange(desc(n))

# L4
data %>% 
  distinct(exp_ID, lang3) %>%
  count(lang3) %>%
  arrange(desc(n))

# Sensory preference
data %>% 
  distinct(exp_ID, sensorypref) %>%
  count(sensorypref) %>%
  arrange(desc(n))

# 1 - hearing only
# 11 - hearing + visual + taste 
# 14 - all: hearing + visual + taste + other
# 2 - visual only
# 3 - taste only
# 4 - other 
# 5 - hearing + visual
# 6 - hearing + taste 
# 7 - hearing + other
# 8 - visual + taste
# 9 - visual + other
```

How much time did each participant take?

```{r participant time, echo=TRUE, message=FALSE, warning=FALSE}
library(lubridate)

# Total duration in minutes
data %>% summarize(
    min_duration = seconds_to_period(min(duration, na.rm = TRUE)),
    max_duration = seconds_to_period(max(duration, na.rm = TRUE)),
    mean_duration = seconds_to_period(mean(duration, na.rm = TRUE)),
    median_duration = seconds_to_period(median(duration, na.rm = TRUE))
  )

# Single concept response time in seconds
data %>% summarize(
    min_response_time = min(response_time, na.rm = TRUE),
    max_response_time = max(response_time, na.rm = TRUE),
    mean_response_time = mean(response_time, na.rm = TRUE)
  )
```

## Words

How many words?

```{r n words, echo=TRUE, message=FALSE, warning=FALSE}
length(unique(data$word))
```

And let's look at the words more closely.

```{r meta words, echo=TRUE, message=FALSE, warning=FALSE}
# PoS distribution
data %>% 
  distinct(word, PoS) %>%
  count(PoS) %>%
  arrange(desc(n))

# Semantic category distribution
data %>% 
  distinct(word, SemanticCat) %>%
  count(SemanticCat) %>%
  arrange(desc(n))

# Semantic subcategory distribution
data %>% 
  distinct(word, SemanticSubcat) %>%
  count(SemanticSubcat) %>%
  arrange(desc(n))

# Later also analyses on the different ratings
```

## Modality / Expressibility

Let's look at the ratings from the perspective of modality.

```{r modality ratings, echo=TRUE, message=FALSE, warning=FALSE}
# How many times each word was rated in each modality 
word_modality_counts <- data %>%
  count(word, modality) %>%
  arrange(desc(n))
word_modality_counts

# Which words have the fewest ratings per modality
word_modality_counts %>% filter(n == min(n))
# 16 ratings is the minimum
```

And now at raw expressibility.

```{r expressibility rating, echo=TRUE, message=FALSE, warning=FALSE}
# Calculate average raw expressibility per word per modality.
expressibility_avg <- data %>%
  group_by(word, modality) %>%
  summarize(avg_expressibility = mean(expressibility, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(avg_expressibility))
expressibility_avg

# Create datasets for each modality (sorted and with ranking)
gesture_avg <- expressibility_avg %>% 
  filter(modality == "gesture") %>%
  arrange(desc(avg_expressibility)) %>%
  mutate(n = row_number())
gesture_avg
multimodal_avg <- expressibility_avg %>% 
  filter(modality == "multimodal") %>%
  arrange(desc(avg_expressibility)) %>%
  mutate(n = row_number())
multimodal_avg
vocal_avg <- expressibility_avg %>% 
  filter(modality == "vocal") %>%
  arrange(desc(avg_expressibility)) %>%
  mutate(n = row_number())
vocal_avg

# Add the ranking information to expressibility_avg
expressibility_avg <- merge(expressibility_avg, gesture_avg[, c("word", "modality", "n")], by = c("word", "modality"), all.x = TRUE)
expressibility_avg <- merge(expressibility_avg, vocal_avg[, c("word", "modality", "n")], by = c("word", "modality"), all.x = TRUE)
expressibility_avg <- merge(expressibility_avg, multimodal_avg[, c("word", "modality", "n")], by = c("word", "modality"), all.x = TRUE)
expressibility_avg$n <- coalesce(expressibility_avg$n.x, expressibility_avg$n.y, expressibility_avg$n)
expressibility_avg <- expressibility_avg %>%
  select(-c(n.x, n.y))

# Estimate the divergence in ratings by expressibility
expressibility_avg <- expressibility_avg %>%
  group_by(word) %>%
  mutate(diff_expr = max(avg_expressibility) - min(avg_expressibility)) %>%
  ungroup()

# Estimate the divergence in ratings by ranking
expressibility_avg <- expressibility_avg %>%
  group_by(word) %>%
  mutate(diff_rank = max(n) - min(n)) %>%
  ungroup()
```

Let's print the list to see the divergence.

```{r print expressibility_avg, echo=TRUE, message=FALSE, warning=FALSE}
# expressibility average by difference in expressibility
expressibility_avg %>%
  arrange(desc(diff_expr))

# expressibility average by difference in rank
expressibility_avg %>%
  arrange(desc(diff_rank))

```

# Visual

We have ratings between -1 and 1. The participants probably tended towards the extremes. On the other hand, there is a big difference between modalities.

```{r scale expressibility, echo=TRUE, message=FALSE, warning=FALSE}
# z-score
data$expressibility_z <- scale(data$expressibility)

# creates values between 0 and 1
data$expressibility_norm <- (data$expressibility-min(data$expressibility))/(max(data$expressibility)-min(data$expressibility))
```

Count raw mean for word per modality

```{r mean from normalized expressibility, echo=TRUE, message=FALSE, warning=FALSE}

# Calculate average raw expressibility per word per modality.
expressibility_norm_avg <- data %>%
  group_by(word, modality) %>%
  summarize(avg_expressibility_norm = mean(expressibility_norm, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(avg_expressibility_norm))
expressibility_norm_avg


# Create datasets for each modality (sorted and with ranking)
gesture_norm_avg <- expressibility_norm_avg %>% 
  filter(modality == "gesture") %>%
  arrange(desc(avg_expressibility_norm)) %>%
  mutate(n = row_number())
gesture_norm_avg
multimodal_norm_avg <- expressibility_norm_avg %>% 
  filter(modality == "multimodal") %>%
  arrange(desc(avg_expressibility_norm)) %>%
  mutate(n = row_number())
multimodal_norm_avg
vocal_norm_avg <- expressibility_norm_avg %>% 
  filter(modality == "vocal") %>%
  arrange(desc(avg_expressibility_norm)) %>%
  mutate(n = row_number())
vocal_norm_avg

# Add the ranking information to expressibility_avg
expressibility_norm_avg <- merge(expressibility_norm_avg, gesture_norm_avg[, c("word", "modality", "n")], by = c("word", "modality"), all.x = TRUE)
expressibility_norm_avg <- merge(expressibility_norm_avg, multimodal_norm_avg[, c("word", "modality", "n")], by = c("word", "modality"), all.x = TRUE)
expressibility_norm_avg <- merge(expressibility_norm_avg, vocal_norm_avg[, c("word", "modality", "n")], by = c("word", "modality"), all.x = TRUE)
expressibility_norm_avg$n <- coalesce(expressibility_norm_avg$n.x, expressibility_norm_avg$n.y, expressibility_norm_avg$n)
expressibility_norm_avg <- expressibility_norm_avg %>%
  select(-c(n.x, n.y))

# Estimate the divergence in ratings by expressibility
expressibility_norm_avg <- expressibility_norm_avg %>%
  group_by(word) %>%
  mutate(diff_expr = max(avg_expressibility_norm) - min(avg_expressibility_norm)) %>%
  ungroup()

# Estimate the divergence in ratings by ranking
expressibility_norm_avg <- expressibility_norm_avg %>%
  group_by(word) %>%
  mutate(diff_rank = max(n) - min(n)) %>%
  ungroup()


# save this df
write.csv(expressibility_norm_avg, paste0(datasets, "df_express_dutch_norm_avg.csv"))
```

Some plots.

```{r violin, echo=TRUE, message=FALSE, warning=FALSE}
# Create a violin plot with box plot overlay
ggplot(data, aes(x = modality, y = expressibility_norm)) +
  geom_violin(fill = "lightgray", color = "black", trim = FALSE) +
  geom_boxplot(width = 0.2, fill = "white", color = "black", outlier.shape = NA) +
  labs(x = "Modality", y = "Expressibility Norm") +
  theme_minimal()

# Create a scatter plot with a loess smoother
ggplot(data, aes(x = modality, y = expressibility_norm)) +
  geom_point(position = position_jitter(width = 0.1), size = 2) +
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "Modality", y = "Expressibility Norm") +
  theme_minimal()


ggplot(data, aes(x = expressibility_norm, fill = modality)) +
  geom_density(alpha = 0.2) +
  scale_fill_manual(values = colorBlindBlack8) +
  labs(x = "Expressibility (normalized)", y = "Density") +
  theme_minimal()

ggplot(data, aes(x = expressibility_norm, fill = PoS)) +
  geom_density(alpha = 0.2) +
  scale_fill_manual(values = colorBlindBlack8) +
  labs(x = "PoS", y = "Density") +
  theme_minimal()
```

# Modeling

## Contrast / dummy coding

```{r contrast coding, echo=TRUE, message=FALSE, warning=FALSE}
# Gender
data$gender <- as.factor(data$gender)
contrasts(data$gender)

# Modality
data$modality <- as.factor(data$modality)
contrasts(data$modality)

# Word
data$word <- as.character(data$word)
data$word <- as.factor(data$word)
```

## Levels

We change the levels() to reflect our beliefs.

```{r adapt levels, echo=TRUE, message=FALSE, warning=FALSE}

library(forcats)

levels(data$modality)
# make multimodality the baseline
levels(data$PoS)
# make verb the baseline
levels(data$posneg)
# negative is baseline

data <- data %>%
  mutate(modality = fct_relevel(modality, "multimodal"),
         PoS = fct_relevel(PoS, "verb"))
```

## Define priors

```{r beta priors, echo=TRUE, message=FALSE, warning=FALSE}

library(brms)

get_prior(formula = expressibility ~ 1 + modality + 
                                  (1 + modality || word),
          data = data,
          family = zero_inflated_beta())

# Define the priors
priors_beta <- c(
  prior('normal(0, 1)', class = 'Intercept', lb = 0, ub = 1),
  prior('normal(0, 0.5)', class = 'b')  # Priors for other predictors
)

priors_beta_narrow <- c(
  prior('normal(0.5, 0.5)', class = 'Intercept', lb = 0, ub = 1),
  prior('normal(0, 0.2)', class = 'b')
  )
```

## Zero-inflated beta model

I run a few models first with various priors and predictors.

```{r mdl zoib 1, echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
# https://journal.r-project.org/archive/2015/RJ-2015-019/
# https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#special-case-1-zero-one-inflated-beta-regression
# https://mvuorre.github.io/posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/#zoib-regression

# compute mdl
mdl_zoib <- brm(bf(expressibility_norm ~ 1 + modality + (1 + modality || word),
                   phi ~ modality + (1 + modality || word),
                   zoi ~ modality + (1 + modality || word),
                   coi ~ modality + (1 + modality || word)),
                data = data,
                prior = priors_beta,
                family = zero_one_inflated_beta(),
                backend = "cmdstanr",
                cores = 4,
                chains = 4,
                init = 0,
                iter = 4000,
                warmup = 2000,
                seed = 42,
                control = list(max_treedepth = 13,
                               adapt_delta = 0.99),
                file = paste0(models, "mdl_zoib.rds"))

# compress the model more
saveRDS(mdl_zoib, file = paste0(models, "mdl_zoib.rds"), compress = "xz")

```

Let's check how it looks like.

```{r mdl zoib 1 check, echo=TRUE, message=FALSE, warning=FALSE}

mdl_zoib <- readRDS(paste0(models, "mdl_zoib.rds"))

summary(mdl_zoib)
pp_check(mdl_zoib, ndraws = 100)

conditional_effects(mdl_zoib, sample_prior = "only")
```

Let's compute the loo for model comparison.

```{r mdl zoib 1 loo, echo=TRUE, message=FALSE, warning=FALSE}
# run loo mdl
if (file.exists(paste0(models, "mdl_zoib_loo.rds"))) {
  mdl_zoib_loo <- readRDS(paste0(models, "mdl_zoib_loo.rds"))
} else {
  mdl_zoib_loo <- loo(mdl_zoib)
  saveRDS(mdl_zoib_loo, paste0(models, "mdl_zoib_loo.rds"))
}
```

Let's try narrower priors.

```{r mdl zoib 2, echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
# compute mdl2
mdl_zoib2 <- brm(bf(expressibility_norm ~ 1 + modality + (1 + modality || word),
                   phi ~ modality + (1 + modality || word),
                   zoi ~ modality + (1 + modality || word),
                   coi ~ modality + (1 + modality || word)),
                data = data,
                prior = priors_beta_narrow,
                family = zero_one_inflated_beta(),
                backend = "cmdstanr",
                cores = 4,
                chains = 4,
                init = 0,
                iter = 4000,
                warmup = 2000,
                seed = 42,
                control = list(max_treedepth = 13,
                               adapt_delta = 0.99),
                file = paste0(models, "mdl_zoib2.rds"))

# compress the model more
saveRDS(mdl_zoib2, file = paste0(models, "mdl_zoib2.rds"), compress = "xz")

```

Check mdl2.

```{r mdl zoib 2 check, echo=TRUE, message=FALSE, warning=FALSE}

mdl_zoib2 <- readRDS(paste0(models, "mdl_zoib2.rds"))

summary(mdl_zoib2)
pp_check(mdl_zoib2, ndraws = 100)
plot(mdl_zoib2)

conditional_effects(mdl_zoib2)
conditional_effects(mdl_zoib2, sample_prior = "only")
```

Compute loo mdl2.

```{r mdl zoib 2 loo, echo=TRUE, message=FALSE, warning=FALSE}
# run loo mdl2
if (file.exists(paste0(models, "mdl_zoib2_loo.rds"))) {
  mdl_zoib2_loo <- readRDS(paste0(models, "mdl_zoib2_loo.rds"))
} else {
  mdl_zoib2_loo <- loo(mdl_zoib2)
  saveRDS(mdl_zoib2_loo, paste0(models, "mdl_zoib2_loo.rds"))
}
```

Now to model 3 with simpler structure of phi, zoi, and coi.

```{r mdl zoib 3, echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
# compute mdl3
mdl_zoib3 <- brm(bf(expressibility_norm ~ 1 + modality + (1 + modality || word),
                   phi ~ modality,
                   zoi ~ modality,
                   coi ~ modality),
                data = data,
                prior = priors_beta_narrow,
                family = zero_one_inflated_beta(),
                backend = "cmdstanr",
                cores = 4,
                chains = 4,
                init = 0,
                iter = 4000,
                warmup = 2000,
                seed = 42,
                control = list(max_treedepth = 13,
                               adapt_delta = 0.99),
                file = paste0(models, "mdl_zoib3.rds"))

# compress the model more
saveRDS(mdl_zoib3, file = paste0(models, "mdl_zoib3.rds"), compress = "xz")

```

Check mdl3.

```{r mdl zoib 3 check, echo=TRUE, message=FALSE, warning=FALSE}

mdl_zoib3 <- readRDS(paste0(models, "mdl_zoib3.rds"))

summary(mdl_zoib3)
pp_check(mdl_zoib3, ndraws = 100)
plot(mdl_zoib3)

conditional_effects(mdl_zoib3)
conditional_effects(mdl_zoib3, sample_prior = "only")
```

Computer mdl3 loo.

```{r mdl zoib 3 loo, echo=TRUE, message=FALSE, warning=FALSE}
# run loo mdl3
if (file.exists(paste0(models, "mdl_zoib3_loo.rds"))) {
  mdl_zoib3_loo <- readRDS(paste0(models, "mdl_zoib3_loo.rds"))
} else {
  mdl_zoib3_loo <- loo(mdl_zoib3)
  saveRDS(mdl_zoib3_loo, paste0(models, "mdl_zoib3_loo.rds"))
}
```

And for the final, we take simple phi, zoi, and coi, but make the predictors more complex.

```{r mdl zoib 4, echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
# compute mdl4
mdl_zoib4 <- brm(bf(expressibility_norm ~ 1 + modality + PoS + SemanticCat + 
                      (1 + modality || word),
                   phi ~ modality,
                   zoi ~ modality,
                   coi ~ modality),
                data = data,
                prior = priors_beta_narrow,
                family = zero_one_inflated_beta(),
                backend = "cmdstanr",
                cores = 4,
                chains = 4,
                init = 0,
                iter = 4000,
                warmup = 2000,
                seed = 42,
                control = list(max_treedepth = 13,
                               adapt_delta = 0.99),
                file = paste0(models, "mdl_zoib4.rds"))

# compress the model more
saveRDS(mdl_zoib4, file = paste0(models, "mdl_zoib4.rds"), compress = "xz")

```

Check mdl4.

```{r mdl zoib 4 check, echo=TRUE, message=FALSE, warning=FALSE}

mdl_zoib4 <- readRDS(paste0(models, "mdl_zoib4.rds"))

summary(mdl_zoib4)
pp_check(mdl_zoib4, ndraws = 100)
plot(mdl_zoib4)

conditional_effects(mdl_zoib4)
conditional_effects(mdl_zoib4, sample_prior = "only")
```

Compute loo mdl4.

```{r mdl zoib 4 loo, echo=TRUE, message=FALSE, warning=FALSE}
# run loo mdl4
if (file.exists(paste0(models, "mdl_zoib4_loo.rds"))) {
  mdl_zoib4_loo <- readRDS(paste0(models, "mdl_zoib4_loo.rds"))
} else {
  mdl_zoib4_loo <- loo(mdl_zoib4)
  saveRDS(mdl_zoib4_loo, paste0(models, "mdl_zoib4_loo.rds"))
}
```

Compare all models to choose the best fit.

```{r loo compare all mdls, echo=TRUE, message=FALSE, warning=FALSE}
if (file.exists(paste0(models, "loo_compare_all_mdls.rds"))) {
  loo_compare_all_mdls <- readRDS(paste0(models, "loo_compare_all_mdls.rds"))
} else {
  loo_compare_all_mdls <- loo_compare(mdl_zoib_loo, mdl_zoib2_loo, mdl_zoib3_loo, mdl_zoib4_loo)
  saveRDS(loo_compare_all_mdls, paste0(models, "loo_compare_all_mdls.rds"))
}

# Print the comparison result
print(loo_compare_all_mdls)
```

The comparison shows that mdl_zoib2 is the best-fitting model.

## Plotting words

Setup data frame with predictors to get predictions for:

```{r prep plot, message=FALSE, warning=FALSE}
unique_words <- unique(data$word)
unique_modalities <- levels(data$modality)

# Define the modality colors using colorblind-friendly colors
modality_colors <- c("multimodal" = "#0173B2", "gesture" = "#DE8F05", "vocal" = "#029E73")
```


### ZOIB model

```{r word predictions zoib, echo=TRUE, message=FALSE, warning=FALSE}
# Create expanded newdata_zoib dataframe
newdata_zoib <- expand.grid(word = unique_words, modality = unique_modalities)
#newdata_zoib$SemanticCat <- data[match(newdata_zoib$word, data$word), ]$SemanticCat
#newdata_zoib$PoS <- data[match(newdata_zoib$word, data$word), ]$PoS
#newdata_zoib$posneg <- data[match(newdata_zoib$word, data$word), ]$posneg

# Get predictions and append to dataframe:
fit_zoib <- fitted(mdl_zoib2, newdata = newdata_zoib, re_formula = NULL, robust = TRUE)
colnames(fit_zoib) <- c('fit', 'se', 'lwr', 'upr')
newdata_zoib <- cbind(newdata_zoib, fit_zoib)

# Order predictions by descriptive average:
newdata_zoib <- arrange(newdata_zoib, fit_zoib)
newdata_zoib <- mutate(newdata_zoib, word = factor(as.character(word), levels = unique_words))
```

Add averages and English translations for better understanding.

```{r add averages zoib, echo=TRUE, message=FALSE, warning=FALSE}
newdata_zoib <- merge(newdata_zoib, expressibility_norm_avg, by = c("word", "modality"), all.x = TRUE)

# Add the column "English" to "newdata_zoib" by matching values in the "word" column

newdata_zoib <- merge(newdata_zoib, concepts, by.x = "word", by.y = "Dutch")

# Remove the translations we removed earlier
newdata_zoib <- newdata_zoib %>%
  filter(!(English %in% c("flesh", "shade", "to beat", "to weep")))

# save the df
write.csv(newdata_zoib, paste0(datasets, "df_estim_dutch.csv"))
```

Plot the words.

```{r plot words zoib, echo=TRUE, message=FALSE, warning=FALSE}
# Filter the top 20 words per modality based on fit
top_words_zoib <- newdata_zoib %>%
  group_by(modality) %>%
  top_n(20, wt = fit)

# Calculate the average fit across modalities for each word
word_avg_fit_zoib <- top_words_zoib %>%
  group_by(English) %>%
  summarise(avg_fit = mean(fit))

# Sort the words based on the average fit in descending order
sorted_words_zoib <- word_avg_fit_zoib %>%
  arrange(desc(avg_fit)) %>%
  pull(English)

# Reorder the levels of the word factor based on the sorted words
top_words_zoib$English <- factor(top_words_zoib$English, levels = sorted_words_zoib)

##FLAGGED: here previously used 'avg_expressibility' that was based on the scale -1:1. However, the model is based on normalized expressibility (only + values), the new version needs to use avg_expressibility_norm
# Plot the top words
p_zoib <- top_words_zoib %>%
  ggplot(aes(x = English, col = modality, y = fit, ymin = lwr, ymax = upr)) +
  geom_errorbar(size = 0.8, width = 0.4, alpha = 0.5, position = position_dodge(width = 0.5)) +
  geom_point(size = 3, shape = 15, alpha = 0.5, position = position_dodge(width = 0.5)) +
  geom_point(aes(y = avg_expressibility_norm, fill = modality), shape = 23, size = 3,
             stroke = 0.5, alpha = 0.5, position = position_dodge(width = 0.5)) +
  guides(fill = FALSE) +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 16),
        legend.position = "bottom",
        text = element_text(size = 18),  # Adjusted font size
        axis.title = element_text(size = 18),  # Adjusted font size
        plot.title = element_text(size = 24, face = "bold")) +
  labs(x = "Word", y = "Expressibility", color = "Modality") +
  geom_hline(yintercept = 0.5, linetype = "blank")

# Assign modality colors
p_zoib <- p_zoib + scale_color_manual(values = modality_colors)
p_zoib <- p_zoib + scale_fill_manual(values = modality_colors)

# Display the plot
p_zoib

# save the plot
#ggsave(plot = p_zoib, filename = paste0(figures,'by_word_zoib_dutch.pdf'),
 #      width = 16, height = 8)
```

Plot the words separately for each modality.

```{r plot words by modality zoib, echo=TRUE, message=FALSE, warning=FALSE}
# Filter the top 20 words per modality based on fit
top_words_zoib_gesture <- top_words_zoib %>%
  filter(modality == "gesture") %>%
  arrange(desc(fit)) %>%
  top_n(20)

# Filter the top 20 words for other modalities in descending order
top_words_zoib_vocal <- top_words_zoib %>%
  filter(modality == "vocal") %>%
  arrange(desc(fit)) %>%
  top_n(20)

top_words_zoib_multimodal <- top_words_zoib %>%
  filter(modality == "multimodal") %>%
  arrange(desc(fit)) %>%
  top_n(20)

# Convert the word column to factor with the desired order within each modality
top_words_zoib_gesture$English <- factor(top_words_zoib_gesture$English, levels = top_words_zoib_gesture$English)
top_words_zoib_vocal$English <- factor(top_words_zoib_vocal$English, levels = top_words_zoib_vocal$English)
top_words_zoib_multimodal$English <- factor(top_words_zoib_multimodal$English, levels = top_words_zoib_multimodal$English)

# Create separate plots for each modality
plot_gesture_zoib <- ggplot(data = top_words_zoib_gesture, aes(x = English, y = fit, ymin = lwr, ymax = upr)) +
  geom_errorbar(aes(color = modality), size = 0.8, width = 0.4, alpha = 0.7) +
  geom_point(aes(color = modality), size = 4, alpha = 0.7) +
  geom_point(aes(y = avg_expressibility_norm, fill = modality), shape = 23, size = 3,
             stroke = 0.5, alpha = 0.8) +
  scale_color_manual(values = modality_colors) +
  scale_fill_manual(values = modality_colors) +
  guides(fill = FALSE) +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 16),
        legend.position = "bottom",
        text = element_text(size = 18),
        axis.title = element_text(size = 18),
        plot.title = element_text(size = 24, face = "bold")) +
  labs(x = "Word", y = "Expressibility", color = "Modality") +
  geom_hline(yintercept = 0.5, linetype = "blank")

plot_vocal_zoib <- ggplot(data = top_words_zoib_vocal, aes(x = English, y = fit, ymin = lwr, ymax = upr)) +
  geom_errorbar(aes(color = modality), size = 0.8, width = 0.4, alpha = 0.7) +
  geom_point(aes(color = modality), size = 4, alpha = 0.7) +
  geom_point(aes(y = avg_expressibility_norm, fill = modality), shape = 23, size = 3,
             stroke = 0.5, alpha = 0.8) +
  scale_color_manual(values = modality_colors) +
  scale_fill_manual(values = modality_colors) +
  guides(fill = FALSE) +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 16),
        legend.position = "bottom",
        text = element_text(size = 18),
        axis.title = element_text(size = 18),
        plot.title = element_text(size = 24, face = "bold")) +
  labs(x = "Word", y = "Expressibility", color = "Modality") +
  geom_hline(yintercept = 0.5, linetype = "blank")

plot_multimodal_zoib <- ggplot(data = top_words_zoib_multimodal, aes(x = English, y = fit, ymin = lwr, ymax = upr)) +
  geom_errorbar(aes(color = modality), size = 0.8, width = 0.4, alpha = 0.7) +
  geom_point(aes(color = modality), size = 4, alpha = 0.7) +
  geom_point(aes(y = avg_expressibility_norm, fill = modality), shape = 23, size = 3,
             stroke = 0.5, alpha = 0.8) +
  scale_color_manual(values = modality_colors) +
  scale_fill_manual(values = modality_colors) +
  guides(fill = FALSE) +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 16),
        legend.position = "bottom",
        text = element_text(size = 18),
        axis.title = element_text(size = 18),
        plot.title = element_text(size = 24, face = "bold")) +
  labs(x = "Word", y = "Expressibility", color = "Modality") +
  geom_hline(yintercept = 0.5, linetype = "blank")

# Display the separate plots
plot_gesture_zoib  # Plot for the gesture modality
plot_vocal_zoib    # Plot for the vocal modality
plot_multimodal_zoib  # Plot for the multimodal modality

# save the plot
ggsave(plot = plot_gesture_zoib, filename = paste0(figures,'plot_gesture_zoib.pdf'),
       width = 12, height = 8)
ggsave(plot = plot_vocal_zoib, filename = paste0(figures,'plot_vocal_zoib.pdf'),
       width = 12, height = 8)
ggsave(plot = plot_multimodal_zoib, filename = paste0(figures,'plot_multimodal_zoib.pdf'),
       width = 12, height = 8)
```

# Create word list for 1b

```{r wordlist 1b, echo=TRUE, message=FALSE, warning=FALSE}
# Make a list with unique top 20 words per modality from additive model
list1b <- unique(top_words_zoib$word)
```

This completes the analysis.
