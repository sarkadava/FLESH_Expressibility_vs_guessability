prior = priors_beta,
family = zero_one_inflated_beta(),
backend = "cmdstanr",
cores = 4,
chains = 4,
init = 0,
iter = 4000,
warmup = 2000,
seed = 42,
control = list(max_treedepth = 13,
adapt_delta = 0.99),
file = paste0(models, "mdl_zoib.rds"))
check_cmdstan_toolchain()
install_cmdstan(cores = 2)
set_cmdstan_path(PATH_TO_CMDSTAN)
cmdstan_path()
set_cmdstan_path("C:/Users/kadava/.cmdstan/cmdstan-2.36.0")
# https://journal.r-project.org/archive/2015/RJ-2015-019/
# https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#special-case-1-zero-one-inflated-beta-regression
# https://mvuorre.github.io/posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/#zoib-regression
# compute mdl
mdl_zoib <- brm(bf(expressibility_norm ~ 1 + modality + (1 + modality || word),
phi ~ modality + (1 + modality || word),
zoi ~ modality + (1 + modality || word),
coi ~ modality + (1 + modality || word)),
data = data,
prior = priors_beta,
family = zero_one_inflated_beta(),
backend = "cmdstanr",
cores = 4,
chains = 4,
init = 0,
iter = 4000,
warmup = 2000,
seed = 42,
control = list(max_treedepth = 13,
adapt_delta = 0.99),
file = paste0(models, "mdl_zoib.rds"))
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: source setup
# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())
#################### packages ####################
required_packages <- c("tidyverse", # data wrangling
"readxl",
"furrr",
"stringr",
"broom", # for tidy model outputs
"brms",
"cmdstanr",
"HDInterval", # package for credible interval computation
"tidybayes", # plotting
"ggpubr",
"loo", # model comparison
"BayesFactor", # model comparison
"lubridate") # for converting seconds to minutes
# if error with cmdstanr, use this
#install.packages("cmdstanr", repos = c('https://stan-dev.r-universe.dev', getOption("repos")))
# Loop over the packages and check if they are installed
for (package in required_packages) {
if (!require(package, character.only = TRUE)) {
# If the package is not installed, install it
install.packages(package)
# Load the package
library(package, character.only = TRUE)
}
}
# option for Bayesian regression models:
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())
colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
#list the folders
datasets  <- paste0(parentfolder, '/data/')
models   <- paste0(parentfolder, '/models/')
figures <- paste0(parentfolder, '/plots/')
scripts <- paste0(parentfolder, '/scripts/')
data <- read.csv(paste0(datasets, 'df_expressibility_dutch_final.csv'))
#suppressWarnings(source(paste0(scripts, "exp1a-preparation.R")))
# Chunk 3: all variables
summary(data)
# Chunk 4: filter rt
library(dplyr)
# length before
before <- nrow(data)
# extract mean and sd
mean_rt <- mean(data$response_time, na.rm = TRUE)
sd_rt <- sd(data$response_time, na.rm = TRUE)
# remove the ones that are 3 sd or more
data <- data %>%
filter(response_time < mean_rt + 3 * sd_rt)
# Remove data points less than 0.5 from response_time column
data <- subset(data, response_time >= 0.5)
# this is how many percent of raw data are left
100*nrow(data)/before
# this is how many percent was removed
100-(100*nrow(data)/before)
# we removed 1 % of data
# Chunk 5: straightlining
# response similarity
data <- data %>%
group_by(exp_ID) %>%
mutate(difference = round(abs(c(0,diff(expressibility))), 4))
# value 0 for each first line of each participant --> should be 215 values of 0
participants_diff_0 <- unique(data$exp_ID[data$difference == 0]) # and indeed there is 215 zeros
# filter them out
difference <- subset(data, difference != 0)
# create a row number column
difference <- difference %>%
mutate(row_number = row_number())
# summary statistics
hist(difference$difference, breaks = "FD", xlab = "Absolute Difference", main = "Distribution of Response Similarities")  # Plot histogram
mean_diff <- mean(data$difference)  # Calculate mean difference - 0.66
sd_diff <- sd(data$difference)  # Calculate standard deviation of difference - 0.541
# let's set a treshold
threshold <- mean_diff - 1*sd_diff
straightlining_data <- difference[difference$difference < threshold, ]  # Filter responses below the threshold
# Find rows where the values in column 2 are increasing by 1 within gesture modality
increasing_rows_g <- straightlining_data[c(NA, diff(straightlining_data$row_number)) == 1 & straightlining_data$modality == "gesture", ]
# within vocal modality
increasing_rows_v <- straightlining_data[c(NA, diff(straightlining_data$row_number)) == 1 & straightlining_data$modality == "vocal", ]
# within multimodal modality
increasing_rows_m <- straightlining_data[c(NA, diff(straightlining_data$row_number)) == 1 & straightlining_data$modality == "multimodal", ]
# It seems like everyone did their best
# Chunk 6: correlation with item mean
# Set the number of parallel workers
library(future)
plan(multicore)
# mean
data_mean <- data %>%
group_by(word, modality) %>%
summarize(mean = mean(expressibility, na.rm = TRUE))
# append the mean to the main df
data <- left_join(data, data_mean)
# df with each unique participant
ppt_corrs <- tibble(exp_ID = unique(data$exp_ID))
# column to be filled with correlation
ppt_corrs$r = numeric(nrow(ppt_corrs))
# compute correlations
for (i in 1:nrow(ppt_corrs)) {
# Extract subject into subset:
id <- ppt_corrs[i, ]$exp_ID
this_df <- filter(data, exp_ID == id)
# Compute and store correlation:
ppt_corrs[i, ]$r <- with(this_df,
cor(expressibility, mean,
use = 'complete.obs'))
}
# the distribution
library(ggplot2)
ppt_corrs %>%
ggplot(aes(x = r)) +
geom_density(fill = 'steelblue', alpha = 0.5) +
geom_vline(aes(xintercept = 0), linetype = 2) +
geom_vline(aes(xintercept = 0.1), col = 'darkgrey') + # threshold value r = 0.1
scale_y_continuous(expand = c(0, 0),
limits = c(0, 4)) +
theme_classic()
# the average of correlation
ppt_corrs %>%
summarize(r_M = mean(r),
r_SD = sd(r))
# order
ppt_corrs <- arrange(ppt_corrs, r)
ppt_corrs
# bad participants under threshold
bad_subs <- filter(ppt_corrs, r < 0.1) %>%
pull(exp_ID)
length(bad_subs) # 2 bads subjects
# how much %
length(bad_subs) / length(unique(data$exp_ID)) # 0.009 %
# Chunk 7: add info
library(readxl)
# load concept excel with info about PoS and SemCat
concepts <- read_excel(paste0(datasets, "concept_list_info.xlsx"))
concepts <- select(concepts, -c(PoS, SemanticCat, Info))
# create df that matches info from df data and concepts
data <- merge(data, concepts, by.x = "word", by.y = "Dutch")
# Because we have multiple translations, we have to choose one.
# As long as we do not have frequency data (for English), we need a work-around.
data$English <- as.factor(data$English)
data %>%
distinct(word, English) %>%
group_by(word) %>%
filter(n() > 1)
# From among those words, we need to choose the ones that fit better (for now, by hand).
# meat, shadow, hit, cry should stay (since I checked by hand that they are more frequent)
data <- data %>%
filter(!(English %in% c("flesh", "shade", "to beat", "to weep", "loud")))
data <- data %>% select(word, English, modality, expressibility, SemanticCat, SemanticSubcat, PoS, response_time, trial_start_time, end_time, duration, exp_ID, age, gender, natlang1, natlang2, natlang3, dialect, lang1, proficiency1, lang2, proficiency2, lang3, proficiency3, lang4, proficiency4, lang5, proficiency5, country, education, dyslexia, mobility, currentloc, sensorypref)
# Remove duplicate rows
data <- unique(data)
# Make them factors.
data$PoS <- as.factor(data$PoS)
data$SemanticCat <- as.factor(data$SemanticCat)
data$SemanticSubcat <- as.factor(data$SemanticSubcat)
# Chunk 8: n participants
length(unique(data$exp_ID))
# 215
# Chunk 9: meta participants
# Gender distribution
data %>%
distinct(exp_ID, gender) %>% # show a list of gender of each participant
count(gender) %>% # make a cumulative count
arrange(desc(n)) # arrange in descending order
#"1" = "female", "2" = "male", "3" = "diverse", "4" = NA_character
# Age distribution
data %>% summarize(
min_age = min(age),
max_age = max(age),
mean_age = mean(age),
median_age = median(age)
)
# Education distribution
data %>%
distinct(exp_ID, education) %>%
count(education) %>%
arrange(desc(n))
# Current location
data %>%
distinct(exp_ID, currentloc) %>%
count(currentloc) %>%
arrange(desc(n))
# Country
data %>%
distinct(exp_ID, country) %>%
count(country) %>%
arrange(desc(n))
# Dyslexia
data %>%
distinct(exp_ID, dyslexia) %>%
count(dyslexia) %>%
arrange(desc(n))
# Mobility
data %>%
distinct(exp_ID, mobility) %>%
count(mobility) %>%
arrange(desc(n))
# Second L1
data %>%
distinct(exp_ID, natlang2) %>%
count(natlang2) %>%
arrange(desc(n))
# Third L1
data %>%
distinct(exp_ID, natlang3) %>%
count(natlang3) %>%
arrange(desc(n))
# L2
data %>%
distinct(exp_ID, lang1) %>%
count(lang1) %>%
arrange(desc(n))
# L3
data %>%
distinct(exp_ID, lang2) %>%
count(lang2) %>%
arrange(desc(n))
# L4
data %>%
distinct(exp_ID, lang3) %>%
count(lang3) %>%
arrange(desc(n))
# Sensory preference
data %>%
distinct(exp_ID, sensorypref) %>%
count(sensorypref) %>%
arrange(desc(n))
# 1 - hearing only
# 11 - hearing + visual + taste
# 14 - all: hearing + visual + taste + other
# 2 - visual only
# 3 - taste only
# 4 - other
# 5 - hearing + visual
# 6 - hearing + taste
# 7 - hearing + other
# 8 - visual + taste
# 9 - visual + other
# Chunk 10: participant time
library(lubridate)
# Total duration in minutes
data %>% summarize(
min_duration = seconds_to_period(min(duration, na.rm = TRUE)),
max_duration = seconds_to_period(max(duration, na.rm = TRUE)),
mean_duration = seconds_to_period(mean(duration, na.rm = TRUE)),
median_duration = seconds_to_period(median(duration, na.rm = TRUE))
)
# Single concept response time in seconds
data %>% summarize(
min_response_time = min(response_time, na.rm = TRUE),
max_response_time = max(response_time, na.rm = TRUE),
mean_response_time = mean(response_time, na.rm = TRUE)
)
# Chunk 11: n words
length(unique(data$word))
# Chunk 12: meta words
# PoS distribution
data %>%
distinct(word, PoS) %>%
count(PoS) %>%
arrange(desc(n))
# Semantic category distribution
data %>%
distinct(word, SemanticCat) %>%
count(SemanticCat) %>%
arrange(desc(n))
# Semantic subcategory distribution
data %>%
distinct(word, SemanticSubcat) %>%
count(SemanticSubcat) %>%
arrange(desc(n))
# Later also analyses on the different ratings
# Chunk 13: modality ratings
# How many times each word was rated in each modality
word_modality_counts <- data %>%
count(word, modality) %>%
arrange(desc(n))
word_modality_counts
# Which words have the fewest ratings per modality
word_modality_counts %>% filter(n == min(n))
# 16 ratings is the minimum
# Chunk 14: expressibility rating
# Calculate average raw expressibility per word per modality.
expressibility_avg <- data %>%
group_by(word, modality) %>%
summarize(avg_expressibility = mean(expressibility, na.rm = TRUE), .groups = "drop") %>%
arrange(desc(avg_expressibility))
expressibility_avg
# Create datasets for each modality (sorted and with ranking)
gesture_avg <- expressibility_avg %>%
filter(modality == "gesture") %>%
arrange(desc(avg_expressibility)) %>%
mutate(n = row_number())
gesture_avg
multimodal_avg <- expressibility_avg %>%
filter(modality == "multimodal") %>%
arrange(desc(avg_expressibility)) %>%
mutate(n = row_number())
multimodal_avg
vocal_avg <- expressibility_avg %>%
filter(modality == "vocal") %>%
arrange(desc(avg_expressibility)) %>%
mutate(n = row_number())
vocal_avg
# Add the ranking information to expressibility_avg
expressibility_avg <- merge(expressibility_avg, gesture_avg[, c("word", "modality", "n")], by = c("word", "modality"), all.x = TRUE)
expressibility_avg <- merge(expressibility_avg, vocal_avg[, c("word", "modality", "n")], by = c("word", "modality"), all.x = TRUE)
expressibility_avg <- merge(expressibility_avg, multimodal_avg[, c("word", "modality", "n")], by = c("word", "modality"), all.x = TRUE)
expressibility_avg$n <- coalesce(expressibility_avg$n.x, expressibility_avg$n.y, expressibility_avg$n)
expressibility_avg <- expressibility_avg %>%
select(-c(n.x, n.y))
# Estimate the divergence in ratings by expressibility
expressibility_avg <- expressibility_avg %>%
group_by(word) %>%
mutate(diff_expr = max(avg_expressibility) - min(avg_expressibility)) %>%
ungroup()
# Estimate the divergence in ratings by ranking
expressibility_avg <- expressibility_avg %>%
group_by(word) %>%
mutate(diff_rank = max(n) - min(n)) %>%
ungroup()
# Chunk 15: print expressibility_avg
# expressibility average by difference in expressibility
expressibility_avg %>%
arrange(desc(diff_expr))
# expressibility average by difference in rank
expressibility_avg %>%
arrange(desc(diff_rank))
# Chunk 16: scale expressibility
# z-score
data$expressibility_z <- scale(data$expressibility)
# creates values between 0 and 1
data$expressibility_norm <- (data$expressibility-min(data$expressibility))/(max(data$expressibility)-min(data$expressibility))
# Chunk 17: mean from normalized expressibility
# Calculate average raw expressibility per word per modality.
expressibility_norm_avg <- data %>%
group_by(word, modality) %>%
summarize(avg_expressibility_norm = mean(expressibility_norm, na.rm = TRUE), .groups = "drop") %>%
arrange(desc(avg_expressibility_norm))
expressibility_norm_avg
# Create datasets for each modality (sorted and with ranking)
gesture_norm_avg <- expressibility_norm_avg %>%
filter(modality == "gesture") %>%
arrange(desc(avg_expressibility_norm)) %>%
mutate(n = row_number())
gesture_norm_avg
multimodal_norm_avg <- expressibility_norm_avg %>%
filter(modality == "multimodal") %>%
arrange(desc(avg_expressibility_norm)) %>%
mutate(n = row_number())
multimodal_norm_avg
vocal_norm_avg <- expressibility_norm_avg %>%
filter(modality == "vocal") %>%
arrange(desc(avg_expressibility_norm)) %>%
mutate(n = row_number())
vocal_norm_avg
# Add the ranking information to expressibility_avg
expressibility_norm_avg <- merge(expressibility_norm_avg, gesture_norm_avg[, c("word", "modality", "n")], by = c("word", "modality"), all.x = TRUE)
expressibility_norm_avg <- merge(expressibility_norm_avg, multimodal_norm_avg[, c("word", "modality", "n")], by = c("word", "modality"), all.x = TRUE)
expressibility_norm_avg <- merge(expressibility_norm_avg, vocal_norm_avg[, c("word", "modality", "n")], by = c("word", "modality"), all.x = TRUE)
expressibility_norm_avg$n <- coalesce(expressibility_norm_avg$n.x, expressibility_norm_avg$n.y, expressibility_norm_avg$n)
expressibility_norm_avg <- expressibility_norm_avg %>%
select(-c(n.x, n.y))
# Estimate the divergence in ratings by expressibility
expressibility_norm_avg <- expressibility_norm_avg %>%
group_by(word) %>%
mutate(diff_expr = max(avg_expressibility_norm) - min(avg_expressibility_norm)) %>%
ungroup()
# Estimate the divergence in ratings by ranking
expressibility_norm_avg <- expressibility_norm_avg %>%
group_by(word) %>%
mutate(diff_rank = max(n) - min(n)) %>%
ungroup()
# save this df
write.csv(expressibility_norm_avg, paste0(datasets, "df_express_dutch_norm_avg.csv"))
# Chunk 18: violin
# Create a violin plot with box plot overlay
ggplot(data, aes(x = modality, y = expressibility_norm)) +
geom_violin(fill = "lightgray", color = "black", trim = FALSE) +
geom_boxplot(width = 0.2, fill = "white", color = "black", outlier.shape = NA) +
labs(x = "Modality", y = "Expressibility Norm") +
theme_minimal()
# Create a scatter plot with a loess smoother
ggplot(data, aes(x = modality, y = expressibility_norm)) +
geom_point(position = position_jitter(width = 0.1), size = 2) +
geom_smooth(method = "loess", se = FALSE) +
labs(x = "Modality", y = "Expressibility Norm") +
theme_minimal()
ggplot(data, aes(x = expressibility_norm, fill = modality)) +
geom_density(alpha = 0.2) +
scale_fill_manual(values = colorBlindBlack8) +
labs(x = "Expressibility (normalized)", y = "Density") +
theme_minimal()
ggplot(data, aes(x = expressibility_norm, fill = PoS)) +
geom_density(alpha = 0.2) +
scale_fill_manual(values = colorBlindBlack8) +
labs(x = "PoS", y = "Density") +
theme_minimal()
# Chunk 19: contrast coding
# Gender
data$gender <- as.factor(data$gender)
contrasts(data$gender)
# Modality
data$modality <- as.factor(data$modality)
contrasts(data$modality)
# Word
data$word <- as.character(data$word)
data$word <- as.factor(data$word)
# Chunk 20: adapt levels
library(forcats)
levels(data$modality)
# make multimodality the baseline
levels(data$PoS)
# make verb the baseline
levels(data$posneg)
# negative is baseline
data <- data %>%
mutate(modality = fct_relevel(modality, "multimodal"),
PoS = fct_relevel(PoS, "verb"))
# Chunk 21: beta priors
library(brms)
get_prior(formula = expressibility ~ 1 + modality +
(1 + modality || word),
data = data,
family = zero_inflated_beta())
# Define the priors
priors_beta <- c(
prior('normal(0, 1)', class = 'Intercept', lb = 0, ub = 1),
prior('normal(0, 0.5)', class = 'b')  # Priors for other predictors
)
priors_beta_narrow <- c(
prior('normal(0.5, 0.5)', class = 'Intercept', lb = 0, ub = 1),
prior('normal(0, 0.2)', class = 'b')
)
# Chunk 23: mdl zoib 1 check
mdl_zoib <- readRDS(paste0(models, "mdl_zoib.rds"))
# https://journal.r-project.org/archive/2015/RJ-2015-019/
# https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#special-case-1-zero-one-inflated-beta-regression
# https://mvuorre.github.io/posts/2019-02-18-analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/#zoib-regression
# compute mdl
mdl_zoib <- brm(bf(expressibility_norm ~ 1 + modality + (1 + modality || word),
phi ~ modality + (1 + modality || word),
zoi ~ modality + (1 + modality || word),
coi ~ modality + (1 + modality || word)),
data = data,
prior = priors_beta,
family = zero_one_inflated_beta(),
backend = "cmdstanr",
cores = 4,
chains = 4,
init = 0,
iter = 4000,
warmup = 2000,
seed = 42,
control = list(max_treedepth = 13,
adapt_delta = 0.99),
file = paste0(models, "mdl_zoib.rds"))
# compress the model more
saveRDS(mdl_zoib, file = paste0(models, "mdl_zoib.rds"), compress = "xz")
mdl_zoib <- readRDS(paste0(models, "mdl_zoib.rds"))
summary(mdl_zoib)
pp_check(mdl_zoib, ndraws = 100)
conditional_effects(mdl_zoib, sample_prior = "only")
# run loo mdl
if (file.exists(paste0(models, "mdl_zoib_loo.rds"))) {
mdl_zoib_loo <- readRDS(paste0(models, "mdl_zoib_loo.rds"))
} else {
mdl_zoib_loo <- loo(mdl_zoib)
saveRDS(mdl_zoib_loo, paste0(models, "mdl_zoib_loo.rds"))
}
