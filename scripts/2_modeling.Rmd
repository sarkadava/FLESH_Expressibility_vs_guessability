---
title: "The relationship between expectation and performance: Methodological evaluation"
author: "Aleksandra Ćwiek, Wim Pouw, Susanne Fuchs, Šárka Kadavá"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  rmarkdown::html_document:
    theme: readable
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float: yes
    df_print: paged
    code_folding: hide
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '3'
  html_notebook:
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This is the modeling of expressibility vs. guessability, preregistered
at [AsPredicted](https://aspredicted.org/kmry-vx5s.pdf).

This script uses the output of `1_descriptive.Rmd` and prodcues all the
values and plots reported in the paper.

# Data preparation

## Setup

```{r source setup, echo = TRUE, message=FALSE, warning = FALSE}

########## folders ##########
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

parentfolder <- dirname(getwd())

rawdata       <- paste0(parentfolder, '/rawdata/');
dataset       <- paste0(parentfolder, '/dataset/');
models        <- paste0(parentfolder, '/models/');
plots         <- paste0(parentfolder, '/plots/');
scripts       <- paste0(parentfolder, '/scripts/');

########## source file ##########

#rmarkdown::render("1_descriptive.Rmd", envir = globalenv(), clean = FALSE)

#################### packages ####################
# Data Manipulation
library(tidyverse);

# Plotting
library(corrplot);
library(cowplot);
library(ggdist);
library(gridExtra);
library(sjPlot);
library(knitr);

# Bayesian
library(brms);
library(cmdstanr);
library(emmeans);
library(posterior);
library(bayesplot);
library(tidybayes);
library(bayestestR);

theme_set(theme_tidybayes() + panel_border());


# use all available cores for parallel computing
options(mc.cores = parallel::detectCores());

colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                       "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

```

## Load in data frames

```{r read metadata, echo=TRUE, message=FALSE, warning=FALSE}
# Load data frame
df <- read_csv(paste0(dataset, "df.csv"))

# Load concept list
concepts <- readxl::read_excel(paste0(dataset, "conceptlist_info.xlsx"))

# Load expressibility German
expr_german <- read_csv(paste0(dataset, "expressibility_german.csv"))

# Load expressibility Dutch
expr_dutch <- read_csv(paste0(dataset, "expressibility_dutch.csv"))

```

## Helper functions

```{r}
# Helper function for overall back-transformation.
# Note: In our parameterization, both the mu and zoi intercepts are on the logit scale.
back_transform_zoib <- function(eta, zoi, coi = 1) {
  p_zoi <- plogis(zoi)       # back-transform the inflation parameter
  # Overall predicted value: with probability p_zoi the observation comes from the beta part (with mean plogis(eta))
  # and with probability 1-p_zoi it is inflated to coi.
  p_zoi * plogis(eta) + (1 - p_zoi) * coi
}

# Helper function for LOO:
get_save_loo <- function(model, file_name) {
  file_path <- paste0(models, file_name)
  if (!file.exists(file_path)) {
    cat("Computing LOO for", file_name, "...\n")
    loo_val <- loo(model)
    saveRDS(loo_val, file = file_path)
  } else {
    cat("Loading saved LOO for", file_name, "...\n")
    loo_val <- readRDS(file_path)
  }
  return(loo_val)
}

# Helper function for WAIC:
get_save_waic <- function(model, file_name) {
  file_path <- paste0(models, file_name)
  if (!file.exists(file_path)) {
    cat("Computing WAIC for", file_name, "...\n")
    waic_val <- waic(model)
    saveRDS(waic_val, file = file_path)
  } else {
    cat("Loading saved WAIC for", file_name, "...\n")
    waic_val <- readRDS(file_path)
  }
  return(waic_val)
}

# Log odds to probability (logistic regression intercept):
lo2p <- function(x){ o <- exp(x); return (o/(1+o));}

# Log odds to odds ratio change as percent (logistic regression slopes):
lo2or <- function(x){ o <- exp(x); if(x < 0){ return (-(1-o)) } else { return (o-1) };}

# Log odds to odds ratio change between level values (logistic regression slopes):
#lo2ps <- function(a,b){if(b > 0){ return (plogis(a+b)-plogis(a)) }else{ return (-(plogis(a)-plogis(a+b))) }}
lo2ps <- function(a,b){ plogis(a+b) - plogis(a) }

# Scientific notation using Markdown conventions (inspired from https://www.r-bloggers.com/2015/03/scientific-notation-for-rlatex/):
scinot <- function(xs, digits=2, pvalue=TRUE)
{
  scinot1 <- function(x)
  {
    sign <- "";
    if(x < 0)
    {
      sign <- "-";
      x <- -x;
    }
    exponent <- floor(log10(x));
    if(exponent && pvalue && exponent < -3) 
    {
      xx <- round(x / 10^exponent, digits=digits);
      e <- paste0("×10^", round(exponent,0), "^");
    } else 
    {
      xx <- round(x, digits=digits+1);
      e <- "";
    }
    paste0(sign, xx, e);
  }
  vapply(xs, scinot1, character(1));
}

# Escape * in a string:
escstar <- function(s)
{
  gsub("*", "\\*", s, fixed=TRUE);
}

# Figure and Table caption adapted from https://stackoverflow.com/questions/37116632/rmarkdown-html-number-figures: 
outputFormat = opts_knit$get("rmarkdown.pandoc.to"); # determine the output format of the document
if( is.null(outputFormat) ) outputFormat = ""; # probably not run within knittr
capTabNo = 1; capFigNo = 1; # figure and table caption numbering, for HTML do it manually
#Function to add the Table Number
capTab = function(x)
{
  if(outputFormat == 'html'){
    x = paste0("**Table ",capTabNo,".** ",x,"")
    capTabNo <<- capTabNo + 1
  }; x
}
#Function to add the Figure Number
capFig = function(x, show_R_version=TRUE, show_package_versions=NULL, is_map=FALSE)
{
  if(outputFormat == 'html')
  {
    x <- paste0("**Figure ",capFigNo,".** ",x,"");
    if( show_R_version || (!is.null(show_package_versions) && length(show_package_versions) > 0) )
    {
      x <- paste0(x, " Figure generated using ");
      if( show_R_version ) x <- paste0(x, stringr::str_replace(R.version.string, stringr::fixed("R "), "[`R`](https://www.r-project.org/) "));
      if( !is.null(show_package_versions) && length(show_package_versions) > 0 )
      {
        x <- paste0(x, ifelse( show_R_version, " and ", " "));
        x <- paste0(x, ifelse( length(show_package_versions) > 1, "packages ", "package "));
        x <- paste0(x, paste0(vapply(show_package_versions, function(x) paste0("`",x,"`", " (version ", packageVersion(x),")"), character(1)), collapse=", "), ".");
      }
      if( is_map ) x <- paste0(x, " Maps are using public domain data from the [Natural Earth project](https://www.naturalearthdata.com/) as provided by the `R` package `maps`.");
    }
    capFigNo <<- capFigNo + 1;
  }; 
  x;
}
```

## Information from the preregistration

We will use weakly informative priors for the fixed and random effect,
just informing the models about the ranges of the scale of the outcome
variable.

The `guessability_match` is a categorical variable and will be coded as
`0` for incorrect guess and `1` for correct guess. The
`guessability_similarity` will be a continuous variable expressing the
cosine similarity of guesser's answer to target (estimated using
ConceptNet). The `N_repetition` encompasses the data from the second
half of the experiment only (where participants could repeat the
production) and represents a number of repetitions until the production
was correctly guessed or until a maximum of three productions was
reached. The `experiment_part` is a categorical variable coded as `1`
and `2`, for the respective halves of the experiment. `Modality` is a
categorical variable with three levels: `combined`, `gesture`, and
`vocalization`.

For `N_repetion`, we will start with a Poisson model and check residuals
as well as compare the mean and variance. If overdispersion is present,
we will switch to negbinomial family.

For the continuous `guessability_similarity` outcome variable, we will
test the best fitting family by inspecting the distribution of the
outcome variable. We do this because the data might not be normally
distributed. We will choose an optimal family among gamma, lognormal, or
skew_normal if the normal is inappropriate.

# Prepare data

## Sample size

Calculate the effective sample size for modeling.

```{r message=FALSE, warning=FALSE, include=FALSE}
nrow(df)

df %>%
  group_by(concept) %>%
  summarise(frequency = n()) %>%
  arrange(desc(frequency)) %>% 
  print(n = 100)
```

There are a total of `r nrow(df)` novel productions, from that,
`r nrow(filter(df, exp_part == "1"))` in experiment part 1 and
`r nrow(filter(df, exp_part == "2"))` in experiment part 2.

In experiment part 2, there are
`r nrow(filter(df, correction == "0" & exp_part == "2"))` data points of
the very first production,
`r nrow(filter(df, correction == "1" & exp_part == "2"))` data points of
the first repetition/correction (i.e., second production), and
`r nrow(filter(df, correction == "2" & exp_part == "2"))` data points of
the second repetition/correction (i.e., third production).

Across modalities, there are
`r nrow(filter(df, modality == "combined"))` productions in combined
(`r nrow(filter(df, modality == "combined" & exp_part == "1"))` in part
1 vs. `r nrow(filter(df, modality == "combined" & exp_part == "2"))` in
part 2), `r nrow(filter(df, modality == "gesture"))` in gesture
(`r nrow(filter(df, modality == "gesture" & exp_part == "1"))` in part 1
vs. `r nrow(filter(df, modality == "gesture" & exp_part == "2"))` in
part 2), and `r nrow(filter(df, modality == "vocal"))` in vocal modality
(`r nrow(filter(df, modality == "vocal" & exp_part == "1"))` in part 1
vs. `r nrow(filter(df, modality == "vocal" & exp_part == "2"))` in part
2).

## Cosine similarity

The cosine similarity was calculated with ConceptNet. Some pairs (N =
130, \~2% of the data) were not available in ConceptNet and for those we
asked participants (HOW MANY) to rate the perceived similarity. Validation data from 25 independent raters showed a strong correlation (r = 0.725, BF10 = 7.724e+17) between perceived similarity and cosine similarity. The data
reviewed below already entails the perceived similarity.

Cosine similarity is inherently on -1 to 1 scale. As provided
[here](https://github.com/deepinsight/insightface/issues/917):

> Cosine similarity is like an inner product. If angle between two
> vector is larger than 90 degree, the value is negative, and that means
> that two faces(features) are clearly distinguishable.

Meaning that a negative value is provided for concepts that are clearly
different (i.e., not confusable).

In order to be able to model the values with a wider choice of families
(including zero-one inflated beta), we transformed the cosine similarity
with

`(df$cosine_similarity + 1) / 2`

This transformation resulted in data distributed in the following way

```{r}
summary(df$cosine_similarity)
```

Below, we break down the missing values and look at data distribution to
choose an appropriate family.

```{r}
# Total missing values for cosine_similarity and its percentage
cat("Total missing values for cosine_similarity: ", sum(is.na(df$cosine_similarity)), "\n")
cat("Percentage missing: ", round(100 * sum(is.na(df$cosine_similarity)) / nrow(df), 1), "%\n\n")

# Breakdown by modality:
cat("Missing values by modality:\n")
df %>% 
  group_by(modality) %>% 
  summarise(
    missing = sum(is.na(cosine_similarity)),
    total = n(),
    percent_missing = round(100 * sum(is.na(cosine_similarity)) / n(), 1)
  ) %>% 
  print(n = Inf)

cat("\nBreakdown by experiment part (exp_part):\n")
df %>% 
  group_by(exp_part) %>% 
  summarise(
    missing = sum(is.na(cosine_similarity)),
    total = n(),
    percent_missing = round(100 * sum(is.na(cosine_similarity)) / n(), 1)
  ) %>% 
  print(n = Inf)

cat("\nBreakdown by repetition (correction):\n")
df %>% 
  group_by(correction) %>% 
  summarise(
    missing = sum(is.na(cosine_similarity)),
    total = n(),
    percent_missing = round(100 * sum(is.na(cosine_similarity)) / n(), 1)
  ) %>% 
  print(n = Inf)
```

Let us inspect the distribution of cosine similarity

```{r}
ggplot(df, aes(x = cosine_similarity)) +
  geom_density(fill = colorBlindBlack8[6]) +
  theme_minimal() +
  labs(x = "Cosine Similarity", y = "Density")

moments::skewness(df$cosine_similarity)
```

The distribution is fairly bimodal. The peak at 1 results from obvious
100% match cases that correspond to binary guess = 1. Then there is
another peak around 0.5 which results from non-ideal guesses.

```{r}
ggplot(df, aes(x = cosine_similarity, fill = factor(guess_binary))) +
  geom_density(alpha = 0.6) +  # alpha to make the fill slightly transparent
  theme_minimal() +
  labs(x = "Cosine Similarity", y = "Density", fill = "Guess binary") +
  scale_fill_manual(values = c(colorBlindBlack8[6], colorBlindBlack8[7])) 
```

The family that will fit this data best is one-inflated beta. In the
brms framework, we don’t have a separate family for one‐inflated beta,
but we can force it by fixing the zero‐inflation parameter (zoi) to 1
(or nearly 1). That way, the model assumes no probability mass at 0, and
all of the inflation is allocated to the one‐inflation (coi) component.

Let's compute the summary statistics of cosine similarity grouped by
guess_binary

```{r}
df %>%
  group_by(guess_binary) %>%
  summarise(
    mean_cosine_similarity = mean(cosine_similarity, na.rm = TRUE),
    sd_cosine_similarity = sd(cosine_similarity, na.rm = TRUE),
    min_cosine_similarity = min(cosine_similarity, na.rm = TRUE),
    max_cosine_similarity = max(cosine_similarity, na.rm = TRUE),
    median_cosine_similarity = median(cosine_similarity, na.rm = TRUE),
    n = n()  # Count of observations
  )

```

## Variables

Our hypotheses revolve around a set of fixed and random predictors for
different outcome variables.

The outcome variables in the data frame `df` are:

-   `guess_binary`: a *binary categorical* variable coded as `0` for
    incorrect guess and `1` for correct guess
-   `cosine_similarity`: a *continuous* variable expressing the cosine
    similarity of guesser's answer to target (estimated using
    ConceptNet)
-   `correction`: a *categorical* variable with three levels `0` for the
    first production, `1` for the first correction, `2` for the second
    correction

The predictor variables in the data frame `df` are:

-   `expressibility`: a *continuous* variable expressing the rated
    posterior expressibility, set for each concept in each modality
-   `exp_part`: a *binary categorical* variable coded as `1` for the
    first experiment part and `2` for the second experiment part
-   `modality`: a *categorical* variable with three levels `combined`
    for both gesture and vocal, `gesture` for gesture only, `vocal` for
    vocal only

The random variables in the data frame `df` are:

-   `concept`: a *categorical* variable for individual concepts (a total
    of `r unique(df$concept)`)
-   `dyad`: a *categorical* variable for individual dyads (a total of
    `r unique(df$dyad)`)
-   `participant_ID` (nested within `dyad`): a *categorical* variable
    for individual participants (a total of
    `r unique(df$participant_ID)`)

Preparing the variables

```{r}
# Outcome variables
#df$guess_binary <- factor(df$guess_binary, levels = c("0", "1")); contrasts(df$guess_binary) <- c(-0.5, +0.5)
## 'cosine_similarity' is continuous, no need to transform it
#df$correction <- factor(df$correction, levels = c("0", "1", "2"))

# Predictor variables
## 'expressibility' is continuous, no transformation needed
df$exp_part <- factor(df$exp_part, levels = c("1", "2")); contrasts(df$exp_part) <- c(-0.5, +0.5)
df$modality <- factor(df$modality, levels = c("combined", "gesture", "vocal"))

# Random variables
df$concept <- factor(df$concept)
df$dyad <- factor(df$dyad)
df$participant_ID <- factor(df$participant_ID)


# Check the balance between the levels

df %>% count(guess_binary) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 
# imbalanced 0: 67.5%, 1: 32.5%

df %>% count(correction) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 
# imbalanced 0: 66.2%, 1: 19.9%, 2: 13.9%

df %>% count(exp_part) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 
# imbalanced 1: 33%, 2: 67%

df %>% count(modality) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 
# pretty balanced, combined: 31%, gesture: 31.1%, vocal: 37.9%

# Random variables just if someone is interested in checking
# df %>% count(concept) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 
# df %>% count(dyad) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 
# df %>% count(participant_ID) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 

```

# Intercept-only guessability

## Guess similarity

Preregistered model:

`guessability_similarity ~ 1 + (1 | dyad/participant) + (1 | concept)`

### Model

Now, let’s build the intercept-only model for the continuous outcome,
cosine_similarity. The model uses the same random effects structure.
Because of the data distribution, we use a zero-one inflated beta model
minimizing the zoi component.

#### Best-fit

##### Nested

With priors for ZOIB

```{r}
mdl_null_similarity_nested_pior <- brm(
  bf(
    cosine_similarity ~ 1 + (1 | dyad/participant_ID) + (1 | concept),
    phi ~ 1,
    zoi ~ 1,
    coi ~ 1
  ),
  data = df,
  family = zero_one_inflated_beta(),
  prior = c(
    # Priors for the main (mu) component:
    prior(normal(0, 5), class = "Intercept"),
    prior(normal(0, 2), class = "sd"),
    # Prior for the phi parameter (precision)
    prior(normal(0, 5), class = "Intercept", dpar = "phi"),
    # Force zoi to be essentially 1, meaning P(y = 0) ~ 0:
    prior(normal(10, 0.001), class = "Intercept", dpar = "zoi"),
    # A neutral prior for coi: centered at 0 (which implies plogis(0)=0.5) with SD=2.
    prior(normal(0, 2), class = "Intercept", dpar = "coi")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12, adapt_delta = 0.99),
  file = paste0(models, "mdl_null_similarity_nested_prior.rds")
)

# if we need to compress the model more
#saveRDS(mdl_null_similarity_nested_prior, file = paste0(models, "mdl_null_similarity_nested_prior.rds"), compress = "xz")

mdl_null_similarity_nested_prior <- readRDS(paste0(models, "mdl_null_similarity_nested_prior.rds"))
```

Without priors for ZOIB

```{r}
mdl_null_similarity_nested <- brm(
  bf(
    cosine_similarity ~ 1 + (1 | dyad/participant_ID) + (1 | concept),
    phi ~ 1,
    zoi ~ 1,
    coi ~ 1
  ),
  data = df,
  family = zero_one_inflated_beta(),
  prior = c(
    # Priors for the main (mu) component:
    prior(normal(0, 5), class = "Intercept"),
    prior(normal(0, 2), class = "sd")#,
    # Prior for the phi parameter (precision)
    #prior(normal(0, 5), class = "Intercept", dpar = "phi"),
    # Force zoi to be essentially 1, meaning P(y = 0) ~ 0:
    #prior(normal(10, 0.001), class = "Intercept", dpar = "zoi"),
    # A neutral prior for coi: centered at 0 (which implies plogis(0)=0.5) with SD=2.
    #prior(normal(0, 2), class = "Intercept", dpar = "coi")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12, adapt_delta = 0.99),
  file = paste0(models, "mdl_null_similarity_nested.rds")
)

# if we need to compress the model more
#saveRDS(mdl_null_similarity_nested, file = paste0(models, "mdl_null_similarity_nested.rds"), compress = "xz")

mdl_null_similarity_nested <- readRDS(paste0(models, "mdl_null_similarity_nested.rds"))
```

##### No nesting

```{r}
mdl_null_similarity_simple <- brm(
  bf(
    cosine_similarity ~ 1 + (1 | dyad) + (1 | concept),
    phi ~ 1,
    zoi ~ 1,
    coi ~ 1
  ),
  data = df,
  family = zero_one_inflated_beta(),
  prior = c(
    # Priors for the main (mu) component:
    prior(normal(0, 5), class = "Intercept"),
    prior(normal(0, 2), class = "sd")#,
    # Prior for phi (precision)
    #prior(normal(0, 5), class = "Intercept", dpar = "phi"),
    # Force zoi nearly 1 so that there’s no mass at 0:
    #prior(normal(10, 0.001), class = "Intercept", dpar = "zoi"),
    # A neutral prior for coi: centered at 0 (which implies plogis(0)=0.5) with SD=2.
    #prior(normal(0, 2), class = "Intercept", dpar = "coi")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12, adapt_delta = 0.99),
  file = paste0(models, "mdl_null_similarity_simple.rds")
)

# if we need to compress the model more
#saveRDS(mdl_null_similarity_simple, file = paste0(models, "mdl_null_similarity_simple.rds"), compress = "xz")

mdl_null_similarity_simple <- readRDS(paste0(models, "mdl_null_similarity_simple.rds"))
```

##### Compare

```{r}
mdl_null_similarity_nested_prior
mdl_null_similarity_nested
mdl_null_similarity_simple
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_null_similarity_nested_prior_R2 <- bayes_R2(mdl_null_similarity_nested_prior)
# mdl_null_similarity_nested_R2 <- bayes_R2(mdl_null_similarity_nested)
# mdl_null_similarity_simple_R2 <- bayes_R2(mdl_null_similarity_simple)
# 
# # Save the R² output
# saveRDS(mdl_null_similarity_nested_prior_R2, file = paste0(models, "mdl_null_similarity_nested_prior_R2.rds"))
# saveRDS(mdl_null_similarity_nested_R2, file = paste0(models, "mdl_null_similarity_nested_R2.rds"))
# saveRDS(mdl_null_similarity_simple_R2, file = paste0(models, "mdl_null_similarity_simple_R2.rds"))

mdl_null_similarity_nested_prior_R2 <- readRDS(paste0(models, "mdl_null_similarity_nested_prior_R2.rds"))
mdl_null_similarity_nested_R2 <- readRDS(paste0(models, "mdl_null_similarity_nested_R2.rds"))
mdl_null_similarity_simple_R2 <- readRDS(paste0(models, "mdl_null_similarity_simple_R2.rds"))

mdl_null_similarity_nested_prior_R2 # barely any explanatory power
mdl_null_similarity_nested_R2
mdl_null_similarity_simple_R2
```

All the models are very weak in explaining the data, however the one
with the ZOIB prior `mdl_null_similarity_nested_prior` is extremely
weak. One argument not to use the priors for family-specific parameters.

```{r}
# Compute and save LOO for both models:
loo_nested_prior  <- get_save_loo(mdl_null_similarity_nested_prior, "mdl_null_similarity_nested_prior_loo.rds")
loo_nested  <- get_save_loo(mdl_null_similarity_nested, "mdl_null_similarity_nested_loo.rds")
loo_simple  <- get_save_loo(mdl_null_similarity_simple, "mdl_null_similarity_simple_loo.rds")

# Compute and save WAIC for both models:
waic_nested_prior <- get_save_waic(mdl_null_similarity_nested_prior, "mdl_null_similarity_nested_prior_waic.rds")
waic_nested <- get_save_waic(mdl_null_similarity_nested, "mdl_null_similarity_nested_waic.rds")
waic_simple <- get_save_waic(mdl_null_similarity_simple, "mdl_null_similarity_simple_waic.rds")

# Compare LOO and WAIC between models:
print(loo_compare(loo_nested_prior, loo_nested))
print(loo_compare(loo_nested, loo_simple))
print(loo_nested_prior)
print(loo_nested)
print(loo_simple)
print(waic_nested)
print(waic_simple)

```

Our model comparisons using leave‐one‐out cross-validation revealed that
the model with explicit ZOIB priors and fully nested random effects
performed extremely poorly relative to our simpler specifications. In
particular, the model with nested random effects and explicit ZOIB
priors had an estimated elpd_loo of `r loo_nested_prior$estimates[1,1]`
(SE `r loo_nested_prior$estimates[1,2]`), which is about
`r loo_nested_prior$estimates[1,1] - loo_nested$estimates[1,1]` (SE
`r loo_nested_prior$estimates[1,2] - loo_nested$estimates[1,2]`) points
worse than the nested model without these additional priors.
Furthermore, when comparing the nested model (elpd_loo ≈
`r loo_nested$estimates[1,1]`) to a simpler model that omitted the extra
nesting `dyad/participant` and only a group-level effect by `dyad`
(elpd_loo ≈ `r loo_simple$estimates[1,1]`), the difference was
negligible at only about
`r loo_nested$estimates[1,1] - loo_simple$estimates[1,1]` (SE
`r loo_nested$estimates[1,2] - loo_simple$estimates[1,2]`) points.

In light of these results, we have decided to proceed **without the explicit ZOIB priors and without the additional nesting**, the latter
contrary to our preregistration. The simpler model not only avoids
potential overparameterization and convergence issues but also yields
nearly identical predictive performance. This choice adheres to the
principle of parsimony while maintaining transparency about our
statistical decisions.

### Best-fit: "Simple"; no ZOIB priors, no nesting

**Intercept-only model for cosine similariy**

```{r echo=FALSE, message=FALSE, warning=FALSE}
mdl_null_similarity_simple_R2
```

R\^2 is `r mdl_null_similarity_simple_R2[1]` suggesting that `r mdl_null_similarity_simple_R2[1]*100`% of the variance in the outcome variable is explained by the model. This value is very low.

**Diagnostic plots**:

```{r echo=TRUE, message=FALSE, warning=FALSE}
plot(mdl_null_similarity_simple)
```

**Convergence**:
All convergence diagnostics are good (Rhat ≤ 1.01).

```{r}
summary(mdl_null_similarity_simple)
```
```{r fig.width=10, fig.height=5, fig.cap=capFig("Trace of the fitting process."), results='hide', warning=FALSE}
mcmc_plot(mdl_null_similarity_simple, type="trace");
```

**Posterior predictive checks**:
Modeling the density of the bimodal distribution is far from ideal.

```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive density overlay for n=500 draws."), results='hide', warning=FALSE}
pp_check(mdl_null_similarity_simple, ndraws=500) + xlab('Cosine similarity') + ggtitle('Posterior predictive density overlay');
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive distribution."), results='hide', warning=FALSE}
grid.arrange(pp_check(mdl_null_similarity_simple, type='stat', sta ='min') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of minimum values') + xlim(0,1),
             pp_check(mdl_null_similarity_simple, type='stat', stat='mean') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of means') + xlim(0,1),
             pp_check(mdl_null_similarity_simple, type='stat', stat='max') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of maximum values') + xlim(0,1),
             ncol=1); 
```

**Fixed effects**:

```{r fig.width=10, fig.height=10, fig.cap=capFig("Fixed effects' posterior distributions."), results='hide', warning=FALSE}
mcmc_plot(mdl_null_similarity_simple);
```

```{r}
back_transform_zoib(
  fixef(mdl_null_similarity_simple, dpar = "mu")["Intercept", "Estimate"],
  fixef(mdl_null_similarity_simple, dpar = "zoi")["Intercept", "Estimate"]
)
```

Our simple ZOIB model for cosine similarity
(mdl_null_similarity_simple), which includes only an intercept and
random effects for dyad and concept (with submodels for \phi, zoi, and
coi), yields an estimated intercept for the \beta‐component of
`r fixef(mdl_null_similarity_simple, dpar = "mu")["Intercept", "Estimate"]`
(95% CI:
[`r fixef(mdl_null_similarity_simple, dpar = "mu")["Intercept", 3]`,
`r fixef(mdl_null_similarity_simple, dpar = "mu")["Intercept", 4]`]). On
the original probability scale this corresponds to an underlying mean of
`r plogis(fixef(mdl_null_similarity_simple, dpar = "mu")["Intercept", "Estimate"]`)
— before accounting for inflation. With a zero–one inflation intercept
of
`r fixef(mdl_null_similarity_simple, dpar = "zoi")["Intercept", "Estimate"]`
(i.e. an inflation probability of about
`r plogis(fixef(mdl_null_similarity_simple, dpar = "zoi")["Intercept", "Estimate"]`)),
the overall expected cosine similarity is
`r back_transform_zoib(fixef(mdl_null_similarity_simple, dpar = "mu")["Intercept", "Estimate"], fixef(mdl_null_similarity_simple, dpar = "zoi")["Intercept", "Estimate"]`).
This value compares favorably to the raw mean of the observed cosine
similarity, which is `r mean(df$cosine_similarity, na.rm = TRUE)`.
Random intercept standard deviations were estimated as approximately
`r VarCorr(mdl_null_similarity_simple)$concept$sd[1]` for `concept` and
`r VarCorr(mdl_null_similarity_simple)$dyad$sd[1]` for `dyad`. 

Since our model is intercept‐only, we create a one‐row dummy data frame
and add a constant y value. This avoids the computational overhead of
using the full data while ensuring that the ggdist functions (which
require a y aesthetic for horizontal plots) run without error.

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior distribution of guessability."), results='hide', warning=FALSE}
# Create a one-row dummy data frame with a constant y value
df_dummy <- tibble(dummy = 1, y = 0)

# Generate posterior predictions on the response scale using our intercept-only model.
# We supply re_formula = NA so that predictions are made without replicating group-level structure.
df_dummy %>% 
  add_epred_draws(mdl_null_similarity_simple, re_formula = NA, type = "response") %>% 
  mutate(y = 0) %>%  # Provide a y aesthetic for horizontal plotting
  ggplot(aes(x = .epred, y = y)) +
  ggdist::stat_halfeye(
    adjust = 0.5,
    width = 0.6,
    .width = c(0.66, 0.95),
    justification = -0.1,
    point_colour = NA
  ) +
  geom_boxplot(
    aes(y = y),
    width = 0.15,
    outlier.shape = NA,
    alpha = 0.5,
    position = position_nudge(y = 0)
  ) +
  theme_minimal(base_size = 18) +
  labs(
    x = "Posterior predicted guessability (cosine similarity)",
    y = ""
  )
```

**Random effects**:

```{r fig.width=1*4, fig.height=1*7, fig.cap=capFig("Posterior estimaes of the random effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
#get_variables(b1_res_field$b1);
grid.arrange(
  # 1 | concept:
  mdl_null_similarity_simple %>%
  spread_draws(b_Intercept, r_concept[concept]) %>%
  # Add the fixed intercept for mu to the concept-specific random effect:
  mutate(mu_concept = fixef(mdl_null_similarity_simple, dpar = "mu")["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform to the original scale using the common inflation parameter from zoi:
  mutate(guessability = back_transform_zoib(mu_concept, fixef(mdl_null_similarity_simple, dpar = "zoi")["Intercept", "Estimate"])) %>%
  ggplot(aes(y = concept, x = guessability, fill = after_stat(x < 0.5))) +
  stat_halfeye(alpha = 0.75, point_interval = median_qi, .width = c(0.50, 0.95)) +
  scale_fill_manual(values = c("lightsalmon", "skyblue"),
                    name = NULL, labels = c("≥ 0.5", "< 0.5")) +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  #ggtitle("Posterior estimates of concept-level\ncosine similarity") +
  xlab("Posterior predicted cosine similarity") +
  ylab(NULL) +
  xlim(0, 1) +
    theme(legend.position="bottom") +
    NULL,

  nrow=1);
```

Get a general overview of the group-level effects.

```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(VarCorr(mdl_null_similarity_simple)$concept$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)

as.data.frame(VarCorr(mdl_null_similarity_simple)$dyad$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)
```

There is a lot of variance across different concepts. But there is
little variance across different dyads.

We can plot the variance across the concepts extracting draws.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Posterior cosine similarity of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
# 1 | concept
mdl_null_similarity_simple %>%
  spread_draws(r_concept[concept, dyad]) %>%
  # Combine the fixed intercept (mu) with the concept-specific random effect.
  mutate(mu_concept = fixef(mdl_null_similarity_simple, dpar = "mu")["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform to the response scale using the common inflation parameter:
  mutate(guessability = back_transform_zoib(mu_concept, fixef(mdl_null_similarity_simple, dpar = "zoi")["Intercept", "Estimate"])) %>%
  median_qi(guessability, .width = c(0.5, 0.8, 0.95)) %>%
  ggplot(aes(x = guessability, y = fct_reorder(concept, guessability))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper), position = position_dodge(width = 0.1)) +
  labs(
    x = "Posterior guessability (cosine similarity)",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

To validate our model’s group-level estimates, we also plot the raw
data. This plot shows the distribution of cosine similarity values for
each concept using a half‐eye plot (which neatly displays medians and
uncertainty intervals) and faceting by concept.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Raw cosine similarity of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
df %>% 
  filter(!is.na(cosine_similarity)) %>% 
  group_by(concept) %>% 
  median_qi(cosine_similarity, .width = c(0.5, 0.8, 0.95)) %>% 
  ggplot(aes(x = cosine_similarity, y = fct_reorder(concept, cosine_similarity))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper), 
                     position = position_dodge(width = 0.1)) +
  labs(
    x = "Raw cosine similarity",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

The raw values are much more "wild", but interestingly they are also lower.

**Interpretation**: 
Our simple ZOIB model for cosine similarity splits the data into two parts: a “beta” part (the regular responses) and an “inflation” part (responses that are exactly 1). The intercept for the beta component is estimated at 0.65 on the logit scale (95% credible interval [0.58, 0.72]). When we back–transform this using the inverse-logit function, it corresponds to an average cosine similarity of about 66% (roughly 64% to 67%) for the non-inflated responses.
The zero–one inflation (zoi) intercept is estimated at –0.73 on the logit scale (95% CI [–0.77, –0.68]), which back–transforms to an inflation probability of about 32% (approximately 32% [31.6%, 33.6%]). This means that **about one–third of all responses are exactly 1**.

By combining these two parts—taking 32% of responses to be perfect (1) and the remaining 68% to have an average score of about 66%—the overall **expected cosine similarity is** roughly calculated as
0.324 × 1 + 0.676 × 0.657 ≈ 0.77
or **about 77%**.

Finally, the random effects show that there is moderate variability across concepts (SD ≈ 0.30 with a 95% CI of roughly [0.25, 0.35]) but very little variability across dyads (SD ≈ 0.03 with a 95% CI of [0.01, 0.05]).

In plain terms, among the “regular” responses the average score is about 66%, but because around one–third of responses are perfect, the overall **average similarity is boosted to about 77%**. This result means that, on average, participants’ nonverbal productions are quite effective, with a high proportion of responses achieving near–perfect expression.

## Guess binary

Preregistered model:

`guessability_match ~ 1 + (1 | dyad/participant) + (1 | concept), family = bernoulli(link='logit')`

**Intercept-only model for binary outcome**

```{r}
mdl_null_binary <- brm(
  formula = guess_binary ~ 1 + (1 | dyad) + (1 | concept),
  data = df,
  family = bernoulli(link = "logit"),
  prior = c(
    # Prior for the intercept on the logit scale:
    prior(normal(0, 2.5), class = "Intercept"),
    # Prior for the standard deviations of the random effects:
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_null_binary.rds")
)

# if we need to compress the model more
#saveRDS(mdl_null_binary, file = paste0(models, "mdl_null_binary.rds"), compress = "xz")

mdl_null_binary <- readRDS(paste0(models, "mdl_null_binary.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_null_binary_R2 <- bayes_R2(mdl_null_binary)
# # Save the R² output
# saveRDS(mdl_null_binary_R2, file = paste0(models, "mdl_null_binary_R2.rds"))

mdl_null_binary_R2 <- readRDS(paste0(models, "mdl_null_binary_R2.rds"))

mdl_null_binary_R2
```

R\^2 is `r mdl_null_binary_R2[1]` suggesting that
`r mdl_null_binary_R2[1]*100`% of the variance in the outcome variable
are explained by the model. Generally, the model has a higher predictive power than the cosine similarity model.

**Diagnostic plots**:

```{r echo=TRUE, message=FALSE, warning=FALSE}
plot(mdl_null_binary)
```

**Convergence**:
All convergence diagnostics are good (Rhat ≤ 1.01).

```{r}
summary(mdl_null_binary)
```
```{r fig.width=10, fig.height=5, fig.cap=capFig("Trace of the fitting process."), results='hide', warning=FALSE}
mcmc_plot(mdl_null_binary, type="trace");
```

**Posterior predictive checks**:
Looking good.

```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive density overlay for n=500 draws."), results='hide', warning=FALSE}
pp_check(mdl_null_binary, ndraws=500) + xlab('p(Guess)') + ggtitle('Posterior predictive density overlay');
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive distribution."), results='hide', warning=FALSE}
grid.arrange(pp_check(mdl_null_binary, type='stat', sta ='min') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of minimum values') + xlim(0,1),
             pp_check(mdl_null_binary, type='stat', stat='mean') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of means') + xlim(0,1),
             pp_check(mdl_null_binary, type='stat', stat='max') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of maximum values') + xlim(0,1),
             ncol=1); 
```

**Fixed effects**:

```{r fig.width=10, fig.height=10, fig.cap=capFig("Fixed effects' posterior distributions."), results='hide', warning=FALSE}
mcmc_plot(mdl_null_binary);
```

```{r}
# because of the logit link, it needs to be backtransformed
emmeans(mdl_null_binary, ~1, type = "response")
```

**Intercept** The intercept is `r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_null_binary)["Intercept", "Estimate"], bayestestR::ci(mdl_null_binary, ci=0.95, method="ETI")[1,"CI_low"], bayestestR::ci(mdl_null_binary, ci=0.95, method="ETI")[1,"CI_high"])` on the log odds ratio (LOR) scale, which is clearly » 0; formal hypothesis testing against 0 is `r sprintf("*p*(*α*>0)=%.2f\\%s", brms::hypothesis(mdl_null_binary, "Intercept > 0")[[1]][1, "Post.Prob"], brms::hypothesis(mdl_null_binary, "Intercept > 0")[[1]][1, "Star"])`; this translates into a probability of a guess `r sprintf("*p*(match)=%.1f%% 95%%CI [%.1f%%, %.1f%%]", 100*plogis(fixef(mdl_null_binary)["Intercept", "Estimate"]), 100*plogis(bayestestR::ci(mdl_null_binary, ci=0.95, method="ETI")[1,"CI_low"]), 100*plogis(bayestestR::ci(mdl_null_binary, ci=0.95, method="ETI")[1,"CI_high"]))`.

```{r}
mcmc_plot(mdl_null_binary);
```

Because our model is intercept–only, we create a one–row dummy data frame to generate overall posterior predictions on the response (probability) scale. (We do not include additional grouping information, which avoids redundancy.)

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior distribution of guess"), results='hide', warning=FALSE}
# Create a one-row dummy data frame with a constant y value
df_dummy <- tibble(dummy = 1, y = 0)

df_dummy %>% 
  add_epred_draws(mdl_null_binary, re_formula = NA, type = "response") %>% 
  mutate(y = 0) %>%  # Provide a y aesthetic for horizontal plotting
  ggplot(aes(x = .epred, y = y)) +
  ggdist::stat_halfeye(
    adjust = 0.5,
    width = 0.6,
    .width = c(0.66, 0.95),
    justification = -0.1,
    point_colour = NA
  ) +
  geom_boxplot(
    aes(y = y),
    width = 0.15,
    outlier.shape = NA,
    alpha = 0.5,
    position = position_nudge(y = 0)
  ) +
  theme_minimal(base_size = 18) +
  labs(
    x = "Posterior predicted guessability (binary)",
    y = ""
  )
```

**Random effects**:

```{r fig.width=1*4, fig.height=1*7, fig.cap=capFig("Posterior estimaes of the random effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
#get_variables(b1_res_field$b1);
grid.arrange(
  # 1 | concept:
  mdl_null_binary %>%
    spread_draws(b_Intercept, r_concept[concept]) %>%
    #filter(Order == "Intercept") %>%
    mutate(condition_mean = r_concept) %>%
    ggplot(aes(y = concept, x = condition_mean, fill = after_stat(x < 0))) +
    stat_halfeye(alpha=0.75, point_interval=median_qi, .width=c(.50, .95)) +
    scale_fill_manual(values = c("lightsalmon", "skyblue"), name=NULL, labels=c("≥ 0", "< 0")) +
    geom_vline(xintercept = 0, linetype = "dashed") +
    ggtitle("1 | concept") + xlab("Intercept") + ylab(NULL) + xlim(-2,2) +
    theme(legend.position="bottom") +
    NULL,

  nrow=1);
```

Get a general overview of the group-level effects.

```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(VarCorr(mdl_null_binary)$concept$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)

as.data.frame(VarCorr(mdl_null_binary)$dyad$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)
```

There is a lot of variance across different concepts. But there is a lot less variance across different dyads.

We can plot the variance across the concepts extracting draws.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Posterior binary guessability of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
mdl_null_binary %>%
  spread_draws(r_concept[concept]) %>%
  # Combine the fixed intercept with the concept-specific random effect
  mutate(mu_concept = fixef(mdl_null_binary)["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform from the logit scale to probabilities
  mutate(guessability = plogis(mu_concept)) %>%
  median_qi(guessability, .width = c(0.5, 0.8, 0.95)) %>%
  ggplot(aes(x = guessability, y = fct_reorder(concept, guessability))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper),
                     position = position_dodge(width = 0.1)) +
  labs(
    x = "Posterior predicted guessability (probability)",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)
```

To validate our model’s group-level estimates, we also plot the raw
data. This plot shows the distribution of binary guessability values for
each concept using a half‐eye plot (which neatly displays medians and
uncertainty intervals) and faceting by concept.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Raw binary guessability of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
df %>% 
  filter(!is.na(guess_binary)) %>% 
  group_by(concept) %>% 
  summarize(prop = mean(as.numeric(as.character(guess_binary))), .groups = "drop") %>% 
  ggplot(aes(x = prop, y = fct_reorder(concept, prop))) +
  geom_point(color = "black", size = 3) +
  labs(
    x = "Observed guessability (proportion correct)",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

**Interpretation**: The intercept‐only model for binary guessability estimates a logit intercept of –0.74 (95% CI [–1.02, –0.46]). When back–transformed, this corresponds to an overall probability of a correct guess of about 32% (95% CI roughly 27% to 39%). As there is no chance level -- because participants were guessing without a forced choice between alternatives and they were essentially guessing blindly -- the **32% reflects the moderate level of correct guessing** observed in the data. Furthermore, the model indicates considerable variability across concepts (SD ≈ 1.29) and only modest variability across dyads (SD ≈ 0.17), suggesting that differences in the characteristics of the concepts largely drive performance differences.

-----

# Hypothesis 1: Expressibility vs. guessability
*The higher the expressibility rating of the concept for the modality in which the concept is produced, the higher the guessability will be.*

## Guess binary

Preregistered model:

`guessability_match ~ expressibility + (1 | dyad/participant) + (1 | concept), family = bernoulli(link='logit')`

```{r}
mdl_H1_binary <- brm(
  formula = guess_binary ~ expressibility_dutch + (1 | dyad) + (1 | concept),
  data = df,
  family = bernoulli(link = "logit"),
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),        # Prior for expressibility
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_H1_binary.rds")
)

# if we need to compress the model more
#saveRDS(mdl_H1_binary, file = paste0(models, "mdl_H1_binary.rds"), compress = "xz")

mdl_H1_binary <- readRDS(paste0(models, "mdl_H1_binary.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_H1_binary_R2 <- bayes_R2(mdl_H1_binary)
# # Save the R² output
# saveRDS(mdl_H1_binary_R2, file = paste0(models, "mdl_H1_binary_R2.rds"))

mdl_H1_binary_R2 <- readRDS(paste0(models, "mdl_H1_binary_R2.rds"))

mdl_H1_binary_R2
```

R\^2 is `r mdl_H1_binary_R2[1]` suggesting that
`r mdl_H1_binary_R2[1]*100`% of the variance in the outcome variable
are explained by the model.

**Diagnostic plots**:

```{r echo=TRUE, message=FALSE, warning=FALSE}
plot(mdl_H1_binary)
```


**Convergence**:
All convergence diagnostics are excellent (Rhat = 1.00).

```{r}
summary(mdl_H1_binary)
```
```{r fig.width=10, fig.height=5, fig.cap=capFig("Trace of the fitting process."), results='hide', warning=FALSE}
mcmc_plot(mdl_H1_binary, type="trace");
```

**Posterior predictive checks**:
Looking good.

```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive density overlay for n=500 draws."), results='hide', warning=FALSE}
pp_check(mdl_H1_binary, ndraws=500) + xlab('p(Guess)') + ggtitle('Posterior predictive density overlay');
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive distribution."), results='hide', warning=FALSE}
grid.arrange(pp_check(mdl_H1_binary, type='stat', sta ='min') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of minimum values') + xlim(0,1),
             pp_check(mdl_H1_binary, type='stat', stat='mean') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of means') + xlim(0,1),
             pp_check(mdl_H1_binary, type='stat', stat='max') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of maximum values') + xlim(0,1),
             ncol=1); 
```

**Fixed effects**:

```{r fig.width=10, fig.height=10, fig.cap=capFig("Fixed effects' posterior distributions."), results='hide', warning=FALSE}
mcmc_plot(mdl_H1_binary);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Fixed effects."), results='hide', warning=FALSE}
plot_model(mdl_H1_binary, type="emm", terms=c("expressibility_dutch"));
```
```{r}
# because of the logit link, it needs to be backtransformed
emmeans(mdl_H1_binary, ~ expressibility_dutch, type = "response")
```
```{r}
bayestestR::hdi(mdl_H1_binary, ci=0.95)
hypothesis(mdl_H1_binary, c("(expressibility_dutch) > 0"))
```

**Intercept**: The intercept is
`r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H1_binary)["Intercept", "Estimate"], bayestestR::ci(mdl_H1_binary, ci = 0.95, method = "ETI")[1, "CI_low"], bayestestR::ci(mdl_H1_binary, ci = 0.95, method = "ETI")[1, "CI_high"])`
on the log odds ratio (LOR) scale. When we convert this value using the inverse‐logit transformation, it corresponds to an overall predicted probability of a correct guess of
`r sprintf("%.1f%% 95%%CI [%.1f%%, %.1f%%]", 100 * plogis(fixef(mdl_H1_binary)["Intercept", "Estimate"]), 100 * plogis(bayestestR::ci(mdl_H1_binary, ci = 0.95, method = "ETI")[1, "CI_low"]), 100 * plogis(bayestestR::ci(mdl_H1_binary, ci = 0.95, method = "ETI")[1, "CI_high"]))`
when expressibility_dutch equals 0. However, since expressibility_dutch is not centered and its average in our data is approximately 0.595, the effective intercept at the average expressibility is
`r sprintf("%.2f", fixef(mdl_H1_binary)["Intercept", "Estimate"] + fixef(mdl_H1_binary)["expressibility_dutch", "Estimate"] * 0.595)`,
which back–transforms to an overall predicted probability of about
`r sprintf("%.1f%%", 100 * plogis(fixef(mdl_H1_binary)["Intercept", "Estimate"] + fixef(mdl_H1_binary)["expressibility_dutch", "Estimate"] * 0.595))`.

**Expressibility**: The slope for expressibility_dutch is `r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H1_binary)["expressibility_dutch", "Estimate"], bayestestR::ci(mdl_H1_binary, ci = 0.95, method = "ETI")[2, "CI_low"], bayestestR::ci(mdl_H1_binary, ci = 0.95, method = "ETI")[2, "CI_high"])`
on the LOR scale. This implies that for every one–unit increase in expressibility_dutch the odds of a correct guess are multiplied by `r sprintf("%.0f", exp(fixef(mdl_H1_binary)["expressibility_dutch", "Estimate"]))`.


```{r fig.width=2*4, fig.height=1*7, fig.cap=capFig("Top: Observed density of expressibility (Dutch) for each guess outcome. The black curve corresponds to observations with incorrect guesses, while the yellow curve corresponds to correct guesses. Bottom: Model-predicted probability of a correct guess as a function of expressibility. The plot shows the median predicted probability along with uncertainty bands, illustrating that higher expressibility is associated with a substantially increased probability of a correct guess."), results='hide', warning=FALSE}
# Panel 1: Observed distribution of expressibility_dutch by guess outcome
p_obs <- df %>% 
  filter(!is.na(guess_binary)) %>% 
  ggplot(aes(x = expressibility_dutch, fill = factor(guess_binary))) +
  geom_density(alpha = 0.5) +
  scale_fill_manual(values = c("0" = colorBlindBlack8[1], "1" = colorBlindBlack8[2]),
                    labels = c("incorrect", "correct")) +
  labs(x = "Expressibility", y = "Density", fill = "Guess") +
  theme_minimal(base_size = 18)

# Panel 2: Conditional effects of expressibility_dutch on predicted probability
ce_exp <- conditional_effects(mdl_H1_binary, effects = "expressibility_dutch")
p_pred <- plot(ce_exp, plot = FALSE)[[1]] +
  labs(x = "Expressibility", y = "Predicted probability\nof correct guess") +
  theme_minimal(base_size = 18)

# Arrange the two panels vertically using grid.arrange
grid.arrange(p_obs, p_pred, ncol = 1)
```

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior draws for the fixed effects."), results='hide', warning=FALSE}
# Extract posterior draws for the fixed effects
# For our binary model, the coefficients are on the logit scale.
posterior_fixed <- as.matrix(mdl_H1_binary, variable = c("b_Intercept", "b_expressibility_dutch"))

# Plot using mcmc_intervals; we add a dashed vertical line at 0 for reference.
mcmc_intervals(posterior_fixed, prob = 0.95, prob_outer = 0.99) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(
    x = "Coefficient (logit scale)"
  ) +
  theme_minimal(base_size = 18)

```

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior estimates of the fixed effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
gather_draws(mdl_H1_binary, `b_.*`, regex = TRUE) %>%
    # remove the b_ for plotting
    #mutate(.variable = gsub("b_","", .variable)) %>%
    ggplot(aes(x = .value, y = .variable, fill = after_stat(x < 0))) +
    geom_vline(xintercept=0.0, linetype="dotted", color="gray30") +
    stat_halfeye(alpha=0.75, point_interval=median_qi, .width=c(.50, .95)) +
    scale_fill_manual(values = c("lightsalmon", "skyblue"), name=NULL, labels=c("≥ 0", "< 0")) +
    xlim(-10, 10) +
    scale_y_discrete(labels=c("b_Intercept" = "Intercept α", 
                              "b_expressibility_dutch" = "expressibility")) +
    ylab(NULL) + xlab("Estimate") + 
    theme(axis.title.x = element_text(margin = margin(t = 12, b = 0,
                                                      r = 0, l = 0),
                                      face = 'bold', size = 14),
          axis.text.x = element_text(face = 'bold', size = 12),
          axis.text.y = element_text(face = 'bold', size = 12))+
    NULL;
```


```{r fig.width=10, fig.height=10, fig.cap=capFig("Aggregated posterior predictions for correct and incorrect guesses. For each posterior draw, the mean predicted probability is computed separately for observations with incorrect and correct guesses. The resulting density curves clearly show that the model predicts very low probabilities for incorrect guesses and very high probabilities for correct guesses."), results='hide', warning=FALSE}
# Extract posterior predictions on the response scale (matrix: draws x observations)
epred_matrix <- posterior_epred(mdl_H1_binary, type = "response")

# Convert guess_binary to numeric 0/1 (if not already)
df <- df %>% mutate(guess_binary_num = as.numeric(as.character(guess_binary)))

# Identify indices for each outcome group
idx_incorrect <- which(df$guess_binary_num == 0)
idx_correct   <- which(df$guess_binary_num == 1)

# For each posterior draw, compute the mean predicted probability for each group
mean_pred_incorrect <- apply(epred_matrix[, idx_incorrect, drop = FALSE], 1, mean)
mean_pred_correct   <- apply(epred_matrix[, idx_correct, drop = FALSE], 1, mean)

# Build a tidy data frame
df_post <- tibble(
  guess = rep(c("incorrect", "correct"), each = length(mean_pred_incorrect)),
  mean_pred = c(mean_pred_incorrect, mean_pred_correct)
)

# Plot the density of these aggregated predictions
ggplot(df_post, aes(x = mean_pred, fill = guess, color = guess)) +
  geom_density(alpha = 0.5, adjust = 1.2) +
  scale_fill_manual(values = c("incorrect" = colorBlindBlack8[1], "correct" = colorBlindBlack8[2])) +
  scale_color_manual(values = c("incorrect" = colorBlindBlack8[1], "correct" = colorBlindBlack8[2])) +
  labs(
    x = "Posterior predicted probability",
    y = "Density",
    #title = "Aggregated posterior predictions by outcome group",
    fill = "Guess", color = "Guess"
  ) +
  theme_minimal(base_size = 18)
```

This plot is generated by first calculating, for each posterior draw, the average predicted probability for all observations in the "incorrect" group and separately for the "correct" group. In other words, for each draw we compute one number per group—the mean prediction across that group. The density curves in this plot show the distribution of these mean values across all posterior draws. As a result, you see two distinct peaks: one for incorrect guesses (with values very close to 0) and one for correct guesses (with values very close to 1). This aggregated view emphasizes the overall group-level prediction and clearly illustrates that, on average, the model assigns a very low probability to incorrect guesses and a very high probability to correct guesses.

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior predicted probabilities by outcome group. This density plot is based on individual-level predictions (or a random subset thereof), stratified by guess outcome. The plot shows that predictions for incorrect guesses are tightly concentrated near 0, while predictions for correct guesses are more variable and generally higher, reflecting the model’s ability to distinguish between the two outcomes."), results='hide', warning=FALSE}
# # Take a random subset of the data (e.g., 500 observations)
# df_subset <- df %>% 
#   filter(!is.na(guess_binary)) %>% 
#   sample_n(500)

# Extract posterior draws of the predicted probability for these observations
# The plot takes a while, so if you just need a general overview, use the code above to just take a sample
df_subset %>% 
#df %>% 
  add_epred_draws(mdl_H1_binary, type = "response") %>% 
  ggplot(aes(x = .epred, fill = factor(guess_binary), color = factor(guess_binary))) +
  geom_density(alpha = 0.5, adjust = 1.2) +
  scale_fill_manual(values = c("0" = colorBlindBlack8[1], "1" = colorBlindBlack8[2]),
                    labels = c("incorrect", "correct")) +
  scale_color_manual(values = c("0" = colorBlindBlack8[1], "1" = colorBlindBlack8[2]),
                     labels = c("incorrect", "correct")) +
  labs(
    x = "Posterior predicted probability",
    y = "Density",
    title = "Posterior predicted probabilities by outcome group",
    fill = "Guess", color = "Guess"
  ) +
  theme_minimal(base_size = 18)

ggsave(plot = last_plot(), filename = paste0(plots, "mdl_H1_binary_postByOutcome.pdf"), width = 6, height = 6);
ggsave(plot = last_plot(), filename = paste0(plots, "mdl_H1_binary_postByOutcome.jpg"), width = 6, height = 6);
ggsave(plot = last_plot(), filename = paste0(plots, "mdl_H1_binary_postByOutcome.tif"), width = 6, height = 6, compression="lzw", dpi=600);
```

In this plot, the model’s posterior predicted probability is extracted for each individual observation (or for a randomly selected subset). The density curves are then constructed based on these individual-level predictions, separately for observations that are actually incorrect versus correct. Here, you will typically see that the distribution for incorrect guesses is highly concentrated near 0, indicating that most individual predictions for the incorrect group are very low. On the other hand, the distribution for correct guesses is more spread out, showing more variability in the predicted probabilities. This plot therefore reveals not only the overall difference between the two groups but also the within-group variation in predictions.

**Random effects**:
```{r fig.width=1*4, fig.height=1*7, fig.cap=capFig("Posterior estimates of the random effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
#get_variables(b1_res_field$b1);
grid.arrange(
  # 1 | concept:
  mdl_H1_binary %>%
    spread_draws(b_Intercept, r_concept[concept]) %>%
    #filter(Order == "Intercept") %>%
    mutate(condition_mean = r_concept) %>%
    ggplot(aes(y = concept, x = condition_mean, fill = after_stat(x < 0))) +
    stat_halfeye(alpha=0.75, point_interval=median_qi, .width=c(.50, .95)) +
    scale_fill_manual(values = c("lightsalmon", "skyblue"), name=NULL, labels=c("≥ 0", "< 0")) +
    geom_vline(xintercept = 0, linetype = "dashed") +
    ggtitle("1 | concept") + xlab("Intercept") + ylab(NULL) + xlim(-2,2) +
    theme(legend.position="bottom") +
    NULL,

  nrow=1);
```

Get a general overview of group-level effects.

```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(VarCorr(mdl_H1_binary)$concept$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)

as.data.frame(VarCorr(mdl_H1_binary)$dyad$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)
```

There is some variance across different concepts. There is a less variance across different dyads.

We can plot the variance across the concepts extracting draws.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Posterior binary guessability of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
mdl_H1_binary %>%
  spread_draws(r_concept[concept]) %>%
  # Combine the fixed intercept with the concept-specific random effect
  mutate(mu_concept = fixef(mdl_H1_binary)["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform from the logit scale to probabilities
  mutate(guessability = plogis(mu_concept)) %>%
  median_qi(guessability, .width = c(0.5, 0.8, 0.95)) %>%
  ggplot(aes(x = guessability, y = fct_reorder(concept, guessability))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper),
                     position = position_dodge(width = 0.1)) +
  labs(
    x = "Posterior predicted guessability (probability)",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)
```

To validate our model’s group-level estimates, we also plot the raw
data. This plot shows the distribution of binary guessability values for
each concept using a half‐eye plot (which neatly displays medians and
uncertainty intervals) and faceting by concept.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Raw binary guessability of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
df %>% 
  filter(!is.na(guess_binary)) %>% 
  group_by(concept) %>% 
  summarize(prop = mean(as.numeric(as.character(guess_binary))), .groups = "drop") %>% 
  ggplot(aes(x = prop, y = fct_reorder(concept, prop))) +
  geom_point(color = "black", size = 3) +
  labs(
    x = "Observed guessability (proportion correct)",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

**Interpretation**: In simple terms, our model shows that if expressibility_dutch were 0, the predicted probability of a correct guess would be extremely low—about 0.5% (with a 95% credible interval from roughly 0.3% to 0.7%). However, the average value of expressibility_dutch in our data is around 0.595. When we evaluate the model at this average, the effective intercept becomes –5.34 + 7.30 × 0.595, which is approximately –1.00 on the logit scale. Converting –1.00 using the inverse–logit transformation gives a **predicted probability of a guess at about 27%** (with a 95% credible interval roughly from 22.9% to 31.3%). Moreover, the effect of expressibility_dutch is extremely strong: **each one–unit increase in expressibility_dutch multiplies the odds of a correct guess by about 1,480**. This indicates that even small increases in expressibility lead to a dramatic boost in the predicted probability of a correct guess.

## Guess similarity

Preregistered model:

`guessability_similarity ~ expressibility + (1 | dyad/participant) + (1 | concept)`

```{r}
mdl_H1_similarity <- brm(
  # Main formula for the μ (mean) component:
  bf(cosine_similarity ~ expressibility_dutch + (1 | dyad) + (1 | concept),
  # Explicit subformulas for the additional parameters:
  phi ~ 1,                           # Precision parameter (controls concentration)
  zoi ~ 1,                           # Zero–one inflation: probability of coming from the beta part
  coi ~ 1),                            # One inflation: probability of an observation being exactly 1
  data = df,
  family = zero_one_inflated_beta(),
  prior = c(
    # Priors for the μ component (cosine_similarity):
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),  # for expressibility_dutch
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_H1_similarity.rds")
)


# if we need to compress the model more
#saveRDS(mdl_H1_similarity, file = paste0(models, "mdl_H1_similarity.rds"), compress = "xz")

mdl_H1_similarity <- readRDS(paste0(models, "mdl_H1_similarity.rds"))
```

Including expressibility_dutch in the subformulas for φ, zoi, or coi is
an option—but it should be done only if you have a strong theoretical or
empirical reason to believe that expressibility affects not only the
average outcome (the μ component) but also the dispersion or the
inflation processes. Considerations:

1.  **Theoretical Rationale:**

    -   **μ Component (Mean):**\
        Your primary hypothesis (H1) is that higher expressibility leads
        to higher guessability—that is, it shifts the central tendency
        of cosine similarity. This is why you include
        expressibility_dutch in the μ model.

    -   **φ Component (Precision):**\
        Adding expressibility_dutch here would imply that expressibility
        influences how concentrated or spread out the responses are
        (i.e., the variability around the mean). If you believe that
        expressibility not only changes the average guessability but
        also affects the consistency (or uncertainty) of the responses,
        then including it in φ might be justified.

    -   **zoi and coi (Inflation Components):**\
        Including expressibility_dutch in these parts would mean that
        expressibility also affects the probability of obtaining
        boundary values (exact 0 or 1). You would do this if you expect
        that higher expressibility systematically leads to more (or
        fewer) extreme values (e.g., more responses hitting exactly 1).

2.  **Empirical and Practical Considerations:**

    -   **Model Complexity:**\
        Adding the predictor to φ, zoi, and coi increases model
        complexity. This can lead to difficulties in convergence,
        overfitting, or less stable estimates if the data do not contain
        enough information for these additional effects.

    -   **Interpretability:**\
        The interpretation of effects on φ, zoi, and coi is more subtle.
        Unless you have a clear rationale, it may be preferable to keep
        expressibility_dutch only in the μ model and allow the other
        parameters to remain constant.

    -   **Exploratory Analysis:**\
        You might start with the simpler model (predictor only in μ) and
        then check if there is evidence—through residual analysis, model
        comparisons, or substantive theory—that expressibility_dutch
        might also be affecting variability or inflation. Only then
        would you consider adding it to the other components.

**Conclusion:**

Unless we have a specific reason to believe that expressibility_dutch
also influences the dispersion (φ) or the probability of hitting the
boundaries (zoi and coi), it is generally advisable to include it only
in the μ component. This keeps the model simpler and the interpretation
more straightforward, directly addressing your hypothesis about the
average effect on guessability.

If we later decide to explore whether expressibility affects other
aspects of the distribution, we could extend the model accordingly, but
that should be guided by theory or clear empirical indications rather
than by default.

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_H1_similarity_R2 <- bayes_R2(mdl_H1_similarity)
# # Save the R² output
# saveRDS(mdl_H1_similarity_R2, file = paste0(models, "mdl_H1_similarity_R2.rds"))

mdl_H1_similarity_R2 <- readRDS(paste0(models, "mdl_H1_similarity_R2.rds"))

mdl_H1_similarity_R2
```

R\^2 is `r mdl_H1_similarity_R2[1]` suggesting that
`r mdl_H1_similarity_R2[1]*100`% of the variance in the outcome variable
are explained by the model.

**Diagnostic plots**:

```{r echo=TRUE, message=FALSE, warning=FALSE}
plot(mdl_H1_similarity)
```

**Convergence**:
All convergence diagnostics are excellent (Rhat = 1.00).

```{r}
summary(mdl_H1_similarity)
```
```{r fig.width=10, fig.height=5, fig.cap=capFig("Trace of the fitting process."), results='hide', warning=FALSE}
mcmc_plot(mdl_H1_similarity, type="trace");
```

**Posterior predictive checks**:
Modeling the density of the bimodal distribution is far from ideal.

```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive density overlay for n=500 draws."), results='hide', warning=FALSE}
pp_check(mdl_H1_similarity, ndraws=500) + xlab('p(Guess)') + ggtitle('Posterior predictive density overlay');
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive distribution."), results='hide', warning=FALSE}
grid.arrange(pp_check(mdl_H1_similarity, type='stat', sta ='min') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of minimum values') + xlim(0,1),
             pp_check(mdl_H1_similarity, type='stat', stat='mean') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of means') + xlim(0,1),
             pp_check(mdl_H1_similarity, type='stat', stat='max') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of maximum values') + xlim(0,1),
             ncol=1); 
```

**Fixed effects**:

```{r fig.width=10, fig.height=10, fig.cap=capFig("Fixed effects' posterior distributions."), results='hide', warning=FALSE}
mcmc_plot(mdl_H1_similarity);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Fixed effects."), results='hide', warning=FALSE}
plot_model(mdl_H1_similarity, type="emm", terms=c("expressibility_dutch"));
```
```{r}
emmeans(mdl_H1_similarity, ~ expressibility_dutch, type = "response")
```
```{r}
back_transform_zoib(
  fixef(mdl_H1_similarity, dpar = "mu")["Intercept", "Estimate"],
  fixef(mdl_H1_similarity, dpar = "zoi")["Intercept", "Estimate"]
)
```

**Intercept**:
The model yields an estimated intercept for the β‐component of
`r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H1_similarity, dpar = "mu")["Intercept", "Estimate"], bayestestR::ci(mdl_H1_similarity, ci = 0.95, method = "ETI")[1, "CI_low"], bayestestR::ci(mdl_H1_similarity, ci = 0.95, method = "ETI")[1, "CI_high"])`
on the log odds scale. Because our model is specified in terms of a zero–one inflated beta, this intercept represents the average log odds for the beta portion (i.e. the “regular” responses). When we convert this value using the inverse‐logit transformation, it corresponds to an underlying mean cosine similarity of
`r sprintf("%.1f%%", 100 * plogis(fixef(mdl_H1_similarity, dpar = "mu")["Intercept", "Estimate"]))`.
However, a fraction of observations is fixed at 1 via the zero–one inflation component. The zoi submodel yields an intercept of
`r sprintf("%.2f", fixef(mdl_H1_similarity, dpar = "zoi")["Intercept", "Estimate"])`,
which back–transforms to an inflation probability of about
`r sprintf("%.1f%%", 100 * plogis(fixef(mdl_H1_similarity, dpar = "zoi")["Intercept", "Estimate"]))`.
Thus, by combining the beta part and the inflation, the overall expected cosine similarity is
`r sprintf("%.1f%%", 100 * back_transform_zoib(fixef(mdl_H1_similarity, dpar = "mu")["Intercept", "Estimate"], fixef(mdl_H1_similarity, dpar = "zoi")["Intercept", "Estimate"]))`.
This tells us that, after accounting for the fact that some observations are fixed at 1 (perfect cosine similarity) and the remainder follow a beta distribution with a mean of about 51.9%, the overall predicted average is about 75.0%. This value is very close to the raw mean of `r sprintf("%.1f%%", 100 * mean(df$cosine_similarity, na.rm = TRUE))` in our data.

**Expressibility**:
The slope for expressibility_dutch is
`r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H1_similarity)["expressibility_dutch", "Estimate"], bayestestR::ci(mdl_H1_similarity, ci = 0.95, method = "ETI")[2, "CI_low"], bayestestR::ci(mdl_H1_similarity, ci = 0.95, method = "ETI")[2, "CI_high"])`
on the log odds scale. This indicates that for every one–unit increase in `expressibility_dutch` the log odds of a higher cosine similarity increase by that amount, which translates into multiplying the odds by
`r sprintf("%.0f", exp(fixef(mdl_H1_similarity)["expressibility_dutch", "Estimate"]))`.
In plain language, even modest increases in expressibility lead to a large boost in the predicted cosine similarity.


```{r fig.width=2*4, fig.height=1*7, fig.cap=capFig("Top: The average cosine_similarity for each unique expressibility_dutch value. The size of the dots represents the number of original observationsc ontributing to each averaged point. Bottom: Model-predicted probability of a correct guess as a function of expressibility. The plot shows the median predicted probability along with uncertainty bands, illustrating that higher expressibility is associated with a substantially increased probability of a correct guess."), results='hide', warning=FALSE}
# Panel 1: Averaged cosine similarity x expressibility
p_obs <- df %>%
  group_by(expressibility_dutch) %>%
  summarise(
    mean_cosine_similarity = mean(cosine_similarity, na.rm = TRUE),  # Mean cosine similarity
    n = n()  # Count of data points for each unique x value
  ) %>%
  ggplot(aes(x = expressibility_dutch, y = mean_cosine_similarity, size = n)) +
  geom_point(color = colorBlindBlack8[1], alpha = 0.5) +  # Scatterplot with size mapped to n
  geom_smooth(method = "lm", se = TRUE, color = colorBlindBlack8[2], size = 1.5) +  # Linear regression line with error bars
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 20),  # Center-align and size of plot title
    axis.title.x = element_text(vjust = -0.5, size = 18),  # Adjust x-axis label position and size
    axis.title.y = element_text(vjust = 1.5, size = 18),   # Adjust y-axis label position and size
    axis.text.x = element_text(size = 14),  # Set size of x-axis text
    axis.text.y = element_text(size = 14),  # Set size of y-axis text
    legend.title = element_text(size = 16),  # Set size of legend title
    legend.text = element_text(size = 14),  # Set size of legend text
    text = element_text(size = 18)  # Increase general font size for better readability
  ) +
  labs(
    x = "Expressibility",
    y = "Mean cosine similarity",
    size = "Number of data points"
  )

# Panel 2: Conditional effects of expressibility_dutch on predicted probability
ce_exp <- conditional_effects(mdl_H1_similarity, effects = "expressibility_dutch")
p_pred <- plot(ce_exp, plot = FALSE)[[1]] +
  labs(x = "Expressibility", y = "Predicted cosine similarity") +
  theme_minimal(base_size = 18)

# Arrange the two panels vertically using grid.arrange
grid.arrange(p_obs, p_pred, ncol = 1)
#save
g <- arrangeGrob(p_obs, p_pred, ncol = 1)

ggsave(paste0(plots, "H1_cosineModel.png"), plot = g, width = 8, height = 8, dpi = 300)
```

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior draws for the fixed effects."), results='hide', warning=FALSE}
# Extract posterior draws for the fixed effects
# For our binary model, the coefficients are on the logit scale.
posterior_fixed <- as.matrix(mdl_H1_similarity, variable = c("b_Intercept", "b_expressibility_dutch"))

# Plot using mcmc_intervals; we add a dashed vertical line at 0 for reference.
mcmc_intervals(posterior_fixed, prob = 0.95, prob_outer = 0.99) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(
    x = "Coefficient (logit scale)"
  ) +
  theme_minimal(base_size = 18)

```

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior estimates of the fixed effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
gather_draws(mdl_H1_similarity, `b_.*`, regex = TRUE) %>%
    # remove the b_ for plotting
    filter(.variable == c("b_Intercept", "b_expressibility_dutch")) %>%
    ggplot(aes(x = .value, y = .variable, fill = after_stat(x < 0))) +
    geom_vline(xintercept=0.0, linetype="dotted", color="gray30") +
    stat_halfeye(alpha=0.75, point_interval=median_qi, .width=c(.50, .95)) +
    scale_fill_manual(values = c("lightsalmon", "skyblue"), name=NULL, labels=c("≥ 0", "< 0")) +
    #xlim(-10, 10) +
    scale_y_discrete(labels=c("b_Intercept" = "Intercept α", 
                              "b_expressibility_dutch" = "expressibility")) +
    ylab(NULL) + xlab("Estimate") + 
    theme(axis.title.x = element_text(margin = margin(t = 12, b = 0,
                                                      r = 0, l = 0),
                                      face = 'bold', size = 14),
          axis.text.x = element_text(face = 'bold', size = 12),
          axis.text.y = element_text(face = 'bold', size = 12))+
    NULL;
```


**Random effects**:

```{r fig.width=1*4, fig.height=1*7, fig.cap=capFig("Posterior estimaes of the random effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
#get_variables(b1_res_field$b1);
grid.arrange(
  # 1 | concept:
  mdl_H1_similarity %>%
  spread_draws(b_Intercept, r_concept[concept]) %>%
  # Add the fixed intercept for mu to the concept-specific random effect:
  mutate(mu_concept = fixef(mdl_H1_similarity, dpar = "mu")["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform to the original scale using the common inflation parameter from zoi:
  mutate(guessability = back_transform_zoib(mu_concept, fixef(mdl_H1_similarity, dpar = "zoi")["Intercept", "Estimate"])) %>%
  ggplot(aes(y = concept, x = guessability, fill = after_stat(x < 0.5))) +
  stat_halfeye(alpha = 0.75, point_interval = median_qi, .width = c(0.50, 0.95)) +
  scale_fill_manual(values = c("lightsalmon", "skyblue"),
                    name = NULL, labels = c("≥ 0.5", "< 0.5")) +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  #ggtitle("Posterior estimates of concept-level\ncosine similarity") +
  xlab("Posterior predicted cosine similarity") +
  ylab(NULL) +
  xlim(0, 1) +
    theme(legend.position="bottom") +
    NULL,

  nrow=1);
```

Get a general overview of the group-level effects.

```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(VarCorr(mdl_H1_similarity)$concept$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)

as.data.frame(VarCorr(mdl_H1_similarity)$dyad$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)
```

There is more variance across different concepts than across different dyads.

We can plot the variance across the concepts extracting draws.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Posterior cosine similarity of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
# 1 | concept
mdl_H1_similarity %>%
  spread_draws(r_concept[concept, dyad]) %>%
  # Combine the fixed intercept (mu) with the concept-specific random effect.
  mutate(mu_concept = fixef(mdl_H1_similarity, dpar = "mu")["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform to the response scale using the common inflation parameter:
  mutate(guessability = back_transform_zoib(mu_concept, fixef(mdl_H1_similarity, dpar = "zoi")["Intercept", "Estimate"])) %>%
  median_qi(guessability, .width = c(0.5, 0.8, 0.95)) %>%
  ggplot(aes(x = guessability, y = fct_reorder(concept, guessability))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper), position = position_dodge(width = 0.1)) +
  labs(
    x = "Posterior guessability (cosine similarity)",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

To validate our model’s group-level estimates, we also plot the raw
data. This plot shows the distribution of cosine similarity values for
each concept using a half‐eye plot (which neatly displays medians and
uncertainty intervals) and faceting by concept.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Raw cosine similarity of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
df %>% 
  filter(!is.na(cosine_similarity)) %>% 
  group_by(concept) %>% 
  median_qi(cosine_similarity, .width = c(0.5, 0.8, 0.95)) %>% 
  ggplot(aes(x = cosine_similarity, y = fct_reorder(concept, cosine_similarity))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper), 
                     position = position_dodge(width = 0.1)) +
  labs(
    x = "Raw cosine similarity",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

**Interpretation**: In simple terms, our zero–one inflated beta model for cosine similarity indicates that if expressibility were 0, the “regular” (beta) part of the model would predict an average cosine similarity of about 52% (this is because the cosine similarity was transformed to a scale in which negative values occupy 0.0-0.5 and positive 0.5-1.0). However, because the model also accounts for a substantial fraction of responses that are fixed at a perfect score (1) -- with roughly half of the responses coming from this inflation component -- **the overall predicted cosine similarity increases to about 75%, which is very close to the observed raw mean of around 76%**. Moreover, the model estimates that each one–unit increase in expressibility increases the log odds of a higher cosine similarity by about 1.02, which translates into multiplying the odds by roughly 3. In plain language, while the baseline performance (if expressibility were very low) is modest, even **small increases in expressibility lead to a considerable improvement in cosine similarity**.

# Hypothesis 2: Expressibility vs. repetitions
*The higher the expressibility rating of the concept for the modality in which the concept is produced, the fewer repetitions there will be.*

This is tested on subset experiment_part = "2".

```{r}
df_part2 <- df %>%
  filter(exp_part == 2)
```

Preregistered models:

`N_repetition ~ expressibility + (1 | dyad/participant) + (1 | concept), family = poisson()`
If data is overdispersed:
`N_repetition ~ expressibility + (1 | dyad/participant) + (1 | concept), family = negbinomial()`

## Poisson model

```{r}
mdl_H2_poisson <- brm(
  formula = correction ~ expressibility_dutch + (1 | dyad) + (1 | concept),
  data = df_part2,
  family = poisson(),
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),        # Prior for expressibility
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "model_H2_poisson.rds")
)

# if we need to compress the model more
#saveRDS(mdl_H2_poisson, file = paste0(models, "mdl_H2_poisson.rds"), compress = "xz")

mdl_H2_poisson <- readRDS(paste0(models, "mdl_H2_poisson.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_H2_poisson_R2 <- bayes_R2(mdl_H2_poisson)
# # Save the R² output
# saveRDS(mdl_H2_poisson_R2, file = paste0(models, "mdl_H2_poisson_R2.rds"))

mdl_H2_poisson_R2 <- readRDS(paste0(models, "mdl_H2_poisson_R2.rds"))

mdl_H2_poisson_R2
```

R\^2 is `r mdl_H2_poisson_R2[1]` suggesting that
`r mdl_H2_poisson_R2[1]*100`% of the variance in the outcome variable
are explained by the model.

**Diagnostic plots**:

```{r echo=TRUE, message=FALSE, warning=FALSE}
plot(mdl_H2_poisson)
```

**Convergence**:
All convergence diagnostics are excellent (Rhat = 1.00).

```{r}
summary(mdl_H2_poisson)
```
```{r fig.width=10, fig.height=5, fig.cap=capFig("Trace of the fitting process."), results='hide', warning=FALSE}
mcmc_plot(mdl_H2_poisson, type="trace");
```

**Checking dispersion**:

```{r}
# Observed data
y <- df_part2$correction

# Posterior predictive draws (some subset, e.g. 1000 draws)
yrep <- posterior_predict(mdl_H2_poisson, ndraws = 1000)

# Calculate variance in each posterior draw
pp_var <- apply(yrep, 1, var)
obs_var <- var(y)

cat("Observed variance in corrections:   ", obs_var, "\n")
cat("Mean posterior-predictive variance: ", mean(pp_var), "\n")

if (obs_var > mean(pp_var) * 1.2) {
  cat("Observed variance is substantially greater than predicted => Overdispersion likely.\n")
} else {
  cat("Observed variance is in line with predicted => No strong overdispersion.\n")
}
```

**Posterior predictive checks**:
Looking ok-ish, there are some additional oscillations.

```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive density overlay for n=500 draws."), results='hide', warning=FALSE}
pp_check(mdl_H2_poisson, ndraws=500) + xlab('Correction') + ggtitle('Posterior predictive density overlay');
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive distribution."), results='hide', warning=FALSE}
grid.arrange(pp_check(mdl_H2_poisson, type='stat', sta ='min') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of minimum values') + xlim(0,1),
             pp_check(mdl_H2_poisson, type='stat', stat='mean') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of means') + xlim(0,1),
             pp_check(mdl_H2_poisson, type='stat', stat='max') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of maximum values') + xlim(0,1),
             ncol=1); 
```

**Fixed effects**:

```{r fig.width=10, fig.height=10, fig.cap=capFig("Fixed effects' posterior distributions."), results='hide', warning=FALSE}
mcmc_plot(mdl_H2_poisson, variable = c("b_Intercept", "b_expressibility_dutch"));
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Fixed effects."), results='hide', warning=FALSE}
plot_model(mdl_H2_poisson, type="emm", terms=c("expressibility_dutch"));
```
```{r}
# This table gives expected corrections at each chosen expressibility, plus intervals.
emmeans(mdl_H2_poisson, ~ expressibility_dutch, at = list(expressibility_dutch = c(0.0, 0.25, 0.5, 0.75, 1.00)), type = "response")
```
```{r}
hypothesis(mdl_H2_poisson, "expressibility_dutch < 0")
```

**Intercept**: On the log scale, the intercept is 
`r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H2_poisson)["Intercept","Estimate"], bayestestR::ci(mdl_H2_poisson, ci=0.95, method="ETI")[1,"CI_low"], bayestestR::ci(mdl_H2_poisson, ci=0.95, method="ETI")[1,"CI_high"])`.
Exponentiating gives an expected repetitions count of 
`r sprintf("%.2f 95%%CI [%.2f, %.2f]", exp(fixef(mdl_H2_poisson)["Intercept","Estimate"]), exp(bayestestR::ci(mdl_H2_poisson, ci=0.95, method="ETI")[1,"CI_low"]), exp(bayestestR::ci(mdl_H2_poisson, ci=0.95, method="ETI")[1,"CI_high"]))`
repetitions when expressibility = 0.

**Expressibility**: The slope is `r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H2_poisson)["expressibility_dutch","Estimate"], bayestestR::ci(mdl_H2_poisson, ci=0.95, method="ETI")[2,"CI_low"], bayestestR::ci(mdl_H2_poisson, ci=0.95, method="ETI")[2,"CI_high"])` on the log scale. Converting to a multiplicative factor, `r sprintf("%.2f 95%%CI [%.2f, %.2f]", exp(fixef(mdl_H2_poisson)["expressibility_dutch","Estimate"]), exp(bayestestR::ci(mdl_H2_poisson, ci=0.95, method="ETI")[2,"CI_low"]), exp(bayestestR::ci(mdl_H2_poisson, ci=0.95, method="ETI")[2,"CI_high"]))`, meaning that a one-unit increase in expressibility multiplies the predicted number of corrections by about `r sprintf("%.2f", exp(fixef(mdl_H2_poisson)["expressibility_dutch","Estimate"]))`. Because this is less than 1, higher expressibility _reduces_ the expected count of corrections.


```{r}
plot(conditional_effects(mdl_H2_poisson, effects="expressibility_dutch"), 
     points=TRUE)
# Plot the partial effect with raw data
plot(conditional_effects(mdl_H2_poisson, "expressibility_dutch", spaghetti = TRUE, points = TRUE))
```

```{r fig.width=4, fig.height=7, fig.cap=capFig("Model-predicted probability of a correct guess as a function of expressibility. The plot shows the median predicted probability along with uncertainty bands, illustrating that falling expressibility is associated with more repetitions."), results='hide', warning=FALSE}
df_predictions <- df_part2 %>%
  modelr::data_grid(expressibility_dutch = modelr::seq_range(expressibility_dutch, n=100)) %>%
  add_epred_draws(mdl_H2_poisson, re_formula = NA)  # ignoring random intercept variation

ggplot(df_predictions, aes(x=expressibility_dutch, y=.epred)) +
  stat_lineribbon(aes(y=.epred), .width=c(.5,.8,.95), alpha=0.3) +
  geom_point(data=df_part2, 
             aes(y=correction), 
             color=colorBlindBlack8[1], alpha=0.1, position = position_jitter(height=0.2)) +
  scale_fill_brewer() +
  labs(x="Expressibility", y="Predicted # of corrections",
       #title="Poisson model: Expressibility vs. # corrections"
       )

ggsave(paste0(plots, "H2_model.png"), plot = last_plot(), width = 6, height = 5, dpi = 300)
ggsave(paste0(plots, "H2_model.tif"), plot = last_plot(), width = 6, height = 5, dpi = 300)
```


```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior draws for the fixed effects."), results='hide', warning=FALSE}
# Extract posterior draws for the fixed effects
# For our binary model, the coefficients are on the logit scale.
posterior_fixed <- as.matrix(mdl_H2_poisson, variable = c("b_Intercept", "b_expressibility_dutch"))

# Plot using mcmc_intervals; we add a dashed vertical line at 0 for reference.
mcmc_intervals(posterior_fixed, prob = 0.95, prob_outer = 0.99) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(
    x = "Coefficient (logit scale)"
  ) +
  theme_minimal(base_size = 18)

```

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior estimates of the fixed effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
gather_draws(mdl_H2_poisson, `b_.*`, regex = TRUE) %>%
    # remove the b_ for plotting
    #mutate(.variable = gsub("b_","", .variable)) %>%
    ggplot(aes(x = .value, y = .variable, fill = after_stat(x < 0))) +
    geom_vline(xintercept=0.0, linetype="dotted", color="gray30") +
    stat_halfeye(alpha=0.75, point_interval=median_qi, .width=c(.50, .95)) +
    scale_fill_manual(values = c("lightsalmon", "skyblue"), name=NULL, labels=c("≥ 0", "< 0")) +
    #xlim(-10, 10) +
    scale_y_discrete(labels=c("b_Intercept" = "Intercept α", 
                              "b_expressibility_dutch" = "Expressibility")) +
    ylab(NULL) + xlab("Estimate") + 
    theme(axis.title.x = element_text(margin = margin(t = 12, b = 0,
                                                      r = 0, l = 0),
                                      face = 'bold', size = 14),
          axis.text.x = element_text(face = 'bold', size = 12),
          axis.text.y = element_text(face = 'bold', size = 12))+
    NULL;
```

**Random effects**:
```{r fig.width=1*4, fig.height=1*7, fig.cap=capFig("Posterior estimates of the random effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
#get_variables(b1_res_field$b1);
grid.arrange(
  # 1 | concept:
  mdl_H2_poisson %>%
    spread_draws(b_Intercept, r_concept[concept]) %>%
    #filter(Order == "Intercept") %>%
    mutate(condition_mean = r_concept) %>%
    ggplot(aes(y = concept, x = condition_mean, fill = after_stat(x < 0))) +
    stat_halfeye(alpha=0.75, point_interval=median_qi, .width=c(.50, .95)) +
    scale_fill_manual(values = c("lightsalmon", "skyblue"), name=NULL, labels=c("≥ 0", "< 0")) +
    geom_vline(xintercept = 0, linetype = "dashed") +
    ggtitle("1 | concept") + xlab("Intercept") + ylab(NULL) + xlim(-2,2) +
    theme(legend.position="bottom") +
    NULL,

  nrow=1);
```

Get a general overview of group-level effects.

```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(VarCorr(mdl_H2_poisson)$concept$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)

as.data.frame(VarCorr(mdl_H2_poisson)$dyad$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)
```

There variance across different concepts is much larger than the variance across different dyads.

We can plot the variance across the concepts extracting draws.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Posterior binary guessability of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
mdl_H2_poisson %>%
  spread_draws(r_concept[concept]) %>%
  # Combine the fixed intercept with the concept-specific random effect
  mutate(mu_concept = fixef(mdl_H2_poisson)["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform from the logit scale to probabilities
  mutate(correction = plogis(mu_concept)) %>%
  median_qi(correction, .width = c(0.5, 0.8, 0.95)) %>%
  ggplot(aes(x = correction, y = fct_reorder(concept, correction))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper),
                     position = position_dodge(width = 0.1)) +
  labs(
    x = "Posterior predicted corrections",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)
```

To validate our model’s group-level estimates, we also plot the raw
data. This plot shows the distribution of correction values for
each concept using a half‐eye plot (which neatly displays medians and
uncertainty intervals) and faceting by concept.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Raw binary guessability of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
df_part2 %>% 
  filter(!is.na(correction)) %>% 
  group_by(concept) %>% 
  summarize(prop = mean(as.numeric(as.character(correction))), .groups = "drop") %>% 
  ggplot(aes(x = prop, y = fct_reorder(concept, prop))) +
  geom_point(color = "black", size = 3) +
  labs(
    x = "Observed corrections",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

**Interpretation**: Our Poisson model indicates that, at an expressibility value of zero, the expected number of repetitions is about 1.6. As expressibility increases by one unit, the number of repetitions decreases by roughly 80%. In other words, when a concept’s expressibility is higher, participants in the second half of the experiment need far fewer repair attempts to get their meaning across. Because the model shows a strongly negative effect for expressibility, these results align well with the hypothesis that **concepts rated with lower expressibility lead to fewer necessary repetitions or corrections**. The posterior predictive checks confirm that our Poisson assumptions are appropriate, and there is no indication of overdispersion requiring a more complex family. Overall, higher expressibility substantially reduces the amount of guess‐producer repair that occurs, offering strong support for Hypothesis 2.

## Looking into the Poisson

I would like to reverse the thinking, so actually find out how the expressibility changes across different corrections.

```{r}
# dpois(k, mu) = Probability of Y=k for Poisson(mu)
poisson_prob_k <- function(k, alpha, beta, x) {
  mu <- exp(alpha + beta * x)
  dpois(k, mu)
}

find_express_for_k <- function(k, alpha, beta, x_seq) {
  # Evaluate P(Y=k) over a grid x_seq
  probs <- sapply(x_seq, function(xx) poisson_prob_k(k, alpha, beta, xx))
  # Identify the x that yields the maximum probability
  best_idx <- which.max(probs)
  x_seq[best_idx]
}

# We'll do it for k=0,1,2
k_levels <- c(0,1,2)

# We'll define a sequence of x from 0..1 (assuming expressibility is in [0,1])
x_seq <- seq(0, 1, length.out=201)

draws <- as_draws_df(mdl_H2_poisson)
# In brms, the intercept is "b_Intercept", the slope is "b_expressibility_dutch"

# Filter out relevant columns
draws_simpl <- draws %>%
  select(starts_with("b_Intercept"), starts_with("b_expressibility_dutch"))

best_x_all <- expand_grid(
  draw_id  = seq_len(nrow(draws_simpl)),
  k        = k_levels
) %>%
  mutate(
    alpha = draws_simpl$b_Intercept[draw_id],
    beta  = draws_simpl$b_expressibility_dutch[draw_id],
    # For each (draw, k), run the search
    best_x = map2_dbl(alpha, beta, ~ find_express_for_k(k, .x, .y, x_seq))
  )

best_x_summary <- best_x_all %>%
  group_by(k) %>%
  mean_qi(best_x, .width = c(.95))  # e.g., 95% credible interval

best_x_summary


```



# Hypothesis 3: Guessability vs. experiment part
*Guessability is higher when there is feedback between a producer and a guesser (i.e., in the second half of the experiment, where the guesser knows if they answered correctly, and the producer knows what the guesser answered).*

## Guess binary

Preregistered model:
`guessability_match ~ experiment_part + (1 | dyad/participant) + (1 | concept), family = bernoulli(link = 'logit')`

```{r}
mdl_H3_binary <- brm(
  formula = guess_binary ~ exp_part + (1 | dyad) + (1 | concept),
  data = df,
  family = bernoulli(link = "logit"),
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),        # Prior for expressibility
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_H3_binary.rds")
)

# if we need to compress the model more
#saveRDS(mdl_H3_binary, file = paste0(models, "mdl_H3_binary.rds"), compress = "xz")

mdl_H3_binary <- readRDS(paste0(models, "mdl_H3_binary.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_H3_binary_R2 <- bayes_R2(mdl_H3_binary)
# # Save the R² output
# saveRDS(mdl_H3_binary_R2, file = paste0(models, "mdl_H3_binary_R2.rds"))

mdl_H3_binary_R2 <- readRDS(paste0(models, "mdl_H3_binary_R2.rds"))

mdl_H3_binary_R2
```

R\^2 is `r mdl_H3_binary_R2[1]` suggesting that
`r mdl_H3_binary_R2[1]*100`% of the variance in the outcome variable
are explained by the model.

**Diagnostic plots**:

```{r echo=TRUE, message=FALSE, warning=FALSE}
plot(mdl_H3_binary)
```

**Convergence**:
All convergence diagnostics are excellent (Rhat = 1.00).

```{r}
summary(mdl_H3_binary)
```
```{r fig.width=10, fig.height=5, fig.cap=capFig("Trace of the fitting process."), results='hide', warning=FALSE}
mcmc_plot(mdl_H3_binary, type="trace");
```

**Posterior predictive checks**:
Looking good.

```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive density overlay for n=500 draws."), results='hide', warning=FALSE}
pp_check(mdl_H3_binary, ndraws=500) + xlab('p(Guess)') + ggtitle('Posterior predictive density overlay');
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive distribution."), results='hide', warning=FALSE}
grid.arrange(pp_check(mdl_H3_binary, type='stat', sta ='min') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of minimum values') + xlim(0,1),
             pp_check(mdl_H3_binary, type='stat', stat='mean') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of means') + xlim(0,1),
             pp_check(mdl_H3_binary, type='stat', stat='max') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of maximum values') + xlim(0,1),
             ncol=1); 
```

**Fixed effects**:

```{r fig.width=10, fig.height=10, fig.cap=capFig("Fixed effects' posterior distributions."), results='hide', warning=FALSE}
mcmc_plot(mdl_H3_binary, variable = c("b_Intercept", "b_exp_part1"));
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Fixed effects."), results='hide', warning=FALSE}
plot_model(mdl_H3_binary, type="emm", terms=c("expressibility_dutch"));
```
```{r}
# because of the logit link, it needs to be backtransformed
emmeans(mdl_H3_binary, ~ exp_part, type = "response")
```
```{r}
bayestestR::hdi(mdl_H3_binary, ci=0.95)
hypothesis(mdl_H3_binary, c("exp_part1 > 0",
                            # whether part2 is better than part2
                            "(plogis(Intercept) + exp_part1) > (plogis(Intercept) - exp_part1)"))
```


**Intercept**: The intercept is
`r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H3_binary)["Intercept", "Estimate"], bayestestR::ci(mdl_H3_binary, ci = 0.95, method = "ETI")[1, "CI_low"], bayestestR::ci(mdl_H3_binary, ci = 0.95, method = "ETI")[1, "CI_high"])`
on the log odds ratio (LOR) scale. When we convert this value using the inverse‐logit transformation, it corresponds to an overall predicted probability of a correct guess of
`r sprintf("%.1f%% 95%%CI [%.1f%%, %.1f%%]", 100 * plogis(fixef(mdl_H3_binary)["Intercept", "Estimate"]), 100 * plogis(bayestestR::ci(mdl_H3_binary, ci = 0.95, method = "ETI")[1, "CI_low"]), 100 * plogis(bayestestR::ci(mdl_H3_binary, ci = 0.95, method = "ETI")[1, "CI_high"]))` (averaged over both halves of the experiment).

**Experiment part**: The effect of exp_part (i.e. feedback) is estimated at
`r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H3_binary)["exp_part1", "Estimate"], bayestestR::ci(mdl_H3_binary, ci = 0.95, method = "ETI")[2, "CI_low"], bayestestR::ci(mdl_H3_binary, ci = 0.95, method = "ETI")[2, "CI_high"])`
on the log odds scale. This implies that, when feedback is provided (exp_part = 2) compared to no feedback (exp_part = 1), the predicted probability of a correct guess increases from approximately
`r sprintf("%.1f%%", 100 * plogis(fixef(mdl_H3_binary)["Intercept", "Estimate"] - fixef(mdl_H3_binary)["exp_part1", "Estimate"]))`
to about
`r sprintf("%.1f%%", 100 * plogis(fixef(mdl_H3_binary)["Intercept", "Estimate"] + fixef(mdl_H3_binary)["exp_part1", "Estimate"]))`.


```{r fig.width=2*4, fig.height=1*7, fig.cap=capFig("Top: Observed density of expressibility for each guess outcome. The black curve corresponds to observations with incorrect guesses, while the yellow curve corresponds to correct guesses. Bottom: Model-predicted probability of a correct guess as a function of expressibility. The plot shows the median predicted probability along with uncertainty bands, illustrating that higher expressibility is associated with a substantially increased probability of a correct guess."), results='hide', warning=FALSE}
# Panel 1: Observed distribution of exp_part by guess outcome
p_obs <- df %>% 
  filter(!is.na(guess_binary)) %>% 
  ggplot(aes(x = exp_part, fill = factor(guess_binary))) +
  geom_boxplot(alpha = 0.5) +
  scale_fill_manual(values = c("0" = colorBlindBlack8[1], "1" = colorBlindBlack8[2]),
                    labels = c("incorrect", "correct")) +
  labs(x = "Expressibility", y = "Density", fill = "Guess") +
  theme_minimal(base_size = 18)

# Panel 2: Conditional effects of exp_part on predicted probability
ce_exp <- conditional_effects(mdl_H3_binary, effects = "exp_part")
p_pred <- plot(ce_exp, plot = FALSE)[[1]] +
  labs(x = "Expressibility", y = "Predicted probability\nof correct guess") +
  theme_minimal(base_size = 18)

# Arrange the two panels vertically using grid.arrange
grid.arrange(p_obs, p_pred, ncol = 1)
```

# TAKE A LOOK LATER WHETHER THIS WORKS:

```{r}
df %>%
  add_epred_draws(mdl_H3_binary, re_formula = NA) %>%
  mutate(
    exp_part = factor(exp_part, levels = c("1", "2"))
  ) %>%
  ggplot(aes(y = exp_part, x = .epred, fill = exp_part)) +
  ggdist::stat_halfeye(
    adjust = 0.5,
    width = 0.6,
    .width = c(0.66, 0.95),
    justification = -0.1,
    point_colour = NA
  ) +
  geom_boxplot(
    width = 0.15,
    outlier.shape = NA,
    alpha = 0.5,
    position = position_nudge(y = 0.2)
  ) +
  stat_summary(
    fun = median,
    geom = "point",
    color = "red",
    size = 2,
    position = position_nudge(y = 0.2)
  ) +
  scale_fill_manual(values = colorBlindBlack8) +
  coord_flip() +
  theme_minimal() +
  theme(text = element_text(size = 18)) +
  #xlim(1, 3.5) +
  labs(
    y = "Perceived prominence level",
    x = "Posterior duration",
    fill = "Perceived\nprominence"
  )
```


```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior draws for the fixed effects."), results='hide', warning=FALSE}
# Extract posterior draws for the fixed effects
# For our binary model, the coefficients are on the logit scale.
posterior_fixed <- as.matrix(mdl_H3_binary, variable = c("b_Intercept", "b_exp_part1"))

# Plot using mcmc_intervals; we add a dashed vertical line at 0 for reference.
mcmc_intervals(posterior_fixed, prob = 0.95, prob_outer = 0.99) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(
    x = "Coefficient (logit scale)"
  ) +
  theme_minimal(base_size = 18)

```

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior estimates of the fixed effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
h3_bin <- gather_draws(mdl_H3_binary, `b_.*`, regex = TRUE) %>%
    # remove the b_ for plotting
    #mutate(.variable = gsub("b_","", .variable)) %>%
    ggplot(aes(x = .value, y = .variable, fill = after_stat(x < 0))) +
    geom_vline(xintercept=0.0, linetype="dotted", color="gray30") +
    stat_halfeye(alpha=0.75, point_interval=median_qi, .width=c(.50, .95)) +
    scale_fill_manual(values = c("lightsalmon", "skyblue"), name=NULL, labels=c("≥ 0", "< 0")) +
    #xlim(-10, 10) +
    scale_y_discrete(labels=c("b_Intercept" = "Intercept α", 
                              "b_exp_part1" = "Experiment part")) +
    ylab(NULL) + xlab("Estimate\n(guess binary)") + 
    theme(axis.title.x = element_text(margin = margin(t = 12, b = 0,
                                                      r = 0, l = 0),
                                      face = 'bold', size = 14),
          axis.text.x = element_text(face = 'bold', size = 12),
          axis.text.y = element_text(face = 'bold', size = 12))+
    NULL;
h3_bin
```


```{r fig.width=10, fig.height=10, fig.cap=capFig("Aggregated posterior predictions for correct and incorrect guesses. For each posterior draw, the mean predicted probability is computed separately for observations with incorrect and correct guesses. The resulting density curves clearly show that the model predicts very low probabilities for incorrect guesses and very high probabilities for correct guesses."), results='hide', warning=FALSE}
# Extract posterior predictions on the response scale (matrix: draws x observations)
epred_matrix <- posterior_epred(mdl_H3_binary, type = "response")

# Convert guess_binary to numeric 0/1 (if not already)
df <- df %>% mutate(guess_binary_num = as.numeric(as.character(guess_binary)))

# Identify indices for each outcome group
idx_incorrect <- which(df$guess_binary_num == 0)
idx_correct   <- which(df$guess_binary_num == 1)

# For each posterior draw, compute the mean predicted probability for each group
mean_pred_incorrect <- apply(epred_matrix[, idx_incorrect, drop = FALSE], 1, mean)
mean_pred_correct   <- apply(epred_matrix[, idx_correct, drop = FALSE], 1, mean)

# Build a tidy data frame
df_post <- tibble(
  guess = rep(c("incorrect", "correct"), each = length(mean_pred_incorrect)),
  mean_pred = c(mean_pred_incorrect, mean_pred_correct)
)

# Plot the density of these aggregated predictions
ggplot(df_post, aes(x = mean_pred, fill = guess, color = guess)) +
  geom_density(alpha = 0.5, adjust = 1.2) +
  scale_fill_manual(values = c("incorrect" = colorBlindBlack8[1], "correct" = colorBlindBlack8[2])) +
  scale_color_manual(values = c("incorrect" = colorBlindBlack8[1], "correct" = colorBlindBlack8[2])) +
  labs(
    x = "Posterior predicted probability",
    y = "Density",
    #title = "Aggregated posterior predictions by outcome group",
    fill = "Guess", color = "Guess"
  ) +
  theme_minimal(base_size = 18)
```

This plot is generated by first calculating, for each posterior draw, the average predicted probability for all observations in the "incorrect" group and separately for the "correct" group. In other words, for each draw we compute one number per group—the mean prediction across that group. The density curves in this plot show the distribution of these mean values across all posterior draws. As a result, you see two distinct peaks: one for incorrect guesses (with values very close to 0) and one for correct guesses (with values very close to 1). This aggregated view emphasizes the overall group-level prediction and clearly illustrates that, on average, the model assigns a very low probability to incorrect guesses and a very high probability to correct guesses.

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior predicted probabilities by outcome group. This density plot is based on individual-level predictions (or a random subset thereof), stratified by guess outcome. The plot shows that predictions for incorrect guesses are tightly concentrated near 0, while predictions for correct guesses are more variable and generally higher, reflecting the model’s ability to distinguish between the two outcomes."), results='hide', warning=FALSE}
# # Take a random subset of the data (e.g., 500 observations)
# df_subset <- df %>% 
#   filter(!is.na(guess_binary)) %>% 
#   sample_n(500)

# Extract posterior draws of the predicted probability for these observations
# The plot takes a while, so if you just need a general overview, use the code above to just take a sample
df_subset %>% 
#df %>% 
  add_epred_draws(mdl_H3_binary, type = "response") %>% 
  ggplot(aes(x = .epred, fill = factor(guess_binary), color = factor(guess_binary))) +
  geom_density(alpha = 0.5, adjust = 1.2) +
  scale_fill_manual(values = c("0" = colorBlindBlack8[1], "1" = colorBlindBlack8[2]),
                    labels = c("incorrect", "correct")) +
  scale_color_manual(values = c("0" = colorBlindBlack8[1], "1" = colorBlindBlack8[2]),
                     labels = c("incorrect", "correct")) +
  labs(
    x = "Posterior predicted probability",
    y = "Density",
    title = "Posterior predicted probabilities by outcome group",
    fill = "Guess", color = "Guess"
  ) +
  theme_minimal(base_size = 18)

# ggsave(plot = last_plot(), filename = paste0(plots, "mdl_H3_binary_postByOutcome.pdf"), width = 6, height = 6);
# ggsave(plot = last_plot(), filename = paste0(plots, "mdl_H3_binary_postByOutcome.jpg"), width = 6, height = 6);
# ggsave(plot = last_plot(), filename = paste0(plots, "mdl_H3_binary_postByOutcome.tif"), width = 6, height = 6, compression="lzw", dpi=600);
```

In this plot, the model’s posterior predicted probability is extracted for each individual observation (or for a randomly selected subset). The density curves are then constructed based on these individual-level predictions, separately for observations that are actually incorrect versus correct. Here, you will typically see that the distribution for incorrect guesses is highly concentrated near 0, indicating that most individual predictions for the incorrect group are very low. On the other hand, the distribution for correct guesses is more spread out, showing more variability in the predicted probabilities. This plot therefore reveals not only the overall difference between the two groups but also the within-group variation in predictions.

**Random effects**:
```{r fig.width=1*4, fig.height=1*7, fig.cap=capFig("Posterior estimates of the random effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
#get_variables(b1_res_field$b1);
grid.arrange(
  # 1 | concept:
  mdl_H3_binary %>%
    spread_draws(b_Intercept, r_concept[concept]) %>%
    #filter(Order == "Intercept") %>%
    mutate(condition_mean = r_concept) %>%
    ggplot(aes(y = concept, x = condition_mean, fill = after_stat(x < 0))) +
    stat_halfeye(alpha=0.75, point_interval=median_qi, .width=c(.50, .95)) +
    scale_fill_manual(values = c("lightsalmon", "skyblue"), name=NULL, labels=c("≥ 0", "< 0")) +
    geom_vline(xintercept = 0, linetype = "dashed") +
    ggtitle("1 | concept") + xlab("Intercept") + ylab(NULL) + xlim(-2,2) +
    theme(legend.position="bottom") +
    NULL,

  nrow=1);
```

Get a general overview of group-level effects.

```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(VarCorr(mdl_H3_binary)$concept$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)

as.data.frame(VarCorr(mdl_H3_binary)$dyad$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)
```

There variance across different concepts is much larger than the variance across different dyads.

We can plot the variance across the concepts extracting draws.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Posterior binary guessability of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
mdl_H3_binary %>%
  spread_draws(r_concept[concept]) %>%
  # Combine the fixed intercept with the concept-specific random effect
  mutate(mu_concept = fixef(mdl_H3_binary)["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform from the logit scale to probabilities
  mutate(guessability = plogis(mu_concept)) %>%
  median_qi(guessability, .width = c(0.5, 0.8, 0.95)) %>%
  ggplot(aes(x = guessability, y = fct_reorder(concept, guessability))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper),
                     position = position_dodge(width = 0.1)) +
  labs(
    x = "Posterior predicted guessability (probability)",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)
```

To validate our model’s group-level estimates, we also plot the raw
data. This plot shows the distribution of binary guessability values for
each concept using a half‐eye plot (which neatly displays medians and
uncertainty intervals) and faceting by concept.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Raw binary guessability of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
df %>% 
  filter(!is.na(guess_binary)) %>% 
  group_by(concept) %>% 
  summarize(prop = mean(as.numeric(as.character(guess_binary))), .groups = "drop") %>% 
  ggplot(aes(x = prop, y = fct_reorder(concept, prop))) +
  geom_point(color = "black", size = 3) +
  labs(
    x = "Observed guessability (proportion correct)",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

**Interpretation**: Our model comparing experiment parts indicates that **feedback has a positive effect on guessability**. In simple terms, when no feedback is provided (experiment part 1), the model predicts a probability of a correct guess of about 29.8% (with a 95% credible interval from roughly 23.8% to 36.4%). When feedback is provided (experiment part 2), the predicted probability increases to about 33.6% (with a 95% credible interval from approximately 27.1% to 40.4%). On the log odds scale, the effect of feedback is estimated at 0.18 (95% CI [0.09, 0.27]), meaning that moving from no feedback to feedback increases the odds of a correct guess. When these log odds are converted to the probability scale, the difference between the conditions is about 36% (with a 95% credible interval from roughly 18% to 54%). In practical terms, this means that providing feedback leads to a meaningful increase in the probability of a correct guess, with nearly 100% posterior certainty that the feedback condition yields a higher probability than the no-feedback condition.

## Guess similarity

Preregistered model:
`guessability_similarity ~ experiment_part + (1 | dyad/participant) + (1 | concept)`

```{r}
mdl_H3_similarity <- brm(
  # Main formula for the μ (mean) component:
  bf(cosine_similarity  ~ exp_part + (1 | dyad) + (1 | concept),
  # Explicit subformulas for the additional parameters:
  phi ~ 1,                           # Precision parameter (controls concentration)
  zoi ~ 1,                           # Zero–one inflation: probability of coming from the beta part
  coi ~ 1),                            # One inflation: probability of an observation being exactly 1
  data = df,
  family = zero_one_inflated_beta(),
  prior = c(
    # Priors for the μ component (cosine_similarity):
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),  # for expressibility_dutch
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_H3_similarity.rds")
)

# if we need to compress the model more
#saveRDS(mdl_H3_similarity, file = paste0(models, "mdl_H3_similarity.rds"), compress = "xz")

mdl_H3_similarity <- readRDS(paste0(models, "mdl_H3_similarity.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_H3_similarity_R2 <- bayes_R2(mdl_H3_similarity)
# # Save the R² output
# saveRDS(mdl_H3_similarity_R2, file = paste0(models, "mdl_H3_similarity_R2.rds"))

mdl_H3_similarity_R2 <- readRDS(paste0(models, "mdl_H3_similarity_R2.rds"))

mdl_H3_similarity_R2
```

R\^2 is `r mdl_H3_similarity_R2[1]` suggesting that
`r mdl_H3_similarity_R2[1]*100`% of the variance in the outcome variable
are explained by the model.

**Diagnostic plots**:

```{r echo=TRUE, message=FALSE, warning=FALSE}
plot(mdl_H3_similarity)
```

**Convergence**:
All convergence diagnostics are excellent (Rhat = 1.00).

```{r}
summary(mdl_H3_similarity)
```
```{r fig.width=10, fig.height=5, fig.cap=capFig("Trace of the fitting process."), results='hide', warning=FALSE}
mcmc_plot(mdl_H3_similarity, type="trace");
```

**Posterior predictive checks**:
Modeling the density of the bimodal distribution is far from ideal.

```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive density overlay for n=500 draws."), results='hide', warning=FALSE}
pp_check(mdl_H3_similarity, ndraws=500) + xlab('p(Guess)') + ggtitle('Posterior predictive density overlay');
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive distribution."), results='hide', warning=FALSE}
grid.arrange(pp_check(mdl_H3_similarity, type='stat', sta ='min') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of minimum values') + xlim(0,1),
             pp_check(mdl_H3_similarity, type='stat', stat='mean') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of means') + xlim(0,1),
             pp_check(mdl_H3_similarity, type='stat', stat='max') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of maximum values') + xlim(0,1),
             ncol=1); 
```

**Fixed effects**:

```{r fig.width=10, fig.height=10, fig.cap=capFig("Fixed effects' posterior distributions."), results='hide', warning=FALSE}
mcmc_plot(mdl_H3_similarity);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Fixed effects."), results='hide', warning=FALSE}
plot_model(mdl_H3_similarity, type="emm", terms=c("exp_part"));
```
```{r}
emmeans(mdl_H3_similarity, ~ exp_part, type = "response")
```
```{r}
back_transform_zoib(
  fixef(mdl_H3_similarity, dpar = "mu")["Intercept", "Estimate"],
  fixef(mdl_H3_similarity, dpar = "zoi")["Intercept", "Estimate"]
)
```
```{r}
bayestestR::hdi(mdl_H3_similarity, ci=0.95)
hypothesis(mdl_H3_similarity, c("exp_part1 > 0",
                            # whether part2 is better than part2
                            "(plogis(Intercept) + exp_part1) > (plogis(Intercept) - exp_part1)"))
```

**Intercept**:
The model yields an estimated intercept for the β‐component of
`r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H3_similarity, dpar = "mu")["Intercept", "Estimate"], bayestestR::ci(mdl_H3_similarity, ci = 0.95, method = "ETI")[1, "CI_low"], bayestestR::ci(mdl_H3_similarity, ci = 0.95, method = "ETI")[1, "CI_high"])`
on the log odds scale. Because our model is specified in terms of a zero–one inflated beta, this intercept represents the average log odds for the beta portion (i.e. the “regular” responses). When we convert this value using the inverse‐logit transformation, it corresponds to an underlying mean cosine similarity of
`r sprintf("%.1f%%", 100 * plogis(fixef(mdl_H3_similarity, dpar = "mu")["Intercept", "Estimate"]))`.
However, a fraction of observations is fixed at 1 via the zero–one inflation component. The zoi submodel yields an intercept of
`r sprintf("%.2f", fixef(mdl_H3_similarity, dpar = "zoi")["Intercept", "Estimate"])`,
which back–transforms to an inflation probability of about
`r sprintf("%.1f%%", 100 * plogis(fixef(mdl_H3_similarity, dpar = "zoi")["Intercept", "Estimate"]))`.
Thus, by combining the beta part and the inflation, the overall expected cosine similarity is
`r sprintf("%.1f%%", 100 * back_transform_zoib(fixef(mdl_H3_similarity, dpar = "mu")["Intercept", "Estimate"], fixef(mdl_H3_similarity, dpar = "zoi")["Intercept", "Estimate"]))`.
This value is very close to the raw mean of `r sprintf("%.1f%%", 100 * mean(df$cosine_similarity, na.rm = TRUE))` in our data.

**Experiment part**:
The effect of experiment part is estimated at
`r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H3_similarity)["exp_part1", "Estimate"], bayestestR::ci(mdl_H3_similarity, ci = 0.95, method = "ETI")[5, "CI_low"], bayestestR::ci(mdl_H3_similarity, ci = 0.95, method = "ETI")[5, "CI_high"])`
on the log odds scale. In plain language, this indicates that there is essentially no meaningful difference in the predicted cosine similarity between the first half of the experiment (without feedback) and the second half (with feedback). In other words, our model does not find evidence that providing feedback substantially alters the effectiveness of nonverbal communication as measured by cosine similarity.


```{r fig.width=2*4, fig.height=1*7, fig.cap=capFig("Top: MUST BE FIXED Bottom: Model-predicted probability of a correct guess as a function of expressibility. The plot shows the median predicted probability along with uncertainty bands, illustrating that higher expressibility is associated with a substantially increased probability of a correct guess."), results='hide', warning=FALSE}
# Panel 1: Averaged cosine similarity x expressibility
p_obs <- df %>% 
  filter(!is.na(cosine_similarity)) %>% 
  ggplot(aes(y = cosine_similarity, fill = factor(exp_part))) +
  geom_boxplot(alpha = 0.5) +
  scale_fill_manual(values = c("1" = colorBlindBlack8[1], "2" = colorBlindBlack8[2]),
                    labels = c("2", "1")) +
  labs(y = "Cosine similarity", y = "Experiment part", fill = "Experiment part") +
  theme_minimal(base_size = 18)

# Panel 2: Conditional effects of expressibility_dutch on predicted probability
ce_exp <- conditional_effects(mdl_H3_similarity, effects = "exp_part")
p_pred <- plot(ce_exp, plot = FALSE)[[1]] +
  labs(x = "Experiment part", y = "Predicted probability\nof correct guess") +
  theme_minimal(base_size = 18)

# Arrange the two panels vertically using grid.arrange
grid.arrange(p_obs, p_pred, ncol = 1)
```

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior draws for the fixed effects."), results='hide', warning=FALSE}
# Extract posterior draws for the fixed effects
# For our binary model, the coefficients are on the logit scale.
posterior_fixed <- as.matrix(mdl_H3_similarity, variable = c("b_Intercept", "b_exp_part1"))

# Plot using mcmc_intervals; we add a dashed vertical line at 0 for reference.
mcmc_intervals(posterior_fixed, prob = 0.95, prob_outer = 0.99) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(
    x = "Coefficient (logit scale)"
  ) +
  theme_minimal(base_size = 18)

```

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior estimates of the fixed effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
h3_sim <- gather_draws(mdl_H3_similarity, `b_.*`, regex = TRUE) %>%
    # remove the b_ for plotting
    filter(.variable == c("b_Intercept", "b_exp_part1")) %>%
    ggplot(aes(x = .value, y = .variable, fill = after_stat(x < 0))) +
    geom_vline(xintercept=0.0, linetype="dotted", color="gray30") +
    stat_halfeye(alpha=0.75, point_interval=median_qi, .width=c(.50, .95)) +
    scale_fill_manual(values = c("lightsalmon", "skyblue"), name=NULL, labels=c("≥ 0", "< 0")) +
    #xlim(-10, 10) +
    scale_y_discrete(labels=c("b_Intercept" = "Intercept α", 
                              "b_exp_part1" = "Experiment part")) +
    ylab(NULL) + xlab("Estimate\n(cosine similarity)") + 
    theme(axis.title.x = element_text(margin = margin(t = 12, b = 0,
                                                      r = 0, l = 0),
                                      face = 'bold', size = 14),
          axis.text.x = element_text(face = 'bold', size = 12),
          axis.text.y = element_text(face = 'bold', size = 12))+
    NULL;
h3_sim

# Arrange the two panels vertically using grid.arrange
grid.arrange(h3_bin + guides(fill="none"), h3_sim + theme(axis.text.y=element_blank()), ncol = 2)
#save
g <- arrangeGrob(h3_bin + guides(fill="none"), h3_sim + theme(axis.text.y=element_blank()), ncol = 2)

ggsave(paste0(plots, "H3_models.png"), plot = g, width = 8, height = 5, dpi = 300)
```


**Random effects**:

```{r fig.width=1*4, fig.height=1*7, fig.cap=capFig("Posterior estimaes of the random effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
#get_variables(b1_res_field$b1);
grid.arrange(
  # 1 | concept:
  mdl_H3_similarity %>%
  spread_draws(b_Intercept, r_concept[concept]) %>%
  # Add the fixed intercept for mu to the concept-specific random effect:
  mutate(mu_concept = fixef(mdl_H3_similarity, dpar = "mu")["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform to the original scale using the common inflation parameter from zoi:
  mutate(guessability = back_transform_zoib(mu_concept, fixef(mdl_H3_similarity, dpar = "zoi")["Intercept", "Estimate"])) %>%
  ggplot(aes(y = concept, x = guessability)) +
  stat_halfeye(alpha = 0.75, point_interval = median_qi, .width = c(0.50, 0.95)) +
  # scale_fill_manual(values = c("lightsalmon", "skyblue"),
  #                   name = NULL, labels = c("≥ 0.5", "< 0.5")) +
  #geom_vline(xintercept = 0.5, linetype = "dashed") +
  #ggtitle("Posterior estimates of concept-level\ncosine similarity") +
  xlab("Posterior predicted cosine similarity") +
  ylab(NULL) +
  xlim(0, 1) +
    theme(legend.position="bottom") +
    NULL,

  nrow=1);
```

Get a general overview of the group-level effects.

```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(VarCorr(mdl_H3_similarity)$concept$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)

as.data.frame(VarCorr(mdl_H3_similarity)$dyad$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)
```

There is more variance across different concepts than across different dyads.

We can plot the variance across the concepts extracting draws.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Posterior cosine similarity of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
# 1 | concept
mdl_H3_similarity %>%
  spread_draws(r_concept[concept, dyad]) %>%
  # Combine the fixed intercept (mu) with the concept-specific random effect.
  mutate(mu_concept = fixef(mdl_H3_similarity, dpar = "mu")["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform to the response scale using the common inflation parameter:
  mutate(guessability = back_transform_zoib(mu_concept, fixef(mdl_H3_similarity, dpar = "zoi")["Intercept", "Estimate"])) %>%
  median_qi(guessability, .width = c(0.5, 0.8, 0.95)) %>%
  ggplot(aes(x = guessability, y = fct_reorder(concept, guessability))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper), position = position_dodge(width = 0.1)) +
  labs(
    x = "Posterior guessability (cosine similarity)",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

To validate our model’s group-level estimates, we also plot the raw
data. This plot shows the distribution of cosine similarity values for
each concept using a half‐eye plot (which neatly displays medians and
uncertainty intervals) and faceting by concept.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Raw cosine similarity of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
df %>% 
  filter(!is.na(cosine_similarity)) %>% 
  group_by(concept) %>% 
  median_qi(cosine_similarity, .width = c(0.5, 0.8, 0.95)) %>% 
  ggplot(aes(x = cosine_similarity, y = fct_reorder(concept, cosine_similarity))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper), 
                     position = position_dodge(width = 0.1)) +
  labs(
    x = "Raw cosine similarity",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

**Interpretation**: Our model comparing experiment parts for cosine similarity indicates that **providing feedback has essentially no effect on the predicted cosine similarity**. In other words, whether feedback is provided (experiment part 2) or not (experiment part 1), the overall predicted cosine similarity remains virtually unchanged. The model estimates the effect of experiment part at only about 0.01 on the log odds scale, with a 95% credible interval that spans from –0.02 to 0.03. When we convert these logit values to the original scale, the predicted average cosine similarity is approximately 75%, which is very close to the observed raw mean of about 76%. In practical terms, this means that the quality of nonverbal communication -- as measured by cosine similarity -- is not substantially affected by whether or not feedback is provided between the producer and the guesser.

# Hypothesis 4: Guessability vs. modality
*Guessability will differ by modality; the order from highest to lowest will be: combined \> gesture \> vocalization.*

## Guess binary

Preregistered model:
`guessability_match ~ modality + (1 | dyad/participant) + (1 | concept), family = bernoulli(link='logit')`

```{r}
mdl_H4_binary <- brm(
  formula = guess_binary ~ modality + (1 | dyad) + (1 | concept),
  data = df,
  family = bernoulli(link = "logit"),
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),        # Prior for modality
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_H4_binary.rds")
)

# if we need to compress the model more
#saveRDS(mdl_H4_binary, file = paste0(models, "mdl_H4_binary.rds"), compress = "xz")

mdl_H4_binary <- readRDS(paste0(models, "mdl_H4_binary.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_H4_binary_R2 <- bayes_R2(mdl_H4_binary)
# # Save the R² output
# saveRDS(mdl_H4_binary_R2, file = paste0(models, "mdl_H4_binary_R2.rds"))

mdl_H4_binary_R2 <- readRDS(paste0(models, "mdl_H4_binary_R2.rds"))

mdl_H4_binary_R2
```

R² is `r mdl_H4_binary_R2[1]` suggesting that `r mdl_H4_binary_R2[1]*100`% of the variance in the outcome variable is explained by the model.

**Diagnostic plots**:

```{r echo=TRUE, message=FALSE, warning=FALSE}
plot(mdl_H4_binary)
```

**Convergence**:
All convergence diagnostics are excellent (Rhat = 1.00).

```{r}
summary(mdl_H4_binary)
```
```{r fig.width=10, fig.height=5, fig.cap=capFig("Trace of the fitting process."), results='hide', warning=FALSE}
mcmc_plot(mdl_H4_binary, type="trace");
```

**Posterior predictive checks**:
Looking good.

```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive density overlay for n=500 draws."), results='hide', warning=FALSE}
pp_check(mdl_H4_binary, ndraws=500) + xlab('p(Guess)') + ggtitle('Posterior predictive density overlay');
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive distribution."), results='hide', warning=FALSE}
grid.arrange(pp_check(mdl_H4_binary, type='stat', sta ='min') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of minimum values') + xlim(0,1),
             pp_check(mdl_H4_binary, type='stat', stat='mean') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of means') + xlim(0,1),
             pp_check(mdl_H4_binary, type='stat', stat='max') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of maximum values') + xlim(0,1),
             ncol=1); 
```

**Fixed effects**:

```{r fig.width=10, fig.height=10, fig.cap=capFig("Fixed effects' posterior distributions."), results='hide', warning=FALSE}
mcmc_plot(mdl_H4_binary);
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Fixed effects."), results='hide', warning=FALSE}
plot_model(mdl_H4_binary, type="emm", terms=c("modality"));
```
```{r}
# because of the logit link, it needs to be backtransformed
emmeans(mdl_H4_binary, ~ modality)
emmeans(mdl_H4_binary, ~ modality, type = "response")
pairs(emmeans(mdl_H4_binary, ~ modality))
pairs(emmeans(mdl_H4_binary, ~ modality, type = "response"))
```
```{r}
bayestestR::hdi(mdl_H4_binary, ci=0.95)
hypothesis(mdl_H4_binary, c("modalitygesture < 0", 
                            "modalityvocal < 0"
                            ))
hypothesis(mdl_H4_binary,
  c(# combined better than gesture
    "inv_logit_scaled(Intercept) > inv_logit_scaled(Intercept + modalitygesture)",
    # combined better than vocal
    "inv_logit_scaled(Intercept) > inv_logit_scaled(Intercept + modalityvocal)",
    # gesture better than vocal
    "inv_logit_scaled(Intercept + modalitygesture) > inv_logit_scaled(Intercept + modalityvocal)"
  ))
```
```{r}
draws <- as_draws_df(mdl_H4_binary) %>%  # or as_draws()
  mutate(
    p_combined = plogis(b_Intercept),
    p_gesture  = plogis(b_Intercept + `b_modalitygesture`),
    p_vocal    = plogis(b_Intercept + `b_modalityvocal`)
  )

draws_summary <- draws %>%
  summarise(
    # Combined
    mean_combined = mean(p_combined),
    ci_combined   = ci(p_combined, ci=0.95, method="ETI"),
    # Gesture
    mean_gesture  = mean(p_gesture),
    ci_gesture    = ci(p_gesture, ci=0.95, method="ETI"),
    # Vocal
    mean_vocal    = mean(p_vocal),
    ci_vocal      = ci(p_vocal, ci=0.95, method="ETI")
  )
# Probability for combined with 95% ETI:
p_comb_mean <- mean(draws$p_combined)
p_comb_ci   <- ci(draws$p_combined, ci=0.95, method="ETI")
```

**Intercept**: The intercept is
`r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H4_binary)["Intercept", "Estimate"], bayestestR::ci(mdl_H4_binary, ci = 0.95, method = "ETI")[1, "CI_low"], bayestestR::ci(mdl_H4_binary, ci = 0.95, method = "ETI")[1, "CI_high"])` on the log-odds ratio (LOR) scale. When we convert this value using the inverse-logit transformation, it corresponds to an overall predicted probability of a correct guess (when **combined** is the reference modality) of `r sprintf("%.1f%% 95%%CI [%.1f%%, %.1f%%]", 100 * plogis(fixef(mdl_H4_binary)["Intercept", "Estimate"]), 100 * plogis(bayestestR::ci(mdl_H4_binary, ci = 0.95, method = "ETI")[1, "CI_low"]), 100 * plogis(bayestestR::ci(mdl_H4_binary, ci = 0.95, method = "ETI")[1, "CI_high"]))`.

**Modality (gesture)**: The effect of *gesture* is `r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H4_binary)["modalitygesture", "Estimate"],bayestestR::ci(mdl_H4_binary, ci=0.95, method="ETI")[2,"CI_low"],bayestestR::ci(mdl_H4_binary, ci=0.95, method="ETI")[2,"CI_high"])` on the log-odds scale. 
In other words, when we switch from **combined** to **gesture**, 
the predicted probability of a correct guess changes from about
`r sprintf("%.1f%% 95%%CI [%.1f%%, %.1f%%]", 100 * draws_summary$mean_combined, 100 * draws_summary$ci_combined$CI_low, 100 * draws_summary$ci_combined$CI_high)` (**combined**) to `r sprintf("%.1f%% 95%%CI [%.1f%%, %.1f%%]", 100 * draws_summary$mean_gesture, 100 * draws_summary$ci_gesture$CI_low, 100 * draws_summary$ci_gesture$CI_high)` (**gesture**). 
Since the estimate is close to zero (and its 95% CI straddles zero),
we see that **combined** and **gesture** yield similar guessability probabilities.

**Modality (vocal)**: The effect of *vocal* is `r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H4_binary)["modalityvocal","Estimate"], bayestestR::ci(mdl_H4_binary, ci=0.95, method="ETI")[3,"CI_low"], bayestestR::ci(mdl_H4_binary, ci=0.95, method="ETI")[3,"CI_high"])` on the log-odds scale. 
Switching from **combined** to **vocal** decreases the probability of a correct guess from about `r sprintf("%.1f%% 95%%CI [%.1f%%, %.1f%%]", 100 * draws_summary$mean_combined, 100 * draws_summary$ci_combined$CI_low, 100 * draws_summary$ci_combined$CI_high)` (**combined**) to `r sprintf("%.1f%% 95%%CI [%.1f%%, %.1f%%]", 100 * draws_summary$mean_vocal, 100 * draws_summary$ci_vocal$CI_low, 100 * draws_summary$ci_vocal$CI_high)` (**vocal**). 
This large negative effect indicates that  **vocal** productions are much less likely to be guessed correctly than **combined**.


```{r fig.width=2*4, fig.height=1*7, fig.cap=capFig("Top: Raw mean binary guessability by modality. Bottom: Model-predicted probability of a correct guess by modality. The plot shows the median predicted probability along with uncertainty bands."), results='hide', warning=FALSE}
# Panel 1: Observed distribution of modality by guess outcome
p_obs <- ggplot(df, aes(x = modality, y = guess_binary, fill = modality)) +
  stat_summary(fun = "mean", geom = "bar", color = "black", alpha = 0.7) +  # Calculate mean for guess_binary (performance)
  stat_summary(fun.data = "mean_cl_normal", geom = "errorbar", width = 0.2) +  # Add error bars (95% CI)
  labs(
    x = "Modality", 
    y = "Mean binary guessability",
  ) +
  scale_fill_manual(values = c("combined" = colorBlindBlack8[1], "gesture" = colorBlindBlack8[2], "vocal" = colorBlindBlack8[3])) +  # Customize colors
  guides(fill = "none") +  # Remove the fill legend
  # scale_y_continuous(breaks = seq(0, 1, by = 0.05),
  #                    limits = c(0,1)) +
  theme_minimal(base_size = 18) +
    theme(
      #text = element_text(size = 14),  # Increase text size for readability
      axis.text.x = element_blank(),
      axis.title.x = element_blank(),
      axis.title.y = element_text(vjust = 1.5)   # Adjust y-axis title position
    )

# Panel 2: Conditional effects of exp_part on predicted probability
ce_exp <- conditional_effects(mdl_H4_binary, effects = "modality")
p_pred <- plot(ce_exp, plot = FALSE)[[1]] +
  labs(x = "Modality", y = "Predicted probability\nof correct guess") +
  theme_minimal(base_size = 18)

# Arrange the two panels vertically using grid.arrange
grid.arrange(p_obs, p_pred, ncol = 1)

#save
g <- arrangeGrob(p_obs, p_pred, ncol = 1)

ggsave(paste0(plots, "H4_binaryModel.png"), plot = g, width = 8, height = 8, dpi = 300)
```

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior draws for the fixed effects."), results='hide', warning=FALSE}
# Extract posterior draws for the fixed effects
# For our binary model, the coefficients are on the logit scale.
posterior_fixed <- as.matrix(mdl_H4_binary, variable = c("b_Intercept", "b_modalitygesture", "b_modalityvocal"))

# Plot using mcmc_intervals; we add a dashed vertical line at 0 for reference.
mcmc_intervals(posterior_fixed, prob = 0.95, prob_outer = 0.99) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(
    x = "Coefficient (logit scale)"
  ) +
  theme_minimal(base_size = 18)

```

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior estimates of the fixed effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
gather_draws(mdl_H4_binary, `b_.*`, regex = TRUE) %>%
    # remove the b_ for plotting
    #mutate(.variable = gsub("b_","", .variable)) %>%
    ggplot(aes(x = .value, y = .variable, fill = after_stat(x < 0))) +
    geom_vline(xintercept=0.0, linetype="dotted", color="gray30") +
    stat_halfeye(alpha=0.75, point_interval=median_qi, .width=c(.50, .95)) +
    scale_fill_manual(values = c("lightsalmon", "skyblue"), name=NULL, labels=c("≥ 0", "< 0")) +
    #xlim(-10, 10) +
    scale_y_discrete(labels=c("b_Intercept" = "Intercept α", 
                              "b_modalitygesture" = "Modality: gesture",
                              "b_modalityvocal" = "Modality: vocal")) +
    ylab(NULL) + xlab("Estimate") + 
    theme(axis.title.x = element_text(margin = margin(t = 12, b = 0,
                                                      r = 0, l = 0),
                                      face = 'bold', size = 14),
          axis.text.x = element_text(face = 'bold', size = 12),
          axis.text.y = element_text(face = 'bold', size = 12))+
    NULL;
```


```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior predicted probabilities by outcome group. This density plot is based on individual-level predictions (or a random subset thereof), stratified by guess outcome. The plot shows that predictions for incorrect guesses are tightly concentrated near 0, while predictions for correct guesses are more variable and generally higher, reflecting the model’s ability to distinguish between the two outcomes."), results='hide', warning=FALSE}
# # Take a random subset of the data (e.g., 500 observations)
# df_subset <- df %>% 
#   filter(!is.na(guess_binary)) %>% 
#   sample_n(500)

# Extract posterior draws of the predicted probability for these observations
# The plot takes a while, so if you just need a general overview, use the code above to just take a sample
df_subset %>% 
#df %>% 
  add_epred_draws(mdl_H4_binary, type = "response") %>% 
  ggplot(aes(x = .epred, fill = factor(guess_binary), color = factor(guess_binary))) +
  geom_density(alpha = 0.5, adjust = 1.2) +
  scale_fill_manual(values = c("0" = colorBlindBlack8[1], "1" = colorBlindBlack8[2]),
                    labels = c("incorrect", "correct")) +
  scale_color_manual(values = c("0" = colorBlindBlack8[1], "1" = colorBlindBlack8[2]),
                     labels = c("incorrect", "correct")) +
  labs(
    x = "Posterior predicted probability",
    y = "Density",
    title = "Posterior predicted probabilities by outcome group",
    fill = "Guess", color = "Guess"
  ) +
  theme_minimal(base_size = 18)

# ggsave(plot = last_plot(), filename = paste0(plots, "mdl_H3_binary_postByOutcome.pdf"), width = 6, height = 6);
# ggsave(plot = last_plot(), filename = paste0(plots, "mdl_H3_binary_postByOutcome.jpg"), width = 6, height = 6);
# ggsave(plot = last_plot(), filename = paste0(plots, "mdl_H3_binary_postByOutcome.tif"), width = 6, height = 6, compression="lzw", dpi=600);
```

In this plot, the model’s posterior predicted probability is extracted for each individual observation (or for a randomly selected subset). The density curves are then constructed based on these individual-level predictions, separately for observations that are actually incorrect versus correct. Here, you will typically see that the distribution for incorrect guesses is highly concentrated near 0, indicating that most individual predictions for the incorrect group are very low. On the other hand, the distribution for correct guesses is more spread out, showing more variability in the predicted probabilities. This plot therefore reveals not only the overall difference between the two groups but also the within-group variation in predictions.

**Random effects**:
```{r fig.width=1*4, fig.height=1*7, fig.cap=capFig("Posterior estimates of the random effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
#get_variables(b1_res_field$b1);
grid.arrange(
  # 1 | concept:
  mdl_H4_binary %>%
    spread_draws(b_Intercept, r_concept[concept]) %>%
    #filter(Order == "Intercept") %>%
    mutate(condition_mean = r_concept) %>%
    ggplot(aes(y = concept, x = condition_mean, fill = after_stat(x < 0))) +
    stat_halfeye(alpha=0.75, point_interval=median_qi, .width=c(.50, .95)) +
    scale_fill_manual(values = c("lightsalmon", "skyblue"), name=NULL, labels=c("≥ 0", "< 0")) +
    geom_vline(xintercept = 0, linetype = "dashed") +
    ggtitle("1 | concept") + xlab("Intercept") + ylab(NULL) + xlim(-2,2) +
    theme(legend.position="bottom") +
    NULL,

  nrow=1);
```

Get a general overview of group-level effects.

```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(VarCorr(mdl_H4_binary)$concept$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)

as.data.frame(VarCorr(mdl_H4_binary)$dyad$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)
```

There variance across different concepts is much larger than the variance across different dyads.

We can plot the variance across the concepts extracting draws.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Posterior binary guessability of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
mdl_H4_binary %>%
  spread_draws(r_concept[concept]) %>%
  # Combine the fixed intercept with the concept-specific random effect
  mutate(mu_concept = fixef(mdl_H3_binary)["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform from the logit scale to probabilities
  mutate(guessability = plogis(mu_concept)) %>%
  median_qi(guessability, .width = c(0.5, 0.8, 0.95)) %>%
  ggplot(aes(x = guessability, y = fct_reorder(concept, guessability))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper),
                     position = position_dodge(width = 0.1)) +
  labs(
    x = "Posterior predicted guessability (probability)",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)
```

To validate our model’s group-level estimates, we also plot the raw
data. This plot shows the distribution of binary guessability values for
each concept using a half‐eye plot (which neatly displays medians and
uncertainty intervals) and faceting by concept.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Raw binary guessability of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
df %>% 
  filter(!is.na(guess_binary)) %>% 
  group_by(concept) %>% 
  summarize(prop = mean(as.numeric(as.character(guess_binary))), .groups = "drop") %>% 
  ggplot(aes(x = prop, y = fct_reorder(concept, prop))) +
  geom_point(color = "black", size = 3) +
  labs(
    x = "Observed guessability (proportion correct)",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

**Interpretation**: This model tested whether *guessability* differs by *modality* (combined, gesture, vocal). The intercept of −0.14 on the log‐odds scale corresponds to an overall predicted probability of a correct guess of roughly **46% in the combined** condition (95% credible interval ≈ 38–54%). The coefficient for gesture (−0.03) is very close to zero, indicating almost no difference between combined (about 46%) and gesture (about 45%) -- the posterior probability that combined is higher than gesture is only about 70%, so there is no strong evidence of a difference. By contrast, the coefficient for vocal (−1.85) is markedly negative, producing a **predicted probability of only about 12%** (95% CI roughly 10–15%) in the vocal condition. Posterior hypothesis tests confirm that both combined and gesture yield substantially higher guessability than vocal (nearly 100% posterior certainty), whereas **combined vs. gesture are statistically indistinguishable**.

In practical terms, these results partially support the hypothesis that guessability differs by modality, but the data show no credible difference between combined and gesture; both are much higher than vocal. This means that performing a concept with only vocal signals strongly decreases the probability that a partner guesses correctly, while using either gesture alone or a combined multimodal signal achieves comparable (and substantially better) success rates.

## Guess similarity

Preregistered model:
`guessability_similarity ~ modality + (1 | dyad/participant) + (1 | concept)`

```{r}
mdl_H4_similarity <- brm(
  # Main formula for the μ (mean) component:
  bf(cosine_similarity  ~ modality + (1 | dyad) + (1 | concept),
  # Explicit subformulas for the additional parameters:
  phi ~ 1,                           # Precision parameter (controls concentration)
  zoi ~ 1,                           # Zero–one inflation: probability of coming from the beta part
  coi ~ 1),                            # One inflation: probability of an observation being exactly 1
  data = df,
  family = zero_one_inflated_beta(),
  prior = c(
    # Priors for the μ component (cosine_similarity):
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),  # for expressibility_dutch
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_H4_similarity.rds")
)

# if we need to compress the model more
#saveRDS(mdl_H4_similarity, file = paste0(models, "mdl_H4_similarity.rds"), compress = "xz")

mdl_H4_similarity <- readRDS(paste0(models, "mdl_H4_similarity.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_H4_similarity_R2 <- bayes_R2(mdl_H4_similarity)
# # Save the R² output
# saveRDS(mdl_H4_similarity_R2, file = paste0(models, "mdl_H4_similarity_R2.rds"))

mdl_H4_similarity_R2 <- readRDS(paste0(models, "mdl_H4_similarity_R2.rds"))

mdl_H4_similarity_R2
```


R\^2 is `r mdl_H4_similarity_R2[1]` suggesting that
`r mdl_H4_similarity_R2[1]*100`% of the variance in the outcome variable
are explained by the model.

**Diagnostic plots**:

```{r echo=TRUE, message=FALSE, warning=FALSE}
plot(mdl_H4_similarity)
```

**Convergence**:
All convergence diagnostics are excellent (Rhat = 1.00).

```{r}
summary(mdl_H4_similarity)
```
```{r fig.width=10, fig.height=5, fig.cap=capFig("Trace of the fitting process."), results='hide', warning=FALSE}
mcmc_plot(mdl_H4_similarity, type="trace");
```

**Posterior predictive checks**:
Modeling the density of the bimodal distribution is far from ideal.

```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive density overlay for n=500 draws."), results='hide', warning=FALSE}
pp_check(mdl_H4_similarity, ndraws=500) + xlab('p(Guess)') + ggtitle('Posterior predictive density overlay');
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Posterior predictive distribution."), results='hide', warning=FALSE}
grid.arrange(pp_check(mdl_H4_similarity, type='stat', sta ='min') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of minimum values') + xlim(0,1),
             pp_check(mdl_H4_similarity, type='stat', stat='mean') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of means') + xlim(0,1),
             pp_check(mdl_H4_similarity, type='stat', stat='max') + xlab('Cosine similarity') + ggtitle('Posterior predictive distribution of maximum values') + xlim(0,1),
             ncol=1); 
```

**Fixed effects**:

```{r fig.width=10, fig.height=10, fig.cap=capFig("Fixed effects' posterior distributions."), results='hide', warning=FALSE}
mcmc_plot(mdl_H4_similarity, variable=c("b_Intercept","b_modalitygesture","b_modalityvocal"));
```
```{r fig.width=5, fig.height=5, fig.cap=capFig("Fixed effects."), results='hide', warning=FALSE}
plot_model(mdl_H4_similarity, type="emm", terms=c("modality"));
```
```{r}
emmeans(mdl_H4_similarity, ~ modality)
emmeans(mdl_H4_similarity, ~ modality, type = "response")
pairs(emmeans(mdl_H4_similarity, ~ modality))
pairs(emmeans(mdl_H4_similarity, ~ modality, type = "response"))
```
```{r}
back_transform_zoib(
  fixef(mdl_H4_similarity, dpar = "mu")["Intercept", "Estimate"],
  fixef(mdl_H4_similarity, dpar = "zoi")["Intercept", "Estimate"]
)
```
```{r}
bayestestR::hdi(mdl_H4_similarity, ci=0.95)
hypothesis(mdl_H4_similarity, c("modalitygesture < 0", 
                            "modalityvocal < 0"
                            ))
hypothesis(mdl_H4_similarity, c(
  # Combined > Gesture:
  "inv_logit_scaled(Intercept) > inv_logit_scaled(Intercept + modalitygesture)",
  # Combined > Vocal:
  "inv_logit_scaled(Intercept) > inv_logit_scaled(Intercept + modalityvocal)",
  # Gesture > Vocal:
  "inv_logit_scaled(Intercept + modalitygesture) > inv_logit_scaled(Intercept + modalityvocal)"
))
```
```{r}
# Let's build newdata for each modality level:
newdata_mod <- data.frame(modality = c("combined","gesture","vocal"))

# Posterior predictions (on the response scale) ignoring random effects, or re_formula=NA
post_mod <- add_epred_draws(mdl_H4_similarity, newdata = newdata_mod, re_formula=NA)

# Summarize each condition
post_mod_summary <- post_mod %>%
  group_by(modality) %>%
  mean_qi(.epred, .width=c(0.95))  # .epred is on the response scale (0..1)

post_mod_summary
```

**Intercept**:  
The model yields an estimated intercept for the β‐component (mu submodel) of `r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H4_similarity, dpar = "mu")["Intercept","Estimate"], bayestestR::ci(mdl_H4_similarity, ci=0.95, method="ETI")[1,"CI_low"], bayestestR::ci(mdl_H4_similarity, ci=0.95, method="ETI")[1,"CI_high"])` on the log‐odds scale. Because our model is specified in terms of a zero–one inflated beta, this intercept represents the baseline log‐odds for the beta portion (i.e., the “regular” responses) when **modality** is *combined*. Converting via the inverse‐logit suggests that, ignoring any inflation, the beta part alone predicts roughly `r sprintf("%.1f%%", 100 * plogis(fixef(mdl_H4_similarity, dpar="mu")["Intercept","Estimate"]))` similarity for *combined*.  

However, the zero–one inflation submodels add in extra probability at exactly 0 or 1. For instance, the *zoi* submodel’s intercept is `r sprintf("%.2f", fixef(mdl_H4_similarity, dpar = "zoi")["Intercept","Estimate"])`, which back‐transforms to an inflation probability of about `r sprintf("%.1f%%", 100 * plogis(fixef(mdl_H4_similarity, dpar = "zoi")["Intercept","Estimate"]))`. When we combine this beta portion with the one‐inflation component (*coi*) and potential zeros, the overall predicted similarity for *combined* ends up at about `r sprintf("%.1f%% 95%%CI [%.1f%%, %.1f%%]", 100 * post_mod_summary$.epred[post_mod_summary$modality=="combined"], 100 * post_mod_summary$.lower[post_mod_summary$modality=="combined"], 100 * post_mod_summary$.upper[post_mod_summary$modality=="combined"])`.

**Modality**:  
We include *gesture* and *vocal* as predictors in the mu submodel (combined was the Intercept). The effect of *gesture* is estimated at `r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H4_similarity, dpar="mu")["modalitygesture","Estimate"], bayestestR::ci(mdl_H4_similarity, ci=0.95, method="ETI")[5,"CI_low"], bayestestR::ci(mdl_H4_similarity, ci=0.95, method="ETI")[5,"CI_high"])` on the log‐odds scale, indicating almost no difference from *combined*. Indeed, the overall posterior mean for *gesture* is about `r sprintf("%.1f%% 95%%CI [%.1f%%, %.1f%%]", 100 * post_mod_summary$.epred[post_mod_summary$modality=="gesture"], 100 * post_mod_summary$.lower[post_mod_summary$modality=="gesture"], 100 * post_mod_summary$.upper[post_mod_summary$modality=="gesture"])` (similar to combined).  

By contrast, the coefficient for *vocal* is `r sprintf("%.2f 95%%CI [%.2f, %.2f]", fixef(mdl_H4_similarity, dpar="mu")["modalityvocal","Estimate"], bayestestR::ci(mdl_H4_similarity, ci=0.95, method="ETI")[6,"CI_low"], bayestestR::ci(mdl_H4_similarity, ci=0.95, method="ETI")[6,"CI_high"])`, implying a modest but credible drop in similarity. Indeed, *vocal* yields a mean similarity of `r sprintf("%.1f%% 95%%CI [%.1f%%, %.1f%%]", 100 * post_mod_summary$.epred[post_mod_summary$modality=="vocal"], 100 * post_mod_summary$.lower[post_mod_summary$modality=="vocal"], 100 * post_mod_summary$.upper[post_mod_summary$modality=="vocal"])`.

Consequently, our model finds that **combined** and **gesture** are virtually identical in their predicted cosine similarity, while **vocal** is somewhat lower (a ~4% difference on average). Overall, the zero–one inflated beta structure provides a good fit, as evidenced by the predicted values aligning with the empirical distribution of similarity.


```{r fig.width=2*4, fig.height=1*7, fig.cap=capFig("Top: MUST BE FIXED Bottom: Model-predicted probability of a correct guess as a function of expressibility. The plot shows the median predicted probability along with uncertainty bands, illustrating that higher expressibility is associated with a substantially increased probability of a correct guess."), results='hide', warning=FALSE}
# Panel 1: Averaged cosine similarity x expressibility
p_obs <- df %>% 
  filter(!is.na(cosine_similarity)) %>% 
  ggplot(aes(y = cosine_similarity, fill = factor(exp_part))) +
  geom_boxplot(alpha = 0.5) +
  scale_fill_manual(values = c("1" = colorBlindBlack8[1], "2" = colorBlindBlack8[2]),
                    labels = c("2", "1")) +
  labs(y = "Cosine similarity", y = "Modality", fill = "Modality") +
  theme_minimal(base_size = 18)

# Panel 2: Conditional effects of expressibility_dutch on predicted probability
ce_exp <- conditional_effects(mdl_H4_similarity, effects = "modality")
p_pred <- plot(ce_exp, plot = FALSE)[[1]] +
  labs(x = "Modality", y = "Predicted probability\nof correct guess") +
  theme_minimal(base_size = 18)

# Arrange the two panels vertically using grid.arrange
grid.arrange(p_obs, p_pred, ncol = 1)
```

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior draws for the fixed effects."), results='hide', warning=FALSE}
# Extract posterior draws for the fixed effects
# For our binary model, the coefficients are on the logit scale.
posterior_fixed <- as.matrix(mdl_H4_similarity, variable = c("b_Intercept", "b_modalitygesture", "b_modalityvocal"))

# Plot using mcmc_intervals; we add a dashed vertical line at 0 for reference.
mcmc_intervals(posterior_fixed, prob = 0.95, prob_outer = 0.99) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(
    x = "Coefficient (logit scale)"
  ) +
  theme_minimal(base_size = 18)

```

```{r fig.width=10, fig.height=10, fig.cap=capFig("Posterior estimates of the fixed effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
gather_draws(mdl_H4_similarity, `b_.*`, regex = TRUE) %>%
    # remove the b_ for plotting
    filter(.variable == c("b_Intercept", "b_modalitygesture", "b_modalityvocal")) %>%
    ggplot(aes(x = .value, y = .variable, fill = after_stat(x < 0))) +
    geom_vline(xintercept=0.0, linetype="dotted", color="gray30") +
    stat_halfeye(alpha=0.75, point_interval=median_qi, .width=c(.50, .95)) +
    scale_fill_manual(values = c("lightsalmon", "skyblue"), name=NULL, labels=c("≥ 0", "< 0")) +
    #xlim(-10, 10) +
    scale_y_discrete(labels=c("b_Intercept" = "Intercept α", 
                              "b_modalitygesture" = "Modality: gesture",
                              "b_modalityvocal" = "Modality: vocal")) +
    ylab(NULL) + xlab("Estimate") + 
    theme(axis.title.x = element_text(margin = margin(t = 12, b = 0,
                                                      r = 0, l = 0),
                                      face = 'bold', size = 14),
          axis.text.x = element_text(face = 'bold', size = 12),
          axis.text.y = element_text(face = 'bold', size = 12))+
    NULL;
```

**Random effects**:

```{r fig.width=1*4, fig.height=1*7, fig.cap=capFig("Posterior estimaes of the random effects showing the median (dot), 50% (thick line) and 95% (thin lines) quantiles. Also showing 0 as the vertical dotted line and negative (blue) vs positive (red) values."), results='hide', warning=FALSE}
#get_variables(b1_res_field$b1);
grid.arrange(
  # 1 | concept:
  mdl_H4_similarity %>%
  spread_draws(b_Intercept, r_concept[concept]) %>%
  # Add the fixed intercept for mu to the concept-specific random effect:
  mutate(mu_concept = fixef(mdl_H4_similarity, dpar = "mu")["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform to the original scale using the common inflation parameter from zoi:
  mutate(guessability = back_transform_zoib(mu_concept, fixef(mdl_H3_similarity, dpar = "zoi")["Intercept", "Estimate"])) %>%
  ggplot(aes(y = concept, x = guessability)) +
  stat_halfeye(alpha = 0.75, point_interval = median_qi, .width = c(0.50, 0.95)) +
  # scale_fill_manual(values = c("lightsalmon", "skyblue"),
  #                   name = NULL, labels = c("≥ 0.5", "< 0.5")) +
  #geom_vline(xintercept = 0.5, linetype = "dashed") +
  #ggtitle("Posterior estimates of concept-level\ncosine similarity") +
  xlab("Posterior predicted cosine similarity") +
  ylab(NULL) +
  xlim(0, 1) +
    theme(legend.position="bottom") +
    NULL,

  nrow=1);
```

Get a general overview of the group-level effects.

```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(VarCorr(mdl_H4_similarity)$concept$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)

as.data.frame(VarCorr(mdl_H4_similarity)$dyad$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)
```

There is more variance across different concepts than across different dyads.

We can plot the variance across the concepts extracting draws.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Posterior cosine similarity of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
# 1 | concept
mdl_H4_similarity %>%
  spread_draws(r_concept[concept, dyad]) %>%
  # Combine the fixed intercept (mu) with the concept-specific random effect.
  mutate(mu_concept = fixef(mdl_H4_similarity, dpar = "mu")["Intercept", "Estimate"] + r_concept) %>%
  # Back-transform to the response scale using the common inflation parameter:
  mutate(guessability = back_transform_zoib(mu_concept, fixef(mdl_H3_similarity, dpar = "zoi")["Intercept", "Estimate"])) %>%
  median_qi(guessability, .width = c(0.5, 0.8, 0.95)) %>%
  ggplot(aes(x = guessability, y = fct_reorder(concept, guessability))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper), position = position_dodge(width = 0.1)) +
  labs(
    x = "Posterior guessability (cosine similarity)",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

To validate our model’s group-level estimates, we also plot the raw
data. This plot shows the distribution of cosine similarity values for
each concept using a half‐eye plot (which neatly displays medians and
uncertainty intervals) and faceting by concept.

```{r fig.width=4, fig.height=1*7, fig.cap=capFig("Raw cosine similarity of concepts showing the median (dot), 50% (thick line), 80% (medium thick line), and 95% (thin lines) quantiles."), results='hide', warning=FALSE}
df %>% 
  filter(!is.na(cosine_similarity)) %>% 
  group_by(concept) %>% 
  median_qi(cosine_similarity, .width = c(0.5, 0.8, 0.95)) %>% 
  ggplot(aes(x = cosine_similarity, y = fct_reorder(concept, cosine_similarity))) +
  geom_pointinterval(aes(xmin = .lower, xmax = .upper), 
                     position = position_dodge(width = 0.1)) +
  labs(
    x = "Raw cosine similarity",
    y = "Concept"
  ) +
  theme_minimal(base_size = 18)

```

**Interpretation**: This zero–one inflated beta model tested whether cosine similarity (i.e., how close the guesser’s label is to the target) differs by modality (combined, gesture, vocal). The baseline (combined) intercept is around 0.80 on the log‐odds scale, corresponding to a mean predicted similarity of roughly 79% (95% CI [78%, 80%]). The coefficient for gesture (−0.01) is very close to zero, indicating that gesture alone yields virtually the same similarity as combined (around 79%, 95% CI [78%, 80%]). By contrast, the coefficient for vocal (−0.29) is credibly negative, producing a predicted mean similarity of 75% (95% CI [74%, 76%])—still high, but clearly lower than either combined or gesture. Posterior hypothesis tests confirm that combined ≈ gesture, both higher than vocal with near‐100% posterior certainty. Thus, while the difference is modest in absolute terms (79% vs. 75%), the vocal condition does reduce guess similarity compared to using gesture or a combined signal, suggesting that **vocal‐only expressions may be slightly less transparent to a guesser**.


# Combined H1, H3, and H4

## Guess binary

```{r}
if( !file.exists(paste0(models, "mdl_combined_res.RData")) )
{
  # the model:
  mdl_combined <- brm(guess_binary ~ 1 + 
                        expressibility_dutch + exp_part + modality +
                        (1 | dyad) + (1 | concept),
                      data = df,
                      family = bernoulli(link = 'logit'),
                      prior = c( brms::set_prior("student_t(5, 0, 2.5)", class = "Intercept"),
                                 brms::set_prior("student_t(5, 0, 1)", class = "b"),
                                 brms::set_prior("normal(0, 2)", class = "sd")
                      ),
                      sample_prior = TRUE,  # needed for hypotheses tests
                      seed = 998, cores = brms_ncores, iter = 10000, warmup = 4000, thin = 2,
                      control = list(adapt_delta = 0.99, max_treedepth = 13));
  
  summary(mdl_combined); 
  mcmc_plot(mdl_combined, type="trace"); mcmc_plot(mdl_combined);
  bayestestR::hdi(mdl_combined, ci=0.95);
  hypothesis(mdl_combined, c("expressibility_dutch > 0", 
                             "exp_part2 > 0", 
                             "modalitygesture < modalitycombined", 
                             "modalityvocal < modalitygesture")); 
  # posterior predictive checks:
  pp_check(mdl_combined, ndraws = 100) + xlab('p(guess)') + ggtitle('Posterior predictive density overlay');
  grid.arrange( pp_check(mdl_combined, type = 'stat', sta = 'min') + xlab('p(guess)') + ggtitle('Posterior predictive distribution of minimum values') + xlim(0,1),
                pp_check(mdl_combined, type = 'stat', stat = 'mean') + xlab('p(guess)') + ggtitle('Posterior predictive distribution of means') + xlim(0,1),
                pp_check(mdl_combined, type = 'stat', stat = 'max') + xlab('p(guess)') + ggtitle('Posterior predictive distribution of maximum values') + xlim(0,1),
                ncol = 1);
  
  # save the model:
  mdl_combined_res <- mdl_combined;
  saveRDS(mdl_combined_res, paste0(models, "mdl_combined_res.RData"), compress = "xz");
} else {
  mdl_combined_res <- readRDS(paste0(models, "mdl_combined_res.RData"));
}

```

**FIX**
Our combined model estimates an intercept of
`r round(fixef(mdl_combined_res)["Intercept", "Estimate"], 2)`
on the logit scale (which back–transforms to a baseline probability of
`r round(plogis(fixef(mdl_combined_res)["Intercept", "Estimate"]), 3))`, and a slope for `expressibility_dutch` of
`r round(fixef(mdl_combined_res)["expressibility_dutch", "Estimate"], 2)`, meaning that for each one–unit increase in `expressibility_dutch` the odds of a correct guess increase by a factor of approximately
`r round(exp(fixef(mdl_combined_res)["expressibility_dutch", "Estimate"]), 2)`.
Moreover, the positive coefficient for `exp_part2` and the differences among modality levels further indicate that, controlling for other factors, higher expressibility and the experimental conditions both increase guessability. Convergence diagnostics are excellent (Rhat ≤ 1.01).

```{r}
mcmc_plot(mdl_combined_res, type="trace")
```

```{r}
grid.arrange( plot_model(mdl_combined_res, type="emm", terms=c("expressibility_dutch")),
              plot_model(mdl_combined_res, type="emm", terms=c("exp_part")),
              plot_model(mdl_combined_res, type="emm", terms=c("modality")),
              ncol=2);

```



# Session info

```{r echo=TRUE, message=FALSE, warning=FALSE}
sessionInfo()
```
