---
title: "The relationship between expectation and performance: Methodological evaluation"
author: "Aleksandra Ćwiek, Wim Pouw, Susanne Fuchs, Šárka Kadavá"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  rmarkdown::html_document:
    theme: readable
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float: yes
    df_print: paged
    code_folding: hide
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '3'
  html_notebook:
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This is the modeling of expressibility vs. guessability, preregistered
at [AsPredicted](https://aspredicted.org/kmry-vx5s.pdf).

This script uses the output of `1_descriptive.Rmd` and prodcues all the
values and plots reported in the paper.

# Data preparation

## Setup

```{r source setup, echo = TRUE, message=FALSE, warning = FALSE}

########## folders ##########
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

parentfolder <- dirname(getwd())

rawdata       <- paste0(parentfolder, '/rawdata/');
dataset       <- paste0(parentfolder, '/dataset/');
models        <- paste0(parentfolder, '/models/');
plots         <- paste0(parentfolder, '/plots/');
scripts       <- paste0(parentfolder, '/scripts/');

########## source file ##########

#rmarkdown::render("1_descriptive.Rmd", envir = globalenv(), clean = FALSE)

#################### packages ####################
# Data Manipulation
library(tidyverse);

# Plotting
library(corrplot);
library(cowplot);
library(ggdist);

# Bayesian
library(brms);
library(cmdstanr);
library(emmeans);
library(posterior);
library(bayesplot);
library(tidybayes);

theme_set(theme_tidybayes() + panel_border());


# use all available cores for parallel computing
options(mc.cores = parallel::detectCores());

colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                       "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

```

## Load in data frames

```{r read metadata, echo=TRUE, message=FALSE, warning=FALSE}
# Load data frame
df <- read_csv(paste0(dataset, "df.csv"))

# Load concept list
concepts <- readxl::read_excel(paste0(dataset, "conceptlist_info.xlsx"))

# Load expressibility German
expr_german <- read_csv(paste0(dataset, "expressibility_german.csv"))

# Load expressibility Dutch
expr_dutch <- read_csv(paste0(dataset, "expressibility_dutch.csv"))

```

## Helper functions

```{r}
back_transform_zoib <- function(eta, zoi, coi = 1) {
  plogis(eta) * (1 - zoi) + zoi * coi
}

# Helper function for LOO:
get_save_loo <- function(model, file_name) {
  file_path <- paste0(models, file_name)
  if (!file.exists(file_path)) {
    cat("Computing LOO for", file_name, "...\n")
    loo_val <- loo(model)
    saveRDS(loo_val, file = file_path)
  } else {
    cat("Loading saved LOO for", file_name, "...\n")
    loo_val <- readRDS(file_path)
  }
  return(loo_val)
}

# Helper function for WAIC:
get_save_waic <- function(model, file_name) {
  file_path <- paste0(models, file_name)
  if (!file.exists(file_path)) {
    cat("Computing WAIC for", file_name, "...\n")
    waic_val <- waic(model)
    saveRDS(waic_val, file = file_path)
  } else {
    cat("Loading saved WAIC for", file_name, "...\n")
    waic_val <- readRDS(file_path)
  }
  return(waic_val)
}

```

## Information from the preregistration

We will use weakly informative priors for the fixed and random effect,
just informing the models about the ranges of the scale of the outcome
variable.

The `guessability_match` is a categorical variable and will be coded as
`0` for incorrect guess and `1` for correct guess. The
`guessability_similarity` will be a continuous variable expressing the
cosine similarity of guesser's answer to target (estimated using
ConceptNet). The `N_repetition` encompasses the data from the second
half of the experiment only (where participants could repeat the
production) and represents a number of repetitions until the production
was correctly guessed or until a maximum of three productions was
reached. The `experiment_part` is a categorical variable coded as `1`
and `2`, for the respective halves of the experiment. `Modality` is a
categorical variable with three levels: `combined`, `gesture`, and
`vocalization`.

For `N_repetion`, we will start with a Poisson model and check residuals
as well as compare the mean and variance. If overdispersion is present,
we will switch to negbinomial family.

For the continuous `guessability_similarity` outcome variable, we will
test the best fitting family by inspecting the distribution of the
outcome variable. We do this because the data might not be normally
distributed. We will choose an optimal family among gamma, lognormal, or
skew_normal if the normal is inappropriate.

# Prepare data

## Sample size

Calculate the effective sample size for modeling.

```{r message=FALSE, warning=FALSE, include=FALSE}
nrow(df)

df %>%
  group_by(concept) %>%
  summarise(frequency = n()) %>%
  arrange(desc(frequency)) %>% 
  print(n = 100)
```

There are a total of `r nrow(df)` novel productions, from that,
`r nrow(filter(df, exp_part == "1"))` in experiment part 1 and
`r nrow(filter(df, exp_part == "2"))` in experiment part 2.

In experiment part 2, there are
`r nrow(filter(df, correction == "0" & exp_part == "2"))` data points of
the very first production,
`r nrow(filter(df, correction == "1" & exp_part == "2"))` data points of
the first repetition/correction (i.e., second production), and
`r nrow(filter(df, correction == "2" & exp_part == "2"))` data points of
the second repetition/correction (i.e., third production).

Across modalities, there are
`r nrow(filter(df, modality == "combined"))` productions in combined
(`r nrow(filter(df, modality == "combined" & exp_part == "1"))` in part
1 vs. `r nrow(filter(df, modality == "combined" & exp_part == "2"))` in
part 2), `r nrow(filter(df, modality == "gesture"))` in gesture
(`r nrow(filter(df, modality == "gesture" & exp_part == "1"))` in part 1
vs. `r nrow(filter(df, modality == "gesture" & exp_part == "2"))` in
part 2), and `r nrow(filter(df, modality == "vocal"))` in vocal modality
(`r nrow(filter(df, modality == "vocal" & exp_part == "1"))` in part 1
vs. `r nrow(filter(df, modality == "vocal" & exp_part == "2"))` in part
2).

## Cosine similarity

The cosine similarity was calculated with ConceptNet. Some pairs (N =
130, \~2% of the data) were not available in ConceptNet and for those we
asked participants (HOW MANY) to rate the perceived similarity. On a
smaller set of pairs, we previously checked whether this is an effective
way to reduce the missing values, and we found that ConceptNet and
perceived similarity ratings are highly correlated (r = ????). The data
reviewed below already entails the perceived similarity.

Cosine similarity is inherently on -1 to 1 scale. As provided
[here](https://github.com/deepinsight/insightface/issues/917):

> Cosine similarity is like an inner product. If angle between two
> vector is larger than 90 degree, the value is negative, and that means
> that two faces(features) are clearly distinguishable.

Meaning that a negative value is provided for concepts that are clearly
different (i.e., not confusable).

In order to be able to model the values with a wider choice of families
(including zero-one inflated beta), we transformed the cosine similarity
with

`(df$cosine_similarity + 1) / 2`

This transformation resulted in data distributed in the following way

```{r}
summary(df$cosine_similarity)
```


Below, we break down the missing values and look at data distribution to
choose an appropriate family.

```{r}
# Total missing values for cosine_similarity and its percentage
cat("Total missing values for cosine_similarity: ", sum(is.na(df$cosine_similarity)), "\n")
cat("Percentage missing: ", round(100 * sum(is.na(df$cosine_similarity)) / nrow(df), 1), "%\n\n")

# Breakdown by modality:
cat("Missing values by modality:\n")
df %>% 
  group_by(modality) %>% 
  summarise(
    missing = sum(is.na(cosine_similarity)),
    total = n(),
    percent_missing = round(100 * sum(is.na(cosine_similarity)) / n(), 1)
  ) %>% 
  print(n = Inf)

cat("\nBreakdown by experiment part (exp_part):\n")
df %>% 
  group_by(exp_part) %>% 
  summarise(
    missing = sum(is.na(cosine_similarity)),
    total = n(),
    percent_missing = round(100 * sum(is.na(cosine_similarity)) / n(), 1)
  ) %>% 
  print(n = Inf)

cat("\nBreakdown by repetition (correction):\n")
df %>% 
  group_by(correction) %>% 
  summarise(
    missing = sum(is.na(cosine_similarity)),
    total = n(),
    percent_missing = round(100 * sum(is.na(cosine_similarity)) / n(), 1)
  ) %>% 
  print(n = Inf)
```

Let us inspect the distribution of cosine similarity

```{r}
ggplot(df, aes(x = cosine_similarity)) +
  geom_density(fill = colorBlindBlack8[6]) +
  theme_minimal() +
  labs(x = "Cosine Similarity", y = "Density")

moments::skewness(df$cosine_similarity)
```

The distribution is fairly bimodal. The peak at 1 results from obvious
100% match cases that correspond to binary guess = 1. Then there is
another peak around 0.5 which results from non-ideal guesses.

```{r}
ggplot(df, aes(x = cosine_similarity, fill = factor(guess_binary))) +
  geom_density(alpha = 0.6) +  # alpha to make the fill slightly transparent
  theme_minimal() +
  labs(x = "Cosine Similarity", y = "Density", fill = "Guess binary") +
  scale_fill_manual(values = c(colorBlindBlack8[6], colorBlindBlack8[7])) 
```

The family that will fit this data best is one-inflated beta. In the brms framework, we don’t have a separate family for one‐inflated beta, but we can force it by fixing the zero‐inflation parameter (zoi) to 1 (or nearly 1). That way, the model assumes no probability mass at 0, and all of the inflation is allocated to the one‐inflation (coi) component.

Let's compute the summary statistics of cosine similarity grouped by
guess_binary

```{r}
df %>%
  group_by(guess_binary) %>%
  summarise(
    mean_cosine_similarity = mean(cosine_similarity, na.rm = TRUE),
    sd_cosine_similarity = sd(cosine_similarity, na.rm = TRUE),
    min_cosine_similarity = min(cosine_similarity, na.rm = TRUE),
    max_cosine_similarity = max(cosine_similarity, na.rm = TRUE),
    median_cosine_similarity = median(cosine_similarity, na.rm = TRUE),
    n = n()  # Count of observations
  )

```

## Variables

Our hypotheses revolve around a set of fixed and random predictors for
different outcome variables.

The outcome variables in the data frame `df` are:

-   `guess_binary`: a *binary categorical* variable coded as `0` for
    incorrect guess and `1` for correct guess
-   `cosine_similarity`: a *continuous* variable expressing the cosine
    similarity of guesser's answer to target (estimated using
    ConceptNet)
-   `correction`: a *categorical* variable with three levels `0` for the
    first production, `1` for the first correction, `2` for the second
    correction

The predictor variables in the data frame `df` are:

-   `expressibility`: a *continuous* variable expressing the rated
    posterior expressibility, set for each concept in each modality
-   `exp_part`: a *binary categorical* variable coded as `1` for the
    first experiment part and `2` for the second experiment part
-   `modality`: a *categorical* variable with three levels `combined`
    for both gesture and vocal, `gesture` for gesture only, `vocal` for
    vocal only

The random variables in the data frame `df` are:

-   `concept`: a *categorical* variable for individual concepts (a total
    of `r unique(df$concept)`)
-   `dyad`: a *categorical* variable for individual dyads (a total of
    `r unique(df$dyad)`)
-   `participant_ID` (nested within `dyad`): a *categorical* variable
    for individual participants (a total of
    `r unique(df$participant_ID)`)

Preparing the variables

```{r}
# Outcome variables
#df$guess_binary <- factor(df$guess_binary, levels = c("0", "1")); contrasts(df$guess_binary) <- c(-0.5, +0.5)
## 'cosine_similarity' is continuous, no need to transform it
#df$correction <- factor(df$correction, levels = c("0", "1", "2"))

# Predictor variables
## 'expressibility' is continuous, no transformation needed
df$exp_part <- factor(df$exp_part, levels = c("1", "2")); contrasts(df$exp_part) <- c(-0.5, +0.5)
df$modality <- factor(df$modality, levels = c("combined", "gesture", "vocal"))

# Random variables
df$concept <- factor(df$concept)
df$dyad <- factor(df$dyad)
df$participant_ID <- factor(df$participant_ID)


# Check the balance between the levels

df %>% count(guess_binary) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 
# imbalanced 0: 67.5%, 1: 32.5%

df %>% count(correction) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 
# imbalanced 0: 66.2%, 1: 19.9%, 2: 13.9%

df %>% count(exp_part) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 
# imbalanced 1: 33%, 2: 67%

df %>% count(modality) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 
# pretty balanced, combined: 31%, gesture: 31.1%, vocal: 37.9%

# Random variables just if someone is interested in checking
# df %>% count(concept) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 
# df %>% count(dyad) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 
# df %>% count(participant_ID) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%")) 

```

# Intercept-only guessability

## Guess similarity

Preregistered model:

`guessability_similarity ~ 1 + (1 | dyad/participant) + (1 | concept)`

### Model

Now, let’s build the intercept-only model for the continuous outcome,
cosine_similarity. The model uses the same random effects structure.
Because of the data distribution, we use a zero-one inflated beta model minimizing the zoi component.

#### Best-fit 

##### Nested

With priors for ZOIB

```{r}
mdl_null_similarity_nested_pior <- brm(
  bf(
    cosine_similarity ~ 1 + (1 | dyad/participant_ID) + (1 | concept),
    phi ~ 1,
    zoi ~ 1,
    coi ~ 1
  ),
  data = df,
  family = zero_one_inflated_beta(),
  prior = c(
    # Priors for the main (mu) component:
    prior(normal(0, 5), class = "Intercept"),
    prior(normal(0, 2), class = "sd"),
    # Prior for the phi parameter (precision)
    prior(normal(0, 5), class = "Intercept", dpar = "phi"),
    # Force zoi to be essentially 1, meaning P(y = 0) ~ 0:
    prior(normal(10, 0.001), class = "Intercept", dpar = "zoi"),
    # A neutral prior for coi: centered at 0 (which implies plogis(0)=0.5) with SD=2.
    prior(normal(0, 2), class = "Intercept", dpar = "coi")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12, adapt_delta = 0.99),
  file = paste0(models, "mdl_null_similarity_nested_prior.rds")
)

# if we need to compress the model more
#saveRDS(mdl_null_similarity_nested_prior, file = paste0(models, "mdl_null_similarity_nested_prior.rds"), compress = "xz")

mdl_null_similarity_nested_prior <- readRDS(paste0(models, "mdl_null_similarity_nested_prior.rds"))
```


Without priors for ZOIB

```{r}
mdl_null_similarity_nested <- brm(
  bf(
    cosine_similarity ~ 1 + (1 | dyad/participant_ID) + (1 | concept),
    phi ~ 1,
    zoi ~ 1,
    coi ~ 1
  ),
  data = df,
  family = zero_one_inflated_beta(),
  prior = c(
    # Priors for the main (mu) component:
    prior(normal(0, 5), class = "Intercept"),
    prior(normal(0, 2), class = "sd")#,
    # Prior for the phi parameter (precision)
    #prior(normal(0, 5), class = "Intercept", dpar = "phi"),
    # Force zoi to be essentially 1, meaning P(y = 0) ~ 0:
    #prior(normal(10, 0.001), class = "Intercept", dpar = "zoi"),
    # A neutral prior for coi: centered at 0 (which implies plogis(0)=0.5) with SD=2.
    #prior(normal(0, 2), class = "Intercept", dpar = "coi")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12, adapt_delta = 0.99),
  file = paste0(models, "mdl_null_similarity_nested.rds")
)

# if we need to compress the model more
#saveRDS(mdl_null_similarity_nested, file = paste0(models, "mdl_null_similarity_nested.rds"), compress = "xz")

mdl_null_similarity_nested <- readRDS(paste0(models, "mdl_null_similarity_nested.rds"))
```

##### No nesting

```{r}
mdl_null_similarity_simple <- brm(
  bf(
    cosine_similarity ~ 1 + (1 | dyad) + (1 | concept),
    phi ~ 1,
    zoi ~ 1,
    coi ~ 1
  ),
  data = df,
  family = zero_one_inflated_beta(),
  prior = c(
    # Priors for the main (mu) component:
    prior(normal(0, 5), class = "Intercept"),
    prior(normal(0, 2), class = "sd")#,
    # Prior for phi (precision)
    #prior(normal(0, 5), class = "Intercept", dpar = "phi"),
    # Force zoi nearly 1 so that there’s no mass at 0:
    #prior(normal(10, 0.001), class = "Intercept", dpar = "zoi"),
    # A neutral prior for coi: centered at 0 (which implies plogis(0)=0.5) with SD=2.
    #prior(normal(0, 2), class = "Intercept", dpar = "coi")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12, adapt_delta = 0.99),
  file = paste0(models, "mdl_null_similarity_simple.rds")
)

# if we need to compress the model more
#saveRDS(mdl_null_similarity_simple, file = paste0(models, "mdl_null_similarity_simple.rds"), compress = "xz")

mdl_null_similarity_simple <- readRDS(paste0(models, "mdl_null_similarity_simple.rds"))
```

##### Compare

```{r}
mdl_null_similarity_nested_prior
mdl_null_similarity_nested
mdl_null_similarity_simple
```



Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Calculate Bayesian R²
mdl_null_similarity_nested_prior_R2 <- bayes_R2(mdl_null_similarity_nested_prior)
mdl_null_similarity_nested_R2 <- bayes_R2(mdl_null_similarity_nested)
mdl_null_similarity_simple_R2 <- bayes_R2(mdl_null_similarity_simple)

# Save the R² output
saveRDS(mdl_null_similarity_nested_prior_R2, file = paste0(models, "mdl_null_similarity_nested_prior_R2.rds"))
saveRDS(mdl_null_similarity_nested_R2, file = paste0(models, "mdl_null_similarity_nested_R2.rds"))
saveRDS(mdl_null_similarity_simple_R2, file = paste0(models, "mdl_null_similarity_simple_R2.rds"))

mdl_null_similarity_nested_prior_R2 <- readRDS(paste0(models, "mdl_null_similarity_nested_prior_R2.rds"))
mdl_null_similarity_nested_R2 <- readRDS(paste0(models, "mdl_null_similarity_nested_R2.rds"))
mdl_null_similarity_simple_R2 <- readRDS(paste0(models, "mdl_null_similarity_simple_R2.rds"))

mdl_null_similarity_nested_prior_R2 # barely any explanatory power
mdl_null_similarity_nested_R2
mdl_null_similarity_simple_R2
```

All the models are very weak in explaining the data, however the one with the ZOIB priors is extremely weak. One argument not to use the priors for family-specific parameters.

```{r}
# Compute and save LOO for both models:
loo_nested_prior  <- get_save_loo(mdl_null_similarity_nested_prior, "mdl_null_similarity_nested_prior_loo.rds")
loo_nested  <- get_save_loo(mdl_null_similarity_nested, "mdl_null_similarity_nested_loo.rds")
loo_simple  <- get_save_loo(mdl_null_similarity_simple, "mdl_null_similarity_simple_loo.rds")

# Compute and save WAIC for both models:
waic_nested_prior <- get_save_waic(mdl_null_similarity_nested_prior, "mdl_null_similarity_nested_prior_waic.rds")
waic_nested <- get_save_waic(mdl_null_similarity_nested, "mdl_null_similarity_nested_waic.rds")
waic_simple <- get_save_waic(mdl_null_similarity_simple, "mdl_null_similarity_simple_waic.rds")

# Compare LOO and WAIC between models:
print(loo_compare(loo_nested_prior, loo_nested))
print(loo_compare(loo_nested, loo_simple))
print(loo_nested_prior)
print(loo_nested)
print(loo_simple)
print(waic_nested)
print(waic_simple)

```


### Best-fit: No nesting

#### Fixed effects

Summarize the model: Check parameter estimates, Rhat values
(convergence) and effective sample sizes.

```{r}
summary(mdl_null_similarity_nested)

back_transform_zoib(
  fixef(mdl_null_similarity_simple, dpar = "mu")["Intercept", "Estimate"],
  fixef(mdl_null_similarity_simple, dpar = "zoi")["Intercept", "Estimate"]
)
```

Estimated posterior similarity guessability is
`r back_transform_zoib(fixef(mdl_null_similarity, dpar = "mu")["Intercept", "Estimate"], fixef(mdl_null_similarity, dpar = "zoi")["Intercept", "Estimate"])`.

Diagnostic plots.

```{r echo=TRUE, message=FALSE, warning=FALSE}
plot(mdl_null_similarity)
```

Posterior predictive check.

```{r echo=TRUE, message=FALSE, warning=FALSE}
pp_check(mdl_null_similarity, ndraws = 100)

pp_check(mdl_null_similarity, type = "error_scatter_avg", ndraws = 100)
```

#### Group-level effects

First, get a general overview.

```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(VarCorr(mdl_null_similarity)$concept$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)
```

THINK A BIT MORE WHAT TO DO WITH THE INTERCEPT ONLY MODEL.

## Guess binary

Preregistered model:

`guessability_match ~ 1 + (1 | dyad/participant) + (1 | concept), family = bernoulli(link='logit')`

### Model

Intercept-only model for binary outcome

```{r}
mdl_null_binary <- brm(
  formula = guess_binary ~ 1 + (1 | dyad) + (1 | concept),
  data = df,
  family = bernoulli(link = "logit"),
  prior = c(
    # Prior for the intercept on the logit scale:
    prior(normal(0, 2.5), class = "Intercept"),
    # Prior for the standard deviations of the random effects:
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_null_binary.rds")
)

# if we need to compress the model more
#saveRDS(mdl_null_binary, file = paste0(models, "mdl_null_binary.rds"), compress = "xz")

mdl_null_binary <- readRDS(paste0(models, "mdl_null_binary.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_null_binary_R2 <- bayes_R2(mdl_null_binary)
# # Save the R² output
# saveRDS(mdl_null_binary_R2, file = paste0(models, "mdl_null_binary_R2.rds"))

mdl_null_binary_R2 <- readRDS(paste0(models, "mdl_null_binary_R2.rds"))

mdl_null_binary_R2
```

R2 explanation.......

#### Fixed effects

Summarize the model: Check parameter estimates, Rhat values
(convergence) and effective sample sizes.

```{r}
summary(mdl_null_binary)

# because of the logit link, it needs to be backtransformed
emmeans(mdl_null_binary, ~1, type = "response")
```

Estimated posterior binary guessability is
`r plogis(fixef(mdl_null_binary)[1])` (ADD CrI). The raw binary
guessability and intervals...

Diagnostic plots.

```{r echo=TRUE, message=FALSE, warning=FALSE}
plot(mdl_null_binary)
```

Posterior predictive check.

```{r echo=TRUE, message=FALSE, warning=FALSE}
pp_check(mdl_null_binary, ndraws = 100)

pp_check(mdl_null_binary, type = "error_scatter_avg", ndraws = 100)
```

#### Group-level effects

First, get a general overview.

```{r echo=FALSE, message=FALSE, warning=FALSE}
as.data.frame(VarCorr(mdl_null_binary)$concept$sd) %>%
  rownames_to_column(var = "Random Effect") %>%
  select(`Random Effect`, Estimate, Est.Error, Q2.5, Q97.5)
```

THINK A BIT MORE WHAT TO DO WITH THE INTERCEPT ONLY MODEL.


# Hypothesis 1: Expressibility vs. guessability

*The higher the expressibility rating of the concept for the modality in
which the concept is produced, the higher the guessability will be.*

## Guess binary

Preregistered model:

`guessability_match ~ expressibility + (1 | dyad/participant) + (1 | concept), family = bernoulli(link='logit')`

### Model

```{r}
mdl_H1_binary <- brm(
  formula = guess_binary ~ expressibility_dutch + (1 | dyad) + (1 | concept),
  data = df,
  family = bernoulli(link = "logit"),
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),        # Prior for expressibility
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_H1_binary.rds")
)

# if we need to compress the model more
#saveRDS(mdl_H1_binary, file = paste0(models, "mdl_H1_binary.rds"), compress = "xz")

mdl_H1_binary <- readRDS(paste0(models, "mdl_H1_binary.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_H1_binary_R2 <- bayes_R2(mdl_H1_binary)
# # Save the R² output
# saveRDS(mdl_H1_binary_R2, file = paste0(models, "mdl_H1_binary_R2.rds"))

mdl_H1_binary_R2 <- readRDS(paste0(models, "mdl_H1_binary_R2.rds"))

mdl_H1_binary_R2
```

R2 explanation.......

#### Fixed effects

Summarize the model: Check parameter estimates, Rhat values
(convergence) and effective sample sizes.

```{r}
summary(mdl_H1_binary)
```

Our model predicting binary guessability indicated a strong positive effect of expressibility (β = 7.30, 95% CI [6.80, 7.81]). The baseline log-odds for a correct guess were very low (Intercept = -5.34, 95% CI [-5.73, -4.95]), corresponding to a predicted probability of approximately 0.48% when expressibility is at its reference level. Variability across concepts (SD = 0.92) was considerably larger than across dyads (SD = 0.21), indicating that the inherent properties of the concepts substantially influence guessability. All model diagnostics indicated good convergence (Rhat = 1.00).

Diagnostic plots.

```{r echo=TRUE, message=FALSE, warning=FALSE}
plot(mdl_H1_binary)
```

Posterior predictive check.

```{r echo=TRUE, message=FALSE, warning=FALSE}
pp_check(mdl_H1_binary, ndraws = 100)

pp_check(mdl_H1_binary, type = "error_scatter_avg", ndraws = 100)
```

## Guess similarity

Preregistered model:

`guessability_similarity ~ expressibility + (1 | dyad/participant) + (1 | concept)`

### Model

```{r}
mdl_H1_similarity <- brm(
  # Main formula for the μ (mean) component:
  bf(cosine_similarity ~ expressibility_dutch + (1 | dyad) + (1 | concept),
  # Explicit subformulas for the additional parameters:
  phi ~ 1,                           # Precision parameter (controls concentration)
  zoi ~ 1,                           # Zero–one inflation: probability of coming from the beta part
  coi ~ 1),                            # One inflation: probability of an observation being exactly 1
  data = df,
  family = zero_one_inflated_beta(),
  prior = c(
    # Priors for the μ component (cosine_similarity):
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),  # for expressibility_dutch
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_H1_similarity.rds")
)


# if we need to compress the model more
#saveRDS(mdl_H1_similarity, file = paste0(models, "mdl_H1_similarity.rds"), compress = "xz")

mdl_H1_similarity <- readRDS(paste0(models, "mdl_H1_similarity.rds"))
```

Including expressibility_dutch in the subformulas for φ, zoi, or coi is
an option—but it should be done only if you have a strong theoretical or
empirical reason to believe that expressibility affects not only the
average outcome (the μ component) but also the dispersion or the
inflation processes. Considerations:

1.  **Theoretical Rationale:**

    -   **μ Component (Mean):**\
        Your primary hypothesis (H1) is that higher expressibility leads
        to higher guessability—that is, it shifts the central tendency
        of cosine similarity. This is why you include
        expressibility_dutch in the μ model.

    -   **φ Component (Precision):**\
        Adding expressibility_dutch here would imply that expressibility
        influences how concentrated or spread out the responses are
        (i.e., the variability around the mean). If you believe that
        expressibility not only changes the average guessability but
        also affects the consistency (or uncertainty) of the responses,
        then including it in φ might be justified.

    -   **zoi and coi (Inflation Components):**\
        Including expressibility_dutch in these parts would mean that
        expressibility also affects the probability of obtaining
        boundary values (exact 0 or 1). You would do this if you expect
        that higher expressibility systematically leads to more (or
        fewer) extreme values (e.g., more responses hitting exactly 1).

2.  **Empirical and Practical Considerations:**

    -   **Model Complexity:**\
        Adding the predictor to φ, zoi, and coi increases model
        complexity. This can lead to difficulties in convergence,
        overfitting, or less stable estimates if the data do not contain
        enough information for these additional effects.

    -   **Interpretability:**\
        The interpretation of effects on φ, zoi, and coi is more subtle.
        Unless you have a clear rationale, it may be preferable to keep
        expressibility_dutch only in the μ model and allow the other
        parameters to remain constant.

    -   **Exploratory Analysis:**\
        You might start with the simpler model (predictor only in μ) and
        then check if there is evidence—through residual analysis, model
        comparisons, or substantive theory—that expressibility_dutch
        might also be affecting variability or inflation. Only then
        would you consider adding it to the other components.

**Conclusion:**

Unless you have a specific reason to believe that expressibility_dutch
also influences the dispersion (φ) or the probability of hitting the
boundaries (zoi and coi), it is generally advisable to include it only
in the μ component. This keeps the model simpler and the interpretation
more straightforward, directly addressing your hypothesis about the
average effect on guessability.

If you later decide to explore whether expressibility affects other
aspects of the distribution, you could extend the model accordingly, but
that should be guided by theory or clear empirical indications rather
than by default.

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_H1_similarity_R2 <- bayes_R2(mdl_H1_similarity)
# # Save the R² output
# saveRDS(mdl_H1_similarity_R2, file = paste0(models, "mdl_H1_similarity_R2.rds"))

mdl_H1_similarity_R2 <- readRDS(paste0(models, "mdl_H1_similarity_R2.rds"))

mdl_H1_similarity_R2
```

R2 explanation.......

#### Fixed effects

Summarize the model: Check parameter estimates, Rhat values
(convergence) and effective sample sizes.

```{r}
summary(mdl_H1_similarity)
```



# Hypothesis 2: Expressibility vs. repetitions

*The higher the expressibility rating of the concept for the modality in
which the concept is produced, the fewer repetitions there will be.*

This is tested on subset experiment_part = "2".

```{r}
df_part2 <- df %>%
  filter(exp_part == 2)
```

Preregistered models:

`N_repetition ~ expressibility + (1 | dyad/participant) + (1 | concept), family = poisson()`
`N_repetition ~ expressibility + (1 | dyad/participant) + (1 | concept), family = negbinomial()`
(if data is overdispersed)

## Poisson model

```{r}
mdl_H2_poisson <- brm(
  formula = correction ~ expressibility_dutch + (1 | dyad) + (1 | concept),
  data = df_part2,
  family = poisson(),
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),        # Prior for expressibility
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "model_H2_poisson.rds")
)

# if we need to compress the model more
#saveRDS(mdl_H2_poisson, file = paste0(models, "mdl_H2_poisson.rds"), compress = "xz")

mdl_H2_poisson <- readRDS(paste0(models, "mdl_H2_poisson.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# # Calculate Bayesian R²
# mdl_H2_poisson_R2 <- bayes_R2(mdl_H2_poisson)
# # Save the R² output
# saveRDS(mdl_H2_poisson_R2, file = paste0(models, "mdl_H2_poisson_R2.rds"))

mdl_H2_poisson_R2 <- readRDS(paste0(models, "mdl_H2_poisson_R2.rds"))

mdl_H2_poisson_R2
```

R2 explanation.......

#### Fixed effects

Summarize the model: Check parameter estimates, Rhat values
(convergence) and effective sample sizes.

```{r}
summary(model_H2_poisson)
```

If you detect overdispersion (variance \>\> mean), consider a negative
binomial model.

# Hypothesis 3: Guessability vs. experiment part

*Guessability is higher when there is feedback between a producer and a
guesser (i.e., in the second half of the experiment, where the guesser
knows if they answered correctly, and the producer knows what the
guesser answered).*

## Guess binary

Preregistered model:
`guessability_match ~ experiment_part + (1 | dyad/participant) + (1 | concept), family = bernoulli(link = 'logit')`

### Model

```{r}
mdl_H3_binary <- brm(
  formula = guess_binary ~ exp_part + (1 | dyad) + (1 | concept),
  data = df,
  family = bernoulli(link = "logit"),
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),        # Prior for expressibility
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_H3_binary.rds")
)

# if we need to compress the model more
#saveRDS(mdl_H3_binary, file = paste0(models, "mdl_H3_binary.rds"), compress = "xz")

mdl_H3_binary <- readRDS(paste0(models, "mdl_H3_binary.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Calculate Bayesian R²
mdl_H3_binary_R2 <- bayes_R2(mdl_H3_binary)
# Save the R² output
saveRDS(mdl_H3_binary_R2, file = paste0(models, "mdl_H3_binary_R2.rds"))

mdl_H3_binary_R2 <- readRDS(paste0(models, "mdl_H3_binary_R2.rds"))

mdl_H3_binary_R2
```

R2 explanation.......

#### Fixed effects

Summarize the model: Check parameter estimates, Rhat values
(convergence) and effective sample sizes.

```{r}
summary(mdl_H3_binary)
```

## Guess similarity

Preregistered model:
`guessability_similarity ~ experiment_part + (1 | dyad/participant) + (1 | concept)`

### Model

```{r}
mdl_H3_similarity <- brm(
  # Main formula for the μ (mean) component:
  bf(cosine_similarity  ~ exp_part + (1 | dyad) + (1 | concept),
  # Explicit subformulas for the additional parameters:
  phi ~ 1,                           # Precision parameter (controls concentration)
  zoi ~ 1,                           # Zero–one inflation: probability of coming from the beta part
  coi ~ 1),                            # One inflation: probability of an observation being exactly 1
  data = df,
  family = zero_one_inflated_beta(),
  prior = c(
    # Priors for the μ component (cosine_similarity):
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),  # for expressibility_dutch
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_H3_similarity.rds")
)

# if we need to compress the model more
#saveRDS(mdl_H3_similarity, file = paste0(models, "mdl_H3_similarity.rds"), compress = "xz")

mdl_H1_similarity <- readRDS(paste0(models, "mdl_H3_similarity.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Calculate Bayesian R²
mdl_H3_similarity_R2 <- bayes_R2(mdl_H3_similarity)
# Save the R² output
saveRDS(mdl_H3_similarity_R2, file = paste0(models, "mdl_H3_similarity_R2.rds"))

mdl_H3_similarity_R2 <- readRDS(paste0(models, "mdl_H3_similarity_R2.rds"))

mdl_H3_similarity_R2
```

R2 explanation.......

#### Fixed effects

Summarize the model: Check parameter estimates, Rhat values
(convergence) and effective sample sizes.

```{r}
summary(mdl_H3_similarity)
```

# Hypothesis 4: Guessability vs. modality

*Guessability will differ by modality; the order from highest to lowest
will be: combined \> gesture \> vocalization.*

## Guess binary

Preregistered model:
`guessability_match ~ modality + (1 | dyad/participant) + (1 | concept), family = bernoulli(link='logit')`

### Model

```{r}
mdl_H4_binary <- brm(
  formula = guess_binary ~ modality + (1 | dyad) + (1 | concept),
  data = df,
  family = bernoulli(link = "logit"),
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),        # Prior for modality
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_H4_binary.rds")
)

# if we need to compress the model more
#saveRDS(mdl_H4_binary, file = paste0(models, "mdl_H4_binary.rds"), compress = "xz")

mdl_H4_binary <- readRDS(paste0(models, "mdl_H4_binary.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Calculate Bayesian R²
mdl_H4_binary_R2 <- bayes_R2(mdl_H4_binary)
# Save the R² output
saveRDS(mdl_H4_binary_R2, file = paste0(models, "mdl_H4_binary_R2.rds"))

mdl_H4_binary_R2 <- readRDS(paste0(models, "mdl_H4_binary_R2.rds"))

mdl_H4_binary_R2
```

R2 explanation.......

#### Fixed effects

Summarize the model: Check parameter estimates, Rhat values
(convergence) and effective sample sizes.

```{r}
summary(mdl_H4_binary)
```

## Guess similarity

Preregistered model:
`guessability_similarity ~ modality + (1 | dyad/participant) + (1 | concept)`

### Model

```{r}
mdl_H4_similarity <- brm(
  # Main formula for the μ (mean) component:
  bf(cosine_similarity  ~ modality + (1 | dyad) + (1 | concept),
  # Explicit subformulas for the additional parameters:
  phi ~ 1,                           # Precision parameter (controls concentration)
  zoi ~ 1,                           # Zero–one inflation: probability of coming from the beta part
  coi ~ 1),                            # One inflation: probability of an observation being exactly 1
  data = df,
  family = zero_one_inflated_beta(),
  prior = c(
    # Priors for the μ component (cosine_similarity):
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),  # for expressibility_dutch
    prior(normal(0, 2), class = "sd")
  ),
  cores = 4,
  chains = 4,
  iter = 10000,
  warmup = 4000,
  seed = 17,
  control = list(max_treedepth = 12,
                 adapt_delta = 0.99),
  file = paste0(models, "mdl_H4_similarity.rds")
)

# if we need to compress the model more
#saveRDS(mdl_H4_similarity, file = paste0(models, "mdl_H4_similarity.rds"), compress = "xz")

mdl_H1_similarity <- readRDS(paste0(models, "mdl_H4_similarity.rds"))
```

Calculate R².

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Calculate Bayesian R²
mdl_H4_similarity_R2 <- bayes_R2(mdl_H4_similarity)
# Save the R² output
saveRDS(mdl_H4_similarity_R2, file = paste0(models, "mdl_H4_similarity_R2.rds"))

mdl_H4_similarity_R2 <- readRDS(paste0(models, "mdl_H4_similarity_R2.rds"))

mdl_H4_similarity_R2
```

R2 explanation.......

#### Fixed effects

Summarize the model: Check parameter estimates, Rhat values
(convergence) and effective sample sizes.

```{r}
summary(mdl_H4_similarity)
```

# Session info

```{r echo=TRUE, message=FALSE, warning=FALSE}
sessionInfo()
```
