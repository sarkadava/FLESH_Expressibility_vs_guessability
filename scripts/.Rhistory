text = element_text(size = 14)
) +
coord_flip()  # Flip coordinates to make the plot more readable
df %>%
group_by(concept, exp_part) %>%
summarise(frequency = n(), .groups = "drop") %>%  # Calculate frequency for each exp_part
pivot_wider(names_from = exp_part, values_from = frequency, names_prefix = "frequency_part") %>%
arrange(desc(frequency_part1), desc(frequency_part2)) %>%
print(n = 100)
df %>%
group_by(exp_part, modality) %>%
summarise(frequency = n(), .groups = "drop") %>%  # Calculate frequency for each concept and modality
pivot_wider(names_from = modality, values_from = frequency, names_prefix = "modality_") %>%
#arrange(desc(modality_combined), desc(modality_gesture), desc(modality_vocal)) %>%  # Sort by modality frequencies
print(n = 100)
length(unique(df))
nrow(df)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: source setup
########## folders ##########
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
parentfolder <- dirname(getwd())
rawdata       <- paste0(parentfolder, '/rawdata/')
dataset       <- paste0(parentfolder, '/dataset/')
models        <- paste0(parentfolder, '/models/')
plots         <- paste0(parentfolder, '/plots/')
scripts       <- paste0(parentfolder, '/scripts/')
########## source file ##########
#source(paste0(scripts, "adjectives-preparation.R"))
#################### packages ####################
# Data Manipulation
library(tibble)
library(stringr)
library(tidyverse) # includes readr, tidyr, dplyr, ggplot2
packageVersion("tidyverse")
library(data.table)
library(readxl)
# Plotting
library(ggforce)
library(ggpubr)
library(gridExtra)
library(corrplot)
library(ggdist)
library(ggbeeswarm)
library(BayesFactor)
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())
colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# Chunk 3: read metadata
# Load data frame
df <- read_csv(paste0(dataset, "similarity_df_final.csv"))
# Load concept list
concepts <- read_excel(paste0(dataset, "conceptlist_info.xlsx"))
# Load expressibility German
expr_german <- read_csv(paste0(dataset, "expressibility_german.csv"))
# Load expressibility Dutch
expr_dutch <- read_csv(paste0(dataset, "expressibility_dutch.csv"))
# Chunk 4
df <- df %>%
mutate(
modality = case_when(
modality == "combinatie" ~ "combined",
modality == "gebaren" ~ "gesture",
modality == "geluiden" ~ "vocal",
TRUE ~ modality
)
) %>%
rename(participant_dyad = participant) %>%
rename(participant_ID = pcnID) %>%
rename(concept = English) %>%
select(-`...1`) %>%  # Remove the first column
rename(stimulus = word)
# Reorder the columns in the specified order
df <- df %>%
select(trial_order, trial_type,
dyad, participant_dyad, participant_ID, exp_part, modality,
expressibility_dutch, concept, correction,
guess_binary, cosine_similarity, stimulus, answer,
SemanticSubcat, sessionID)
# Only keep target trials
df <- df %>%
filter(trial_type == "target")%>%
select(-`trial_type`) # Remove trial_type columns
# Chunk 5
# exclude here (make to NA)
View(concepts)
length(unique(df$dyad))
length(unique(df$participant_ID))
length(unique(df$concept))
length(unique(df$stimulus))
(unique(df$concept)
)
View(concepts)
View(expr_dutch)
########## folders ##########
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
parentfolder <- dirname(getwd())
rawdata       <- paste0(parentfolder, '/rawdata/');
dataset       <- paste0(parentfolder, '/dataset/');
models        <- paste0(parentfolder, '/models/');
plots         <- paste0(parentfolder, '/plots/');
scripts       <- paste0(parentfolder, '/scripts/');
########## source file ##########
#rmarkdown::render("1_descriptive.Rmd", envir = globalenv(), clean = FALSE)
#################### packages ####################
# Data Manipulation
library(tidyverse);
# Plotting
library(corrplot);
library(cowplot);
library(ggdist);
# Bayesian
library(brms);
library(cmdstanr);
library(emmeans);
library(posterior);
library(bayesplot);
library(tidybayes);
theme_set(theme_tidybayes() + panel_border());
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores());
colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: source setup
########## folders ##########
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
parentfolder <- dirname(getwd())
rawdata       <- paste0(parentfolder, '/rawdata/');
dataset       <- paste0(parentfolder, '/dataset/');
models        <- paste0(parentfolder, '/models/');
plots         <- paste0(parentfolder, '/plots/');
scripts       <- paste0(parentfolder, '/scripts/');
########## source file ##########
#rmarkdown::render("1_descriptive.Rmd", envir = globalenv(), clean = FALSE)
#################### packages ####################
# Data Manipulation
library(tidyverse);
# Plotting
library(corrplot);
library(cowplot);
library(ggdist);
# Bayesian
library(brms);
library(cmdstanr);
library(emmeans);
library(posterior);
library(bayesplot);
library(tidybayes);
theme_set(theme_tidybayes() + panel_border());
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores());
colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# Chunk 3: read metadata
# Load data frame
df <- read_csv(paste0(dataset, "df.csv"))
# Load concept list
concepts <- readxl::read_excel(paste0(dataset, "conceptlist_info.xlsx"))
# Load expressibility German
expr_german <- read_csv(paste0(dataset, "expressibility_german.csv"))
# Load expressibility Dutch
expr_dutch <- read_csv(paste0(dataset, "expressibility_dutch.csv"))
# Chunk 4
nrow(df)
df %>%
group_by(concept) %>%
summarise(frequency = n()) %>%
arrange(desc(frequency)) %>%
print(n = 100)
# Chunk 5
# Total missing values for cosine_similarity and its percentage
cat("Total missing values for cosine_similarity: ", sum(is.na(df$cosine_similarity)), "\n")
cat("Percentage missing: ", round(100 * sum(is.na(df$cosine_similarity)) / nrow(df), 1), "%\n\n")
# Breakdown by modality:
cat("Missing values by modality:\n")
df %>%
group_by(modality) %>%
summarise(
missing = sum(is.na(cosine_similarity)),
total = n(),
percent_missing = round(100 * sum(is.na(cosine_similarity)) / n(), 1)
) %>%
print(n = Inf)
cat("\nBreakdown by experiment part (exp_part):\n")
df %>%
group_by(exp_part) %>%
summarise(
missing = sum(is.na(cosine_similarity)),
total = n(),
percent_missing = round(100 * sum(is.na(cosine_similarity)) / n(), 1)
) %>%
print(n = Inf)
cat("\nBreakdown by repetition (correction):\n")
df %>%
group_by(correction) %>%
summarise(
missing = sum(is.na(cosine_similarity)),
total = n(),
percent_missing = round(100 * sum(is.na(cosine_similarity)) / n(), 1)
) %>%
print(n = Inf)
# Chunk 6
ggplot(df, aes(x = cosine_similarity)) +
geom_density(fill = colorBlindBlack8[6]) +
theme_minimal() +
labs(x = "Cosine Similarity", y = "Density")
moments::skewness(df$cosine_similarity)
# Chunk 7
ggplot(df, aes(x = cosine_similarity, fill = factor(guess_binary))) +
geom_density(alpha = 0.6) +  # alpha to make the fill slightly transparent
theme_minimal() +
labs(x = "Cosine Similarity", y = "Density", fill = "Guess binary") +
scale_fill_manual(values = c(colorBlindBlack8[6], colorBlindBlack8[7]))
# Chunk 8
df %>%
group_by(guess_binary) %>%
summarise(
mean_cosine_similarity = mean(cosine_similarity, na.rm = TRUE),
sd_cosine_similarity = sd(cosine_similarity, na.rm = TRUE),
min_cosine_similarity = min(cosine_similarity, na.rm = TRUE),
max_cosine_similarity = max(cosine_similarity, na.rm = TRUE),
median_cosine_similarity = median(cosine_similarity, na.rm = TRUE),
n = n()  # Count of observations
)
# Chunk 9
# Outcome variables
df$guess_binary <- factor(df$guess_binary, levels = c("0", "1")); contrasts(df$guess_binary) <- c(-0.5, +0.5)
## 'cosine_similarity' is continuous, no need to transform it
df$correction <- factor(df$correction, levels = c("0", "1", "2"))
# Predictor variables
## 'expressibility' is continuous, no transformation needed
df$exp_part <- factor(df$exp_part, levels = c("1", "2")); contrasts(df$exp_part) <- c(-0.5, +0.5)
df$modality <- factor(df$modality, levels = c("combined", "gesture", "vocal"))
# Random variables
df$concept <- factor(df$concept)
df$dyad <- factor(df$dyad)
df$participant_ID <- factor(df$participant_ID)
# Check the balance between the levels
df %>% count(guess_binary) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
# imbalanced 0: 67.5%, 1: 32.5%
df %>% count(correction) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
# imbalanced 0: 66.2%, 1: 19.9%, 2: 13.9%
df %>% count(exp_part) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
# imbalanced 1: 33%, 2: 67%
df %>% count(modality) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
# pretty balanced, combined: 31%, gesture: 31.1%, vocal: 37.9%
# Random variables just if someone is interested in checking
# df %>% count(concept) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
# df %>% count(dyad) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
# df %>% count(participant_ID) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
mdl_null_binary <- brm(
formula = guess_binary ~ 1 + (1 | dyad/participant_ID) + (1 | concept),
data = df,
family = bernoulli(link = "logit"),
prior = c(
# Prior for the intercept on the logit scale:
prior(normal(0, 2.5), class = "Intercept"),
# Prior for the standard deviations of the random effects:
prior(normal(0, 2), class = "sd")
),
cores = 4,
chains = 4,
iter = 10000,
warmup = 4000,
seed = 17,
control = list(max_treedepth = 12,
adapt_delta = 0.99),
file = paste0(models, "mdl_null_binary.rds")
)
mdl_null_binary <- brm(
formula = guess_binary ~ 1 + (1 | dyad/participant_ID) + (1 | concept),
data = df,
family = bernoulli(link = "logit"),
prior = c(
# Prior for the intercept on the logit scale:
prior(normal(0, 2.5), class = "Intercept"),
# Prior for the standard deviations of the random effects:
prior(normal(0, 2), class = "sd")
),
cores = 4,
chains = 4,
iter = 10000,
warmup = 4000,
seed = 17,
control = list(max_treedepth = 12,
adapt_delta = 0.99),
file = paste0(models, "mdl_null_binary.rds")
)
# if we need to compress the model more
#saveRDS(mdl_null_binary, file = paste0(models, "mdl_null_binary.rds"), compress = "xz")
mdl_null_binary <- readRDS(paste0(models, "mdl_null_binary.rds"))
summary(mdl_null_binary)
pp_check(mdl_null_binary, ndraws = 100)
pp_check(mdl_null_binary, type = "error_scatter_avg", ndraws = 100)
# Calculate Bayesian R²
mdl_null_binary_R2 <- bayes_R2(mdl_null_binary)
# Save the R² output
saveRDS(mdl_null_binary_R2, file = paste0(models, "mdl_null_binary_R2.rds"))
saveRDS(mdl_null_binary, file = paste0(models, "mdl_null_binary.rds"), compress = "xz")
# Intercept-only model for continuous outcome:
mdl_null_similarity <- brm(
formula = cosine_similarity ~ 1 + (1 | dyad/participant_ID) + (1 | concept),
data = df,
family = zero_one_inflated_beta(),
prior = c(
prior(normal(0, 5), class = "Intercept"),
prior(normal(0, 2), class = "sd")
),
cores = 4,
chains = 4,
iter = 10000,
warmup = 4000,
seed = 17,
control = list(max_treedepth = 12,
adapt_delta = 0.99),
file = paste0(models, "mdl_null_similarity.rds")
)
saveRDS(mdl_null_similarity, file = paste0(models, "mdl_null_similarity.rds"), compress = "xz")
# Calculate Bayesian R²
mdl_null_similarity_R2 <- bayes_R2(mdl_null_similarity)
# Save the R² output
saveRDS(mdl_null_similarity_R2, file = paste0(models, "mdl_null_similarity_R2.rds"))
mdl_null_similarity
_
pp_check(mdl_null_similarity, ndraws = 100)
pp_check(mdl_null_similarity, type = "error_scatter_avg", ndraws = 100)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: source setup
########## folders ##########
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
parentfolder <- dirname(getwd())
rawdata       <- paste0(parentfolder, '/rawdata/');
dataset       <- paste0(parentfolder, '/dataset/');
models        <- paste0(parentfolder, '/models/');
plots         <- paste0(parentfolder, '/plots/');
scripts       <- paste0(parentfolder, '/scripts/');
########## source file ##########
#rmarkdown::render("1_descriptive.Rmd", envir = globalenv(), clean = FALSE)
#################### packages ####################
# Data Manipulation
library(tidyverse);
# Plotting
library(corrplot);
library(cowplot);
library(ggdist);
# Bayesian
library(brms);
library(cmdstanr);
library(emmeans);
library(posterior);
library(bayesplot);
library(tidybayes);
theme_set(theme_tidybayes() + panel_border());
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores());
colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# Chunk 3: read metadata
# Load data frame
df <- read_csv(paste0(dataset, "df.csv"))
# Load concept list
concepts <- readxl::read_excel(paste0(dataset, "conceptlist_info.xlsx"))
# Load expressibility German
expr_german <- read_csv(paste0(dataset, "expressibility_german.csv"))
# Load expressibility Dutch
expr_dutch <- read_csv(paste0(dataset, "expressibility_dutch.csv"))
# Chunk 4
nrow(df)
df %>%
group_by(concept) %>%
summarise(frequency = n()) %>%
arrange(desc(frequency)) %>%
print(n = 100)
# Chunk 5
# Total missing values for cosine_similarity and its percentage
cat("Total missing values for cosine_similarity: ", sum(is.na(df$cosine_similarity)), "\n")
cat("Percentage missing: ", round(100 * sum(is.na(df$cosine_similarity)) / nrow(df), 1), "%\n\n")
# Breakdown by modality:
cat("Missing values by modality:\n")
df %>%
group_by(modality) %>%
summarise(
missing = sum(is.na(cosine_similarity)),
total = n(),
percent_missing = round(100 * sum(is.na(cosine_similarity)) / n(), 1)
) %>%
print(n = Inf)
cat("\nBreakdown by experiment part (exp_part):\n")
df %>%
group_by(exp_part) %>%
summarise(
missing = sum(is.na(cosine_similarity)),
total = n(),
percent_missing = round(100 * sum(is.na(cosine_similarity)) / n(), 1)
) %>%
print(n = Inf)
cat("\nBreakdown by repetition (correction):\n")
df %>%
group_by(correction) %>%
summarise(
missing = sum(is.na(cosine_similarity)),
total = n(),
percent_missing = round(100 * sum(is.na(cosine_similarity)) / n(), 1)
) %>%
print(n = Inf)
# Chunk 6
ggplot(df, aes(x = cosine_similarity)) +
geom_density(fill = colorBlindBlack8[6]) +
theme_minimal() +
labs(x = "Cosine Similarity", y = "Density")
moments::skewness(df$cosine_similarity)
# Chunk 7
ggplot(df, aes(x = cosine_similarity, fill = factor(guess_binary))) +
geom_density(alpha = 0.6) +  # alpha to make the fill slightly transparent
theme_minimal() +
labs(x = "Cosine Similarity", y = "Density", fill = "Guess binary") +
scale_fill_manual(values = c(colorBlindBlack8[6], colorBlindBlack8[7]))
# Chunk 8
df %>%
group_by(guess_binary) %>%
summarise(
mean_cosine_similarity = mean(cosine_similarity, na.rm = TRUE),
sd_cosine_similarity = sd(cosine_similarity, na.rm = TRUE),
min_cosine_similarity = min(cosine_similarity, na.rm = TRUE),
max_cosine_similarity = max(cosine_similarity, na.rm = TRUE),
median_cosine_similarity = median(cosine_similarity, na.rm = TRUE),
n = n()  # Count of observations
)
# Outcome variables
#df$guess_binary <- factor(df$guess_binary, levels = c("0", "1")); contrasts(df$guess_binary) <- c(-0.5, +0.5)
## 'cosine_similarity' is continuous, no need to transform it
df$correction <- factor(df$correction, levels = c("0", "1", "2"))
# Predictor variables
## 'expressibility' is continuous, no transformation needed
df$exp_part <- factor(df$exp_part, levels = c("1", "2")); contrasts(df$exp_part) <- c(-0.5, +0.5)
df$modality <- factor(df$modality, levels = c("combined", "gesture", "vocal"))
# Random variables
df$concept <- factor(df$concept)
df$dyad <- factor(df$dyad)
df$participant_ID <- factor(df$participant_ID)
# Check the balance between the levels
df %>% count(guess_binary) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
# imbalanced 0: 67.5%, 1: 32.5%
df %>% count(correction) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
# imbalanced 0: 66.2%, 1: 19.9%, 2: 13.9%
df %>% count(exp_part) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
# imbalanced 1: 33%, 2: 67%
df %>% count(modality) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
# pretty balanced, combined: 31%, gesture: 31.1%, vocal: 37.9%
# Random variables just if someone is interested in checking
# df %>% count(concept) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
# df %>% count(dyad) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
# df %>% count(participant_ID) %>% mutate(prop = paste0(round(100* n / sum(n),1),"%"))
mdl_null_binary <- brm(
formula = guess_binary ~ 1 + (1 | dyad/participant_ID) + (1 | concept),
data = df,
family = bernoulli(link = "logit"),
prior = c(
# Prior for the intercept on the logit scale:
prior(normal(0, 2.5), class = "Intercept"),
# Prior for the standard deviations of the random effects:
prior(normal(0, 2), class = "sd")
),
cores = 4,
chains = 4,
iter = 10000,
warmup = 4000,
seed = 17,
control = list(max_treedepth = 12,
adapt_delta = 0.99),
file = paste0(models, "mdl_null_binary.rds")
)
# if we need to compress the model more
#saveRDS(mdl_null_binary, file = paste0(models, "mdl_null_binary.rds"), compress = "xz")
mdl_null_binary <- readRDS(paste0(models, "mdl_null_binary.rds"))
aveRDS(mdl_null_binary, file = paste0(models, "mdl_null_binary.rds"), compress = "xz")
saveRDS(mdl_null_binary, file = paste0(models, "mdl_null_binary.rds"), compress = "xz")
# Calculate Bayesian R²
mdl_null_binary_R2 <- bayes_R2(mdl_null_binary)
# Save the R² output
saveRDS(mdl_null_binary_R2, file = paste0(models, "mdl_null_binary_R2.rds"))
summary(mdl_null_binary)
View(df)
summary(df$guess_binary)
posterior_epred(mdl_null_binary, transform = TRUE)
emmeans(mdl_null_binary, ~1, type = "response")
# because of the logit link, it needs to be backtransformed
emmeans(mdl_null_binary, ~1, type = "response")
mdl_null_binary_R2
fixef(mdl_null_binary)
fixef(mdl_null_binary)[1]
plogis(fixef(mdl_null_binary)[1])
summary(mdl_null_similarity)
summary(df$cosine_similarity)
fixef(mdl_null_similarity)
emmeans(mdl_null_similarity, ~1, type = "response")
back_transform_zoib <- function(eta, zoi, coi = 1) {
plogis(eta) * (1 - zoi) + zoi * coi
}
back_transform_zoib(
fixef(mdl_null_similarity)["Intercept", "Estimate"],
fixef(mdl_null_similarity, dpar = "zoi")["Intercept", "Estimate"]
)
back_transform_zoib(
fixef(mdl_null_similarity, dpar = "mu")["Intercept", "Estimate"],
fixef(mdl_null_similarity, dpar = "zoi")["Intercept", "Estimate"]
)
